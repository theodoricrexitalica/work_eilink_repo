{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libs \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statistics as st\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score as r2, mean_absolute_error as mae, mean_squared_error as mse, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "import random\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the ACG_wells_JOINT_BEST_v6.csv file\n",
    "# path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "# data_init = pd.read_csv(path + 'ACG_wells_JOINT_BEST_v10.csv', sep=',')\n",
    "# df = data_init.copy()\n",
    "# df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Select only neccessary data\n",
    "# df_cln = df[[   'wellName', 'MD', 'BADPORLOG', 'Casings', 'FORMATION', 'DEVI', 'HAZI',\n",
    "#                 'FLANK', 'FLANK1', 'FLANK2', 'NET', 'NET_VSH','FLUIDS',\n",
    "#                 'LPERM', 'PHIT', \n",
    "#                 'GR_N', 'GRMATRIX', 'GRSHALE','VSH', 'NPSS', 'RHOB', 'RHOF', 'RHOMA', \n",
    "#                 'RDEEP',  'SON', 'SONSH', \n",
    "#                 'TVD_SCS','TST', 'X', 'Y']]\n",
    "# #Fill up nan and -9999 values with 0\n",
    "# df_cln = df_cln.fillna(0)\n",
    "# df_cln = df_cln.replace(-9999, 0)\n",
    "# df_cln = df_cln.replace('-9999', '0')\n",
    "# #Assing proper datatypes for df\n",
    "# dicttypes = {   'wellName':'string', 'MD':'float', 'BADPORLOG':'int', 'Casings':'float', 'FORMATION':'string','DEVI':'float', 'HAZI':'float',\n",
    "#                 'FLANK':'int', 'FLANK1':'int', 'FLANK2':'int', 'NET':'int', 'NET_VSH':'int', 'FLUIDS':'int',\n",
    "#                 'LPERM':'float','PHIT':'float',\n",
    "#                 'GR_N':'float', 'GRMATRIX':'float', 'GRSHALE':'float', 'VSH':'float', 'NPSS':'float','RHOB':'float','RHOF':'float', 'RHOMA':'float',\n",
    "#                 'RDEEP':'float', 'SON':'float', 'SONSH':'float',\n",
    "#                 'TVD_SCS':'float', 'TST':'float', 'X':'float', 'Y':'float'}\n",
    "# df_cln = df_cln.astype(dicttypes, errors='ignore')\n",
    "# df_cln.loc[df_cln.FORMATION=='0', 'FORMATION']='None'\n",
    "# # Save data to parquet\n",
    "# df_cln.to_parquet('ACG_wells_JOINT_BEST_v10.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata, distribution wells per Platforms and all the that.\n",
    "path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "metadata_init = pd.read_csv(path + 'ACG_wells_metadata.csv', sep=',')\n",
    "metadata = metadata_init.copy()\n",
    "metadata = metadata.rename(columns={'X':'X_wellhead', 'Y':'Y_wellhead'})\n",
    "metadata.Status = metadata.Status.str.strip()\n",
    "metadata.Status = metadata.Status.str.lower()\n",
    "metadata.loc[metadata.Status == 'oil', 'Status' ] = 'production oil'\n",
    "metadata.loc[metadata.Status == 'oil producer', 'Status' ] = 'production oil'\n",
    "metadata.loc[metadata.Status == 'production', 'Status' ] = 'production oil'\n",
    "metadata.loc[metadata.Status == 'produiction oil', 'Status' ] = 'production oil'\n",
    "metadata.loc[metadata.Status == 'production_oil', 'Status' ] = 'production oil'\n",
    "metadata.loc[metadata.Status == 'abandoned production oil', 'Status' ] = 'abandoned oil'\n",
    "metadata.loc[metadata.Status == 'abandoned  oil', 'Status' ] = 'abandoned oil'\n",
    "metadata.loc[metadata.Status == 'abandoned oi', 'Status' ] = 'abandoned oil'\n",
    "metadata.loc[metadata.Status == 'injector  - water', 'Status' ] = 'injector - water'\n",
    "metadata.loc[metadata.Status == 'injector water', 'Status' ] = 'injector - water'\n",
    "metadata.loc[metadata.Status == 'injetor  - water', 'Status' ] = 'injector - water'\n",
    "metadata.loc[metadata.Status == 'abandoned injector - water per b', 'Status' ] = 'abandoned injector - water'\n",
    "metadata.loc[metadata.Status == 'plugged and abandoned', 'Status' ] = 'p&a'\n",
    "metadata.loc[metadata.X_wellhead==118.270, 'X_wellhead'] = 526258.84\n",
    "metadata.loc[metadata.Y_wellhead==526261.510, 'Y_wellhead'] = 4435802.01\n",
    "metadata.loc[metadata.well=='C39', 'X_wellhead'] = 526258.840\n",
    "metadata.loc[metadata.well=='C39', 'Y_wellhead'] = 4435802.010\n",
    "metadata.loc[metadata.field=='West Azeri', 'field'] = 'WEST AZERI'\n",
    "metadata.loc[metadata.field=='COP', 'field'] = 'WEST CHIRAG'\n",
    "metadata.loc[metadata.well=='AZERI2', 'field'] = 'WEST AZERI'\n",
    "metadata.loc[metadata.well=='AZERI3', 'field'] = 'WEST AZERI'\n",
    "metadata.loc[metadata.well=='B31', 'field'] = 'CENTRAL AZERI'\n",
    "metadata.loc[metadata.well=='J28_bpQIP', 'field'] = 'WEST CHIRAG'\n",
    "\n",
    "#Read data from parquet\n",
    "path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "df_prq = pd.read_parquet(path + 'ACG_wells_JOINT_BEST_v10.parquet.gzip')\n",
    "df_prq.rename(columns={'wellName':'well'}, inplace=True)\n",
    "df_prq = df_prq.set_index('well').join(metadata.set_index('well')).reset_index()\n",
    "# print('wells in df totally:', len(df_prq.well.unique()))\n",
    "# Filter data with bad_well_list \n",
    "bad_well_list = ['E10Z','Predrill_J01Z', 'Predrill_J08', 'J28_bpQIP']\n",
    "df_prq = df_prq[~df_prq.well.isin(bad_well_list)]\n",
    "#Assign any Fluidcode_mod number by variable gross_pay=1 and gross_pay=0 if Fluidcode_mod as NaN\n",
    "df_prq.loc[df_prq.FLUIDS>0, 'FLUIDS_int'] = 1\n",
    "df_prq.loc[df_prq.FLUIDS<=0, 'FLUIDS_int'] = 0\n",
    "df_prq.FLUIDS_int = df_prq.FLUIDS_int.astype('int')\n",
    "#Getting XY coords of Balakhany formation tops\n",
    "xy_coord = df_prq[['well', 'FORMATION', 'X', 'Y']]\n",
    "xy_coord = xy_coord.groupby(['well', 'FORMATION']).apply(lambda x: x.iloc[0]).drop(columns=['well', 'FORMATION']).reset_index()\n",
    "xy_coord = xy_coord[xy_coord.FORMATION.str.contains('Balakhany') & (xy_coord.X>0) & (xy_coord.Y>0)]\n",
    "#Find top TVD_SCS for each formation\n",
    "df_prq_tvdss = df_prq[['well','MD','FORMATION','TVD_SCS']].groupby(['well','FORMATION']).apply(lambda x: x.iloc[0])\n",
    "df_prq_tvdss = df_prq_tvdss.drop(['well','FORMATION'], axis=1).reset_index()\n",
    "df_prq_tvdss = df_prq_tvdss[df_prq_tvdss.TVD_SCS>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading csv with initial KHtst_v3, joining xy-coord & TVD_SCS tops of formation\n",
    "path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\'\n",
    "df_khtst = pd.read_csv(path + 'df_prq_khtst_v3.csv')\n",
    "df_khtst_xy = df_khtst.set_index(['well','FORMATION']).join(xy_coord.set_index(['well','FORMATION'])).reset_index()\n",
    "df_khtst_xy_tvd = df_khtst_xy.set_index(['well', 'FORMATION']).join(df_prq_tvdss.set_index(['well','FORMATION'])).reset_index()\n",
    "df_khtst_xy_tvd_fld = df_khtst_xy_tvd.set_index('well').join(df_prq.groupby('well')['field'].apply(lambda x: x.iloc[0])).reset_index()\n",
    "#Clean dataset for outliers for Balakhany VIII & X  for AZR and CHG fields by rule 1.5 * IQR\n",
    "fm_list_8_10 = ['Balakhany VIII', 'Balakhany VIII sand', 'Balakhany VIII 25','Balakhany VIII 20', \n",
    "                'Balakhany VIII 15', 'Balakhany VIII 10', 'Balakhany VIII 5',\n",
    "                'Balakhany X', 'Balakhany X sand', 'Balakhany X 40', 'Balakhany X 20'] \n",
    "azr_lst = ['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']\n",
    "chg_lst = ['CHIRAG', 'DWG', 'DDGG', 'WEST CHIRAG']\n",
    "df_lst = []\n",
    "for fm in fm_list_8_10:\n",
    "    df_khtst_fm = df_khtst_xy_tvd_fld[(df_khtst_xy_tvd_fld.FORMATION == fm) & (df_khtst_xy_tvd_fld.field.isin(azr_lst))]\n",
    "    Q1 = df_khtst_fm['KHtst'].quantile(0.25)\n",
    "    Q3 = df_khtst_fm['KHtst'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # print(f'bal {fm} azr IQR', IQR, 'bot limit:', (Q1 - 1.5 * IQR), 'top limit:', (Q3 + 1.5 * IQR))\n",
    "    df_khtst_fm_qcl = df_khtst_fm[~((df_khtst_fm['KHtst'] < (Q1 - 1.5 * IQR)) | (df_khtst_fm['KHtst'] > (Q3 + 1.5 * IQR)))]\n",
    "    df_lst.append(df_khtst_fm_qcl)\n",
    "for fm in fm_list_8_10:\n",
    "    df_khtst_fm = df_khtst_xy_tvd_fld[(df_khtst_xy_tvd_fld.FORMATION == fm) & (df_khtst_xy_tvd_fld.field.isin(chg_lst))]\n",
    "    Q1 = df_khtst_fm['KHtst'].quantile(0.25)\n",
    "    Q3 = df_khtst_fm['KHtst'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # print(f'bal {fm} chg IQR', IQR, 'bot limit:', (Q1 - 1.5 * IQR), 'top limit:', (Q3 + 1.5 * IQR))\n",
    "    df_khtst_fm_qcl = df_khtst_fm[~((df_khtst_fm['KHtst'] < (Q1 - 1.5 * IQR)) | (df_khtst_fm['KHtst'] > (Q3 + 1.5 * IQR)))]\n",
    "    df_lst.append(df_khtst_fm_qcl)\n",
    "df_khtst_bal_qcl = pd.concat(df_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllFeatures dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading k_htst data from csv-file & Calculation of Euclidean Distances\n",
    "path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\'\n",
    "df_khtst = pd.read_csv(path + 'df_prq_khtst_v3.csv')\n",
    "df_khtst_xy = df_khtst.set_index(['well','FORMATION']).join(\n",
    "                                                            df_prq[['well','FORMATION','X','Y','TVD_SCS']].groupby(\n",
    "                                                            ['well','FORMATION']).apply(lambda x: x.iloc[0]).drop(\n",
    "                                                            ['well','FORMATION'], axis=1)\n",
    "                                                            ).reset_index()\n",
    "def well_display(wellname, fmname):\n",
    "    data = df_prq[(df_prq.well==wellname) & (df_prq.FORMATION == fmname)]\n",
    "    depth = data['MD']\n",
    "    grn = data['GR_N']\n",
    "    rhob = data['RHOB'] \n",
    "    npss = data['NPSS']\n",
    "    rdeep = data['RDEEP']\n",
    "    phit = data['PHIT'] \n",
    "    fluid = data['gross_pay']\n",
    "    perm = data['LPERM']\n",
    "    fig, ax = plt.subplots(1,5, figsize=(7,7), sharey=True)\n",
    "    ax[0].plot(grn, depth, color='lightgreen'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150)\n",
    "    ax[1].plot(rhob, depth, color='red'), ax[1].invert_yaxis(), ax[1].set_xlim(1.65, 2.65)\n",
    "    twin1 = ax[1].twiny()\n",
    "    twin1.plot(npss, depth, color='blue')\n",
    "    twin1.set_xlim(0.6, 0)\n",
    "    ax[2].plot(rdeep, depth, color='black'), ax[2].set_xscale('log'), ax[2].set_xlim(1, 100), ax[2].invert_yaxis(), ax[2].grid(axis='x', which='both')\n",
    "    ax[3].plot(phit, depth, color='green'), ax[3].set_xlim(0.3, 0), ax[3].grid(axis='x'), ax[3].invert_yaxis()\n",
    "    ax[3].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "    twin2 = ax[3].twiny()\n",
    "    twin2.plot(fluid, depth, color='orange', linewidth=0.5)\n",
    "    twin2.fill_betweenx(depth,fluid, color='orange', alpha=0.33)\n",
    "    twin2.set_xlim(0, 1)\n",
    "    twin2.set_ylim(min(depth), max(depth))\n",
    "    ax[4].plot(perm, depth, color='purple'), ax[4].set_xscale('log'), ax[4].set_xlim(1, 1000), ax[4].grid(axis='x'), ax[4].invert_yaxis()\n",
    "    fig.suptitle(wellname + ' ' + fmname, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "#Calculation of Euclidean Distances for the top of Balakhany VIII sand & Balakhany X sand\n",
    "def well_dist_calc(dist_formation, iqr_clean, iqr_wells):\n",
    "    \"\"\"\n",
    "    dist_formation, iqr_clean=f(1,0), iqr_wells == df_khtst_bal_qcl\n",
    "    \"\"\"\n",
    "    data = df_khtst_xy_tvd[(df_khtst_xy_tvd.FORMATION == dist_formation) & (df_khtst_xy_tvd.X > 0) & (df_khtst_xy_tvd.Y > 0) &\n",
    "                            (df_khtst_xy_tvd.MD.notna())]\n",
    "    if iqr_clean == 1:\n",
    "        data = data[data.well.isin(iqr_wells)]\n",
    "    else:\n",
    "        pass\n",
    "    row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "    distance_fm = pd.DataFrame(euclidean_distances(data[['X', 'Y', 'TVD_SCS']]), columns=list(data.well))\n",
    "    distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "    distance_fm_well.reset_index()\n",
    "    return distance_fm_well.reset_index()\n",
    "#Collecting of EucDist, XY and KHtst data \n",
    "def well_kh_accum(data, kh_formation):\n",
    "    well_kh_accum = []\n",
    "    well_x_accum = []\n",
    "    well_y_accum = []\n",
    "    for i in data:\n",
    "        well_kh_accum.append(df_khtst_xy[(df_khtst_xy.well==i)&(df_khtst_xy.FORMATION == kh_formation)]['KHtst'].reset_index())    \n",
    "        well_x_accum.append(df_khtst_xy[(df_khtst_xy.well==i)&(df_khtst_xy.FORMATION == kh_formation)]['X'].reset_index())\n",
    "        well_y_accum.append(df_khtst_xy[(df_khtst_xy.well==i)&(df_khtst_xy.FORMATION == kh_formation)]['Y'].reset_index())\n",
    "    well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "    well_kh3.columns = ['kh1','kh2','kh3']\n",
    "    well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "    well_x3.columns = ['x1','x2','x3']\n",
    "    well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "    well_y3.columns = ['y1','y2','y3']\n",
    "    final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                        well_x3.reset_index().drop('index',axis=1), \n",
    "                        well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "    return final\n",
    "#Collecting of Dist & Wells data togher with func(well_kh_accum)\n",
    "def well_offsets_collection(dist_dataset, off_formation, kh_dataset, metadata):\n",
    "    df_collect = []\n",
    "    for num, well_name in enumerate(dist_dataset.well[:]):\n",
    "        well_dist3 = dist_dataset[dist_dataset.well == well_name].T[1:].sort_values(by=num)[1:4].reset_index()\n",
    "        well_dist3_tuple = tuple(well_dist3['index'])\n",
    "        well_dist3_res = well_dist3.T[1:].reset_index().drop('index', axis=1)   \n",
    "        well_name3_res = well_dist3.T[:1].reset_index().drop('index', axis=1)\n",
    "        well_kh3_res = well_kh_accum(well_dist3_tuple, off_formation)\n",
    "        well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "        well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "        concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "        result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "        df_collect.append(result)       \n",
    "    df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "    df_khtst_xy_bal = kh_dataset[kh_dataset.FORMATION==off_formation][['well', 'FORMATION', 'KHtst']]\n",
    "    df_well_kh_dist_frm = df_well_kh_dist.set_index('well').join(df_khtst_xy_bal.set_index('well')).reset_index()\n",
    "    df_well_kh_dist_frm_fld = df_well_kh_dist_frm.set_index('well').join(metadata[['well','field']].set_index('well')).reset_index()\n",
    "    return df_well_kh_dist_frm_fld\n",
    "#Calculating avg properties and collecting df per FU\n",
    "def df_avg_properties(formation_avg_prop, df_avg_prop, df_joined):\n",
    "    df_hpv = df_joined[df_joined.FORMATION == formation_avg_prop][['well','FORMATION','res_tst','phit_wavg','vsh_wavg']]\n",
    "    df_permh = df_avg_prop[df_avg_prop.FORMATION == formation_avg_prop].groupby(['well','FORMATION'])['kavg_htst'].sum().reset_index()\n",
    "    df_phhpv = df_hpv.set_index(['well','FORMATION']).join(df_permh.set_index(['well','FORMATION'])).reset_index()\n",
    "    return df_phhpv\n",
    "#Rotate x,y around xo,yo by theta (rad)\n",
    "def rotate(x,y): \n",
    "    theta = (math.pi/180)*34\n",
    "    xo = st.median(np.array(df_khtst_xy['X'].to_list()))\n",
    "    yo = st.median(np.array(df_khtst_xy['Y'].to_list()))\n",
    "    xr = math.cos(theta)*(x-xo)-math.sin(theta)*(y-yo) + xo\n",
    "    yr = math.sin(theta)*(x-xo)+math.cos(theta)*(y-yo) + yo\n",
    "    return [xr,yr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_train_test_split(train_dataset):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','kavg_htst']])\n",
    "    x = np.array(train_dataset.drop('kavg_htst', axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop('kavg_htst', axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    x_train_wells = x_train_init[:,1]\n",
    "    x_test_wells = x_test_init[:,1]\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_train_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    grid_rfr = RandomForestRegressor()\n",
    "    grid_param_rfr = {\n",
    "                        'bootstrap': [True],\n",
    "                        'max_depth': [2,10,40,100],\n",
    "                        'min_samples_leaf': [1,2,5,10],\n",
    "                        'min_samples_split': [1,2,5,10],\n",
    "                        'n_estimators': [50,75,125]}\n",
    "    scorer = make_scorer(r2, greater_is_better=True)\n",
    "    grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = grid_param_rfr, scoring=scorer, cv = 15)\n",
    "    grid_calc_rfr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_rfr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    # Applying Pipeline for ML-model\n",
    "    rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(   bootstrap= gd_sr_setting['bootstrap'], \n",
    "                                                                                max_depth=gd_sr_setting['max_depth'],\n",
    "                                                                                min_samples_leaf=gd_sr_setting['min_samples_leaf'], \n",
    "                                                                                min_samples_split=gd_sr_setting['min_samples_split'], \n",
    "                                                                                n_estimators=gd_sr_setting['n_estimators']))])\n",
    "    rfr.fit(x_train, y_train)\n",
    "    y_pred_train = rfr.predict(x_train)\n",
    "    y_pred_test = rfr.predict(x_test)\n",
    "    # Reporting\n",
    "    df_rfr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_rfr_train['l_limit'] = df_rfr_train.actual*0.75\n",
    "    df_rfr_train['h_limit'] = df_rfr_train.actual*1.25\n",
    "    df_rfr_train['qc'] = 'out'\n",
    "    df_rfr_train['dataset'] = 'train'\n",
    "    df_rfr_train.loc[(df_rfr_train.predict >= df_rfr_train.l_limit) & (df_rfr_train.predict <= df_rfr_train.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_rfr_test['l_limit'] = df_rfr_test.actual*0.75\n",
    "    df_rfr_test['h_limit'] = df_rfr_test.actual*1.25\n",
    "    df_rfr_test['qc'] = 'out'\n",
    "    df_rfr_test['dataset'] = 'test'\n",
    "    df_rfr_test.loc[(df_rfr_test.predict >= df_rfr_test.l_limit) & (df_rfr_test.predict <= df_rfr_test.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_result = pd.concat([df_rfr_train,df_rfr_test])\n",
    "\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in':      df_rfr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict, 'grid_search':grids_setting_list, 'result_df':df_gbr_result}\n",
    "def rfr_train_test_split_iter(train_dataset,target_feature):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well',target_feature]])\n",
    "    x = np.array(train_dataset.drop(target_feature, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop(target_feature, axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    x_train_wells = x_train_init[:,1]\n",
    "    x_test_wells = x_test_init[:,1]\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_train_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    grid_rfr = RandomForestRegressor()\n",
    "    grid_param_rfr = {\n",
    "                        'bootstrap': [True],\n",
    "                        'max_depth': [10],\n",
    "                        'min_samples_leaf': [2],\n",
    "                        'min_samples_split': [5],\n",
    "                        'n_estimators': [50]}\n",
    "    # scorer = make_scorer(r2, greater_is_better=True)\n",
    "    # grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = grid_param_rfr, scoring=scorer, cv = 15)\n",
    "    # grid_calc_rfr.fit(x_train, y_train)\n",
    "    # gd_sr_setting = grid_calc_rfr.best_params_\n",
    "    # grids_setting_list.append(gd_sr_setting)\n",
    "    # Applying Pipeline for ML-model\n",
    "    rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(   bootstrap= grid_param_rfr['bootstrap'][0], \n",
    "                                                                                max_depth=grid_param_rfr['max_depth'][0],\n",
    "                                                                                min_samples_leaf=grid_param_rfr['min_samples_leaf'][0], \n",
    "                                                                                min_samples_split=grid_param_rfr['min_samples_split'][0], \n",
    "                                                                                n_estimators=grid_param_rfr['n_estimators'][0]))])\n",
    "    rfr.fit(x_train, y_train)\n",
    "    y_pred_train = rfr.predict(x_train)\n",
    "    y_pred_test = rfr.predict(x_test)\n",
    "    # Reporting\n",
    "    df_rfr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_rfr_train['l_limit'] = df_rfr_train.actual*0.75\n",
    "    df_rfr_train['h_limit'] = df_rfr_train.actual*1.25\n",
    "    df_rfr_train['qc'] = 'out'\n",
    "    df_rfr_train['dataset'] = 'train'\n",
    "    df_rfr_train.loc[(df_rfr_train.predict >= df_rfr_train.l_limit) & (df_rfr_train.predict <= df_rfr_train.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_rfr_test['l_limit'] = df_rfr_test.actual*0.75\n",
    "    df_rfr_test['h_limit'] = df_rfr_test.actual*1.25\n",
    "    df_rfr_test['qc'] = 'out'\n",
    "    df_rfr_test['dataset'] = 'test'\n",
    "    df_rfr_test.loc[(df_rfr_test.predict >= df_rfr_test.l_limit) & (df_rfr_test.predict <= df_rfr_test.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_result = pd.concat([df_rfr_train,df_rfr_test])\n",
    "\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in':      df_rfr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict, 'grid_search':grids_setting_list, 'result_df':df_rfr_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svr_train_test_split(train_dataset):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','kavg_htst']])\n",
    "    x = np.array(train_dataset.drop('kavg_htst', axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop('kavg_htst', axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    x_train_wells = x_train_init[:,1]\n",
    "    x_test_wells = x_test_init[:,1]\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_train_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    grid_svr = SVR()\n",
    "    grid_param_svr = {'kernel' : (['rbf']),\n",
    "                    'C' : [10, 100, 500, 1000, 2000, 3000],\n",
    "                    'gamma':[0.005, 0.01, 0.5],\n",
    "                    'epsilon': [0.001,0.01, 1, 5]}\n",
    "    scorer = make_scorer(r2, greater_is_better=True)\n",
    "    grid_calc_svr = GridSearchCV(estimator = grid_svr, param_grid = grid_param_svr, scoring=scorer, cv = 15)\n",
    "    grid_calc_svr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_svr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    # Applying Pipeline for ML-model\n",
    "    svr = Pipeline([(\"scaler\",StandardScaler()),(\"svr\",SVR( kernel = 'rbf', \n",
    "                                                            C=gd_sr_setting['C'], \n",
    "                                                            gamma = gd_sr_setting['gamma'], \n",
    "                                                            epsilon=gd_sr_setting['epsilon']))])\n",
    "    svr.fit(x_train, y_train)\n",
    "    y_pred_train = svr.predict(x_train)\n",
    "    y_pred_test = svr.predict(x_test)\n",
    "    # Reporting\n",
    "    df_svr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_svr_train['l_limit'] = df_svr_train.actual*0.75\n",
    "    df_svr_train['h_limit'] = df_svr_train.actual*1.25\n",
    "    df_svr_train['qc'] = 'out'\n",
    "    df_svr_train['dataset'] = 'train'\n",
    "    df_svr_train.loc[(df_svr_train.predict >= df_svr_train.l_limit) & (df_svr_train.predict <= df_svr_train.h_limit), 'qc'] = 'in'\n",
    "    df_svr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_svr_test['l_limit'] = df_svr_test.actual*0.75\n",
    "    df_svr_test['h_limit'] = df_svr_test.actual*1.25\n",
    "    df_svr_test['qc'] = 'out'\n",
    "    df_svr_test['dataset'] = 'test'\n",
    "    df_svr_test.loc[(df_svr_test.predict >= df_svr_test.l_limit) & (df_svr_test.predict <= df_svr_test.h_limit), 'qc'] = 'in'\n",
    "    df_svr_result = pd.concat([df_svr_train,df_svr_test])\n",
    "\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in':      df_svr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict, 'grid_search':grids_setting_list, 'result_df':df_gbr_result}\n",
    "def svr_train_test_split_iter(train_dataset,target_feature):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset = df_allf\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well',target_feature]])\n",
    "    x = np.array(train_dataset.drop(target_feature, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop(target_feature, axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    x_train_wells = x_train_init[:,1]\n",
    "    x_test_wells = x_test_init[:,1]\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_train_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    # 'grid_search': [{'C': 3000, 'epsilon': 5, 'gamma': 0.005, 'kernel': 'rbf'}]\n",
    "    grid_svr = SVR()\n",
    "    grid_param_svr = {  'C' : [3000],\n",
    "                        'gamma':[0.005],\n",
    "                        'epsilon': [5]}\n",
    "    # Applying Pipeline for ML-model\n",
    "    svr = Pipeline([(\"scaler\",StandardScaler()),(\"svr\", SVR( kernel = 'rbf', \n",
    "                                                            C=grid_param_svr['C'][0], \n",
    "                                                            gamma = grid_param_svr['gamma'][0], \n",
    "                                                            epsilon=grid_param_svr['epsilon'][0]))])\n",
    "    svr.fit(x_train, y_train)\n",
    "    y_pred_train = svr.predict(x_train)\n",
    "    y_pred_test = svr.predict(x_test)\n",
    "    # Reporting\n",
    "    df_svr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_svr_train['l_limit'] = df_svr_train.actual*0.75\n",
    "    df_svr_train['h_limit'] = df_svr_train.actual*1.25\n",
    "    df_svr_train['qc'] = 'out'\n",
    "    df_svr_train['dataset'] = 'train'\n",
    "    df_svr_train.loc[(df_svr_train.predict >= df_svr_train.l_limit) & (df_svr_train.predict <= df_svr_train.h_limit), 'qc'] = 'in'\n",
    "    df_svr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_svr_test['l_limit'] = df_svr_test.actual*0.75\n",
    "    df_svr_test['h_limit'] = df_svr_test.actual*1.25\n",
    "    df_svr_test['qc'] = 'out'\n",
    "    df_svr_test['dataset'] = 'test'\n",
    "    df_svr_test.loc[(df_svr_test.predict >= df_svr_test.l_limit) & (df_svr_test.predict <= df_svr_test.h_limit), 'qc'] = 'in'\n",
    "    df_svr_result = pd.concat([df_svr_train,df_svr_test])\n",
    "\n",
    "    metrics_dict_svr = {'r2_train':r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test': r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test': mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in': df_svr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict_svr, 'result_df':df_svr_result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbr_train_test_split(train_dataset):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','kavg_htst']])\n",
    "    x = np.array(train_dataset.drop('kavg_htst', axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop('kavg_htst', axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_test_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    grid_gbr = GradientBoostingRegressor()\n",
    "    grid_param_gbr = {  \"n_estimators\": [1, 5, 10, 50],\n",
    "                        \"max_depth\": [1, 10, 50, 100],\n",
    "                        \"learning_rate\": [0.001, 0.1, 1],\n",
    "                        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "                        \"random_state\": [42, 90]}\n",
    "    scorer = make_scorer(r2, greater_is_better=True)\n",
    "    grid_calc_gbr = GridSearchCV(estimator = grid_gbr, param_grid = grid_param_gbr, scoring=scorer, cv = 5)\n",
    "    grid_calc_gbr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_gbr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    # Applying for ML-model\n",
    "    gbr = Pipeline([(\"scaler\",StandardScaler()),(\"gbr\",GradientBoostingRegressor(   n_estimators = gd_sr_setting['n_estimators'], \n",
    "                                                                                    max_depth = gd_sr_setting['max_depth'],  \n",
    "                                                                                    learning_rate = gd_sr_setting['learning_rate'], \n",
    "                                                                                    max_features = gd_sr_setting['max_features'],\n",
    "                                                                                    random_state = gd_sr_setting['random_state']))]) \n",
    "    gbr.fit(x_train, y_train)\n",
    "    y_pred_train = gbr.predict(x_train)\n",
    "    y_pred_test = gbr.predict(x_test)\n",
    "    # Reporting\n",
    "    df_gbr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_gbr_train['l_limit'] = df_gbr_train.actual*0.75\n",
    "    df_gbr_train['h_limit'] = df_gbr_train.actual*1.25\n",
    "    df_gbr_train['qc'] = 'out'\n",
    "    df_gbr_train['dataset'] = 'train'\n",
    "    df_gbr_train.loc[(df_gbr_train.predict >= df_gbr_train.l_limit) & (df_gbr_train.predict <= df_gbr_train.h_limit), 'qc'] = 'in'\n",
    "    df_gbr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_gbr_test['l_limit'] = df_gbr_test.actual*0.75\n",
    "    df_gbr_test['h_limit'] = df_gbr_test.actual*1.25\n",
    "    df_gbr_test['qc'] = 'out'\n",
    "    df_gbr_test['dataset'] = 'test'\n",
    "    df_gbr_test.loc[(df_gbr_test.predict >= df_gbr_test.l_limit) & (df_gbr_test.predict <= df_gbr_test.h_limit), 'qc'] = 'in'\n",
    "    df_gbr_result = pd.concat([df_gbr_train,df_gbr_test])\n",
    "\n",
    "    metrics_dict = {    'r2_train':r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test': r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test': mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in': df_gbr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict, 'grid_search':grids_setting_list, 'result_df':df_gbr_result} \n",
    "def gbr_train_test_split_iters(train_dataset, target_feature):\n",
    "    \"\"\"\n",
    "    'train_ds', 'metrics: r2_train, r2_test, mae_train, mae_test, test_in', 'grid_search', 'result_df'\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well',target_feature]])\n",
    "    x = np.array(train_dataset.drop(target_feature, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    train_dataset_list.append(train_dataset.drop(target_feature, axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    y_train_wells = y_train_init[:,0]\n",
    "    y_test_wells = y_test_init[:,0]\n",
    "    x_train = x_train_init[:,1:]\n",
    "    x_test = x_test_init[:,1:]\n",
    "    y_train = y_train_init[:,1]\n",
    "    y_test = y_test_init[:,1]\n",
    "    # GridSearch for ML-model\n",
    "    grid_gbr = GradientBoostingRegressor()\n",
    "    grid_param_gbr = {  \"n_estimators\": [50],\n",
    "                        \"max_depth\": [10],\n",
    "                        \"learning_rate\": [0.1],\n",
    "                        \"max_features\": [\"sqrt\"],\n",
    "                        \"random_state\": [42]}\n",
    "    gbr = Pipeline([(\"scaler\",StandardScaler()),(\"gbr\",GradientBoostingRegressor(   n_estimators = grid_param_gbr['n_estimators'][0], \n",
    "                                                                                    max_depth = grid_param_gbr['max_depth'][0],  \n",
    "                                                                                    learning_rate = grid_param_gbr['learning_rate'][0], \n",
    "                                                                                    max_features = grid_param_gbr['max_features'][0],\n",
    "                                                                                    random_state = grid_param_gbr['random_state'][0]))]) \n",
    "    gbr.fit(x_train, y_train)\n",
    "    y_pred_train = gbr.predict(x_train)\n",
    "    y_pred_test = gbr.predict(x_test)\n",
    "    \n",
    "    # QC of predicted values for train & test datasets\n",
    "    df_gbr_train = pd.DataFrame(zip(y_train_wells, y_train, y_pred_train), columns=['well', 'actual','predict'])\n",
    "    df_gbr_train['l_limit'] = df_gbr_train.actual*0.75\n",
    "    df_gbr_train['h_limit'] = df_gbr_train.actual*1.25\n",
    "    df_gbr_train['qc'] = 'out'\n",
    "    df_gbr_train['dataset'] = 'train'\n",
    "    df_gbr_train.loc[(df_gbr_train.predict >= df_gbr_train.l_limit) & (df_gbr_train.predict <= df_gbr_train.h_limit), 'qc'] = 'in'\n",
    "    df_gbr_test = pd.DataFrame(zip(y_test_wells, y_test, y_pred_test), columns=['well', 'actual','predict'])\n",
    "    df_gbr_test['l_limit'] = df_gbr_test.actual*0.75\n",
    "    df_gbr_test['h_limit'] = df_gbr_test.actual*1.25\n",
    "    df_gbr_test['qc'] = 'out'\n",
    "    df_gbr_test['dataset'] = 'test'\n",
    "    df_gbr_test.loc[(df_gbr_test.predict >= df_gbr_test.l_limit) & (df_gbr_test.predict <= df_gbr_test.h_limit), 'qc'] = 'in'\n",
    "    df_gbr_result = pd.concat([df_gbr_train,df_gbr_test])\n",
    "\n",
    "    metrics_dict = {    'r2_train':r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test': r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test': mae(y_test, y_pred_test).round(2),\n",
    "                        'test_in': df_gbr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    return {'train_ds':train_dataset_list[0], 'metrics':metrics_dict, 'result_df':df_gbr_result} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting wells without outliers\n",
    "well_clean_8a = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII')].well \n",
    "well_clean_8 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII sand')].well\n",
    "well_clean_10a = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany X')].well\n",
    "well_clean_10 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany X sand')].well\n",
    "well_clean_10_20 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany X 20')].well\n",
    "well_clean_10_40 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany X 40')].well\n",
    "well_clean_8_25 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII 25')].well\n",
    "well_clean_8_20 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII 20')].well\n",
    "well_clean_8_15 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII 15')].well\n",
    "well_clean_8_10 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII 10')].well\n",
    "well_clean_8_5 = df_khtst_bal_qcl[(df_khtst_bal_qcl.FORMATION == 'Balakhany VIII 5')].well\n",
    "# Distance calculation\n",
    "qc_n = 1\n",
    "dist_bal8a = well_dist_calc('Balakhany VIII', qc_n, well_clean_8a)\n",
    "dist_bal8 = well_dist_calc('Balakhany VIII sand', qc_n, well_clean_8)\n",
    "dist_bal10a = well_dist_calc('Balakhany X', qc_n, well_clean_10a)\n",
    "dist_bal10 = well_dist_calc('Balakhany X sand', qc_n, well_clean_10)\n",
    "dist_bal10_20 = well_dist_calc('Balakhany X 20', qc_n, well_clean_10_20)\n",
    "dist_bal10_40 = well_dist_calc('Balakhany X 40', qc_n, well_clean_10_40)\n",
    "dist_bal8_25 = well_dist_calc('Balakhany VIII 25', qc_n, well_clean_8_25)\n",
    "dist_bal8_20 = well_dist_calc('Balakhany VIII 20', qc_n, well_clean_8_20)\n",
    "dist_bal8_15 = well_dist_calc('Balakhany VIII 15', qc_n, well_clean_8_15)\n",
    "dist_bal8_10 = well_dist_calc('Balakhany VIII 10', qc_n, well_clean_8_10)\n",
    "dist_bal8_5 = well_dist_calc('Balakhany VIII 5', qc_n, well_clean_8_5)\n",
    "# Creating datasets according to FU\n",
    "df_well_kh_dist_bal8a_fld = well_offsets_collection(dist_bal8a, 'Balakhany VIII', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_fld = well_offsets_collection(dist_bal8, 'Balakhany VIII sand', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_25_fld = well_offsets_collection(dist_bal8_25, 'Balakhany VIII 25', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_20_fld = well_offsets_collection(dist_bal8_20, 'Balakhany VIII 20', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_15_fld = well_offsets_collection(dist_bal8_15, 'Balakhany VIII 15', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_10_fld = well_offsets_collection(dist_bal8_10, 'Balakhany VIII 10', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal8_5_fld = well_offsets_collection(dist_bal8_5, 'Balakhany VIII 5', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal10a_fld = well_offsets_collection(dist_bal10a, 'Balakhany X', df_khtst_xy, metadata)                                   \n",
    "df_well_kh_dist_bal10_fld = well_offsets_collection(dist_bal10, 'Balakhany X sand', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal10_40_fld = well_offsets_collection(dist_bal10_40, 'Balakhany X 40', df_khtst_xy, metadata)\n",
    "df_well_kh_dist_bal10_20_fld = well_offsets_collection(dist_bal10_20, 'Balakhany X 20', df_khtst_xy, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenation all dist-datasets                                                  \n",
    "df_well_kh_dist_all = pd.concat([   df_well_kh_dist_bal8a_fld,\n",
    "                                    df_well_kh_dist_bal8_fld,    \n",
    "                                    df_well_kh_dist_bal8_25_fld,\n",
    "                                    df_well_kh_dist_bal8_20_fld,\n",
    "                                    df_well_kh_dist_bal8_15_fld,\n",
    "                                    df_well_kh_dist_bal8_10_fld, \n",
    "                                    df_well_kh_dist_bal8_5_fld,\n",
    "                                    df_well_kh_dist_bal10a_fld,                                    \n",
    "                                    df_well_kh_dist_bal10_fld,\n",
    "                                    df_well_kh_dist_bal10_40_fld,\n",
    "                                    df_well_kh_dist_bal10_20_fld])\n",
    "df_well_kh_dist_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation of TST-thickness Balakhany VIII & X\n",
    "df_fu_tst = df_prq[(df_prq.FORMATION.str.contains('Balakhany VIII')) | (df_prq.FORMATION.str.contains('Balakhany X'))]\n",
    "df_fu_tst = df_fu_tst[['well', 'MD','FORMATION','TST']]\n",
    "df_fu_tst_top = df_fu_tst.groupby(['well','FORMATION'])['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "df_fu_tst_top.rename(columns={'TST':'TST_top'}, inplace=True)\n",
    "df_fu_tst_bot = df_fu_tst.groupby(['well','FORMATION'])['TST'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "df_fu_tst_bot.rename(columns={'TST':'TST_bot'}, inplace=True)\n",
    "df_fu_tst_final = df_fu_tst_top.set_index(['well','FORMATION']).join(df_fu_tst_bot.set_index(['well','FORMATION'])).reset_index()\n",
    "df_fu_tst_final['TST_interv'] = round((df_fu_tst_final.TST_bot - df_fu_tst_final.TST_top),0)\n",
    "#Joining coordinates datasets\n",
    "df_fu_tst_final = df_fu_tst_final.set_index(['well','FORMATION']).join(xy_coord.set_index(['well','FORMATION'])).reset_index()\n",
    "df_fu_tst_final = df_fu_tst_final.set_index(['well', 'FORMATION']).join(df_prq_tvdss.set_index(['well','FORMATION'])).reset_index()\n",
    "df_fu_tst_final = df_fu_tst_final.set_index('well').join(df_prq.groupby('well')['field'].apply(lambda x: x.iloc[0])).reset_index()\n",
    "df_fu_tst_final = df_fu_tst_final[(df_fu_tst_final.TST_interv > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading df_prq_htst_avgprop_v1 and getting outliers\n",
    "path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\' \n",
    "df_htst_avgprop = pd.read_csv(path + 'df_prq_htst_avgprop_v1.csv')\n",
    "#Preparation weighted average df_htst_avgprop-dataset\n",
    "cutoff_h_tst = -1\n",
    "cutoff_perm_avg = -1\n",
    "#Applying filtration to dataset with cutoffs\n",
    "df_htst_avgprop_nz = df_htst_avgprop[(df_htst_avgprop.h_tst > cutoff_h_tst) & (df_htst_avgprop.md_perm_avg > cutoff_perm_avg)]\n",
    "#Multiplaying htst by resprop values\n",
    "df_htst_avgprop_nz['kavg_htst'] = df_htst_avgprop_nz.h_tst * df_htst_avgprop_nz.md_perm_avg\n",
    "df_htst_avgprop_nz['phit_htst'] = df_htst_avgprop_nz.h_tst * df_htst_avgprop_nz.md_phit_avg\n",
    "df_htst_avgprop_nz['vsh_htst'] = df_htst_avgprop_nz.h_tst * df_htst_avgprop_nz.md_vsh_avg\n",
    "#Summarizing h_tst via well & formation\n",
    "df_htst_fm = df_htst_avgprop_nz.groupby(['well','FORMATION'])['h_tst'].sum().reset_index()\n",
    "df_htst_fm.rename(columns={'h_tst':'res_tst'}, inplace=True)\n",
    "#Calculating weighted averages\n",
    "df_htst_avgprop_nz_avgpropsum = df_htst_avgprop_nz.groupby(['well','FORMATION'])[['phit_htst','vsh_htst']].sum().reset_index()\n",
    "df_htst_avgprop_nz_avgpropsum_join = df_htst_avgprop_nz_avgpropsum.set_index(\n",
    "                                     ['well','FORMATION']).join(df_htst_fm.set_index(['well','FORMATION'])).reset_index()\n",
    "df_htst_avgprop_nz_avgpropsum_join['phit_wavg'] = df_htst_avgprop_nz_avgpropsum_join.phit_htst / df_htst_avgprop_nz_avgpropsum_join.res_tst\n",
    "df_htst_avgprop_nz_avgpropsum_join['vsh_wavg'] = df_htst_avgprop_nz_avgpropsum_join.vsh_htst / df_htst_avgprop_nz_avgpropsum_join.res_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating avg properties and collecting df per FU\n",
    "df_8abal_phhpv = df_avg_properties('Balakhany VIII', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_phhpv = df_avg_properties('Balakhany VIII sand', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_10abal_phhpv = df_avg_properties('Balakhany X', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_10bal_phhpv = df_avg_properties('Balakhany X sand', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_10bal_20_phhpv = df_avg_properties('Balakhany X 20', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_10bal_40_phhpv = df_avg_properties('Balakhany X 40', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_25_phhpv = df_avg_properties('Balakhany VIII 25', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_20_phhpv = df_avg_properties('Balakhany VIII 20', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_15_phhpv = df_avg_properties('Balakhany VIII 15', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_10_phhpv = df_avg_properties('Balakhany VIII 10', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "df_8bal_5_phhpv = df_avg_properties('Balakhany VIII 5', df_htst_avgprop_nz, df_htst_avgprop_nz_avgpropsum_join)\n",
    "#Concatenation of all df per FU\n",
    "df_all_bal_phhpv = pd.concat([  df_8abal_phhpv, df_8bal_phhpv, df_10abal_phhpv, df_10bal_phhpv, df_10bal_20_phhpv, df_10bal_40_phhpv, \n",
    "                                df_8bal_25_phhpv, df_8bal_20_phhpv, df_8bal_15_phhpv, df_8bal_10_phhpv, df_8bal_5_phhpv])\n",
    "df_all_bal_phhpv_tstint = df_all_bal_phhpv.set_index(['well','FORMATION']).join(df_fu_tst_final.set_index(['well','FORMATION'])).reset_index()\n",
    "df_avgprop_final_wa = df_all_bal_phhpv_tstint.copy()\n",
    "df_avgprop_final_wa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Shahriyar\n",
    "perm_cutoff = -1\n",
    "perm_cutoff_top = 140_000\n",
    "azr_lst = ['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI', 'CHIRAG']\n",
    "chg_lst = [ 'DWG', 'DDGG', 'WEST CHIRAG']\n",
    "df_dist_kh_bal_shahriayr_init = df_avgprop_final_wa.set_index(['well','FORMATION']).join(\n",
    "                                df_well_kh_dist_all.drop('field',axis=1).set_index(['well','FORMATION'])).reset_index()\n",
    "df_dist_kh_bal_shahriayr_init.rename(columns={'TST_interv':'interv_tst', 'TST_top':'tst_top', 'TST_bot':'tst_bot'}, inplace=True)\n",
    "df_dist_kh_bal_shahriayr = df_dist_kh_bal_shahriayr_init[df_dist_kh_bal_shahriayr_init.MD.notna()]\n",
    "df_dist_kh_bal_shahriayr = df_dist_kh_bal_shahriayr[ (df_dist_kh_bal_shahriayr.kh1>perm_cutoff) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kh2>perm_cutoff) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kh3>perm_cutoff) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kavg_htst>perm_cutoff)]\n",
    "df_dist_kh_bal_shahriayr = df_dist_kh_bal_shahriayr[ (df_dist_kh_bal_shahriayr.kh1<perm_cutoff_top) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kh2<perm_cutoff_top) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kh3<perm_cutoff_top) &\n",
    "                                                     (df_dist_kh_bal_shahriayr.kavg_htst<perm_cutoff_top)]\n",
    "df_dist_kh_bal_shahriayr.loc[df_dist_kh_bal_shahriayr.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "df_dist_kh_bal_shahriayr.loc[df_dist_kh_bal_shahriayr.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "df_dist_kh_bal_shahriayr.loc[df_dist_kh_bal_shahriayr.field.isin(azr_lst), 'fields_zone'] = 'azeri'\n",
    "df_dist_kh_bal_shahriayr.loc[df_dist_kh_bal_shahriayr.field.isin(chg_lst), 'fields_zone'] = 'chirag'\n",
    "df_dist_kh_bal_shahriayr['FORMATION_up_orig'] = df_dist_kh_bal_shahriayr['FORMATION_up']\n",
    "df_dist_kh_bal_shahriayr['kavg_htst_lg10'] = np.log10(df_dist_kh_bal_shahriayr['kavg_htst'])\n",
    "df_dist_kh_bal_shahriayr['kh1_lg10'] = np.log10(df_dist_kh_bal_shahriayr['kh1'])\n",
    "df_dist_kh_bal_shahriayr['kh2_lg10'] = np.log10(df_dist_kh_bal_shahriayr['kh2'])\n",
    "df_dist_kh_bal_shahriayr['kh3_lg10'] = np.log10(df_dist_kh_bal_shahriayr['kh3'])\n",
    "df_dist_kh_bal_shahriayr[['X_new', 'Y_new']] = df_dist_kh_bal_shahriayr.apply(lambda row: rotate(row['X'], row['Y']), axis=1, result_type='expand')\n",
    "df_dist_kh_bal_shahriayr = pd.get_dummies(df_dist_kh_bal_shahriayr, columns = ['FORMATION_up'])\n",
    "# df_dist_kh_bal_shahriayr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dist_kh_bal_shahriayr_init.to_csv('df_all_features_nofilters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xplots depiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generation\n",
    "df_allf_xplot = df_dist_kh_bal_shahriayr[[    \n",
    "                                        # 'well', \n",
    "                                        'kavg_htst', \n",
    "                                        'interv_tst','res_tst', 'vsh_wavg',\n",
    "                                        'dist1', 'dist2', 'dist3',\n",
    "                                        'phit_wavg',  \n",
    "                                        'kh1', 'kh2', 'kh3', \n",
    "                                        'X', 'Y', \n",
    "                                        'MD', 'TVD_SCS',\n",
    "                                        # 'FORMATION_up_orig', 'fields_zone', 'field',\n",
    "                                        # 'x1', 'x2', 'x3', 'y1', 'y2', 'y3',\n",
    "                                        # 'well1', 'well2', 'well3', 'KHtst', 'kavg_htst', \n",
    "                                        # 'kavg_htst_lg10', \n",
    "                                        # 'kh1_lg10', 'kh2_lg10', 'kh3_lg10', 'X_new', 'Y_new',\n",
    "                                        # 'FORMATION_up_Balakhany VIII', 'FORMATION_up_Balakhany X',\n",
    "                                        ]]\n",
    "df_allf_xplot = df_allf_xplot[df_allf_xplot.phit_wavg.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corr heatmap\n",
    "f, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.heatmap(df_allf_xplot.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplot\n",
    "df_allf_xplot_v2 = df_dist_kh_bal_shahriayr[[    \n",
    "                                        # 'well',              \n",
    "                                        'interv_tst','res_tst', 'vsh_wavg',\n",
    "                                        'dist1', 'dist2', 'dist3',\n",
    "                                        'phit_wavg',  \n",
    "                                        'kh1', 'kh2', 'kh3',\n",
    "                                        'kavg_htst',  \n",
    "                                        # 'MD', 'TVD_SCS',\n",
    "                                        'fields_zone',]]\n",
    "sns.pairplot(df_allf_xplot_v2, hue='fields_zone',plot_kws=dict(alpha=0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xplot\n",
    "sns.scatterplot(df_allf_xplot_v2, y='res_tst', x='interv_tst', hue='fields_zone');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 70/30 splits testing polygon for different ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allf = df_dist_kh_bal_shahriayr[[    \n",
    "                                        'well', \n",
    "                                        'kavg_htst', \n",
    "                                        'interv_tst','res_tst', 'vsh_wavg',\n",
    "                                        'dist1', 'dist2', 'dist3',\n",
    "                                        'fields_zone',\n",
    "                                        # 'field',\n",
    "                                        'kh1', 'kh2', 'kh3', \n",
    "                                        'X', 'Y', \n",
    "                                        'MD', 'TVD_SCS',\n",
    "                                        # 'FORMATION_up_orig', \n",
    "                                        'x1', 'x2', 'x3', 'y1', 'y2', 'y3',\n",
    "                                        # 'well1', 'well2', 'well3', 'KHtst', 'kavg_htst', \n",
    "                                        # 'kavg_htst_lg10', \n",
    "                                        # 'kh1_lg10', 'kh2_lg10', 'kh3_lg10', 'X_new', 'Y_new',\n",
    "                                        'FORMATION_up_Balakhany VIII', 'FORMATION_up_Balakhany X',\n",
    "                                        'phit_wavg',  \n",
    "                                        ]]\n",
    "df_allf = df_allf[df_allf.phit_wavg.notna()]\n",
    "# df_allf_v2 = df_allf\n",
    "df_allf_v2 = pd.get_dummies(df_allf, columns = ['fields_zone'])\n",
    "# df_allf = df_allf[(df_allf.kh1_lg10>0) & (df_allf.kh2_lg10>0) & (df_allf.kh3_lg10>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBR run\n",
    "report_gbr_list = []\n",
    "targ_feat = 'kavg_htst'\n",
    "out_list = ['well', targ_feat]\n",
    "for i in df_allf_v2.columns[2:]:\n",
    "    out_list.append(i)\n",
    "    gbr_iter = gbr_train_test_split_iters(df_allf_v2[out_list], targ_feat)\n",
    "    report_gbr_list.append((i, gbr_iter['metrics']['test_in'], gbr_iter['metrics']['r2_test']))\n",
    "report_gbr = pd.DataFrame(report_gbr_list, columns=['feature','qc_in','r2_test'])\n",
    "fig_rep = px.line(report_gbr, x='feature', y='qc_in')\n",
    "fig_r2 = px.line(report_gbr, x='feature', y='r2_test')\n",
    "fig_r2.update_traces(line=dict(color = 'red', dash='dash'))\n",
    "fig_final = go.Figure(data = fig_rep.data + fig_r2.data)\n",
    "fig_final.update_layout(title='Step by step adding new feature GBR, red = f(R2), blue = f(predicted_kh)', width=1300,height=400, margin=dict(l=10,r=10,b=10,t=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SVR run - need to be repaired!!!\n",
    "# targ_feat = 'kavg_htst'\n",
    "# out_list = ['well', targ_feat]\n",
    "# report_svr_list = []\n",
    "# for i in df_allf_v2.columns[2:]:\n",
    "#     out_list.append(i)\n",
    "#     svr_iter = svr_train_test_split_iter(df_allf_v2[out_list], targ_feat)\n",
    "#     report_svr_list.append((i, svr_iter['metrics']['test_in'], svr_iter['metrics']['r2_test']))\n",
    "# report_svr = pd.DataFrame(report_svr_list, columns=['feature','qc_in','r2_test'])\n",
    "# fig_rep = px.line(report_svr, x='feature', y='qc_in')\n",
    "# fig_r2 = px.line(report_svr, x='feature', y='r2_test')\n",
    "# fig_r2.update_traces(line=dict(color = 'red', dash='dash'))\n",
    "# fig_final = go.Figure(data = fig_rep.data + fig_r2.data)\n",
    "# fig_final.update_layout(title='Step by step adding new feature SVR, red = f(R2), blue = f(predicted_kh)', width=1300,height=400, margin=dict(l=10,r=10,b=10,t=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFR run\n",
    "targ_feat = 'kavg_htst'\n",
    "out_list = ['well', targ_feat]\n",
    "report_rfr_list = []\n",
    "for i in df_allf_v2.columns[2:]:\n",
    "    out_list.append(i)\n",
    "    rfr_iter = rfr_train_test_split_iter(df_allf_v2[out_list], targ_feat)\n",
    "    report_rfr_list.append((i, rfr_iter['metrics']['test_in'], rfr_iter['metrics']['r2_test']))\n",
    "report_rfr = pd.DataFrame(report_rfr_list, columns=['feature','qc_in', 'r2_test'])\n",
    "fig_rep = px.line(report_rfr, x='feature', y='qc_in')\n",
    "fig_r2 = px.line(report_rfr, x='feature', y='r2_test')\n",
    "fig_r2.update_traces(line=dict(color = 'red', dash='dash'))\n",
    "fig_final = go.Figure(data = fig_rep.data + fig_r2.data)\n",
    "fig_final.update_layout(title='Step by step adding new feature RFR, red = f(R2), blue = f(predicted_kh)', width=1300,height=400, margin=dict(l=10,r=10,b=10,t=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Permutation importance train set & test sets\n",
    "# result_pi_train = permutation_importance(gbr, x_train, y_train, n_repeats=20, random_state=num, n_jobs=2)\n",
    "# sorted_importances_idx_train = result_pi_train.importances_mean.argsort()\n",
    "# importances_train = pd.DataFrame(result_pi_train.importances[sorted_importances_idx_train].T,\n",
    "#                                 columns=df_dist_kh_bal_shahriayr_final.drop(['well','kavg_htst'], axis=1).columns[sorted_importances_idx_train])\n",
    "# fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "# ax[0].boxplot(importances_train, labels=importances_train.columns[sorted_importances_idx_train], vert=False, whis=10)\n",
    "# ax[0].set_title(\"Permut Imp (train set)\")\n",
    "# ax[0].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "# ax[0].set_xlabel(\"Decrease in accuracy score\")\n",
    "# # Permutation importance test set\n",
    "# result_pi_test = permutation_importance(gbr, x_test, y_test, n_repeats=20, random_state=num, n_jobs=2)\n",
    "# sorted_importances_idx_test = result_pi_test.importances_mean.argsort()\n",
    "# importances_test = pd.DataFrame(result_pi_test.importances[sorted_importances_idx_test].T,\n",
    "#                            columns=df_dist_kh_bal_shahriayr_final.drop(['well','kavg_htst'], axis=1).columns[sorted_importances_idx_test])\n",
    "# ax[1].boxplot(importances_test, labels=importances_test.columns[sorted_importances_idx_test], vert=False, whis=10)\n",
    "# ax[1].set_title(\"Permut Imp (test set)\")\n",
    "# ax[1].axvline(x=0, color=\"k\", linestyle=\"--\")\n",
    "# ax[1].set_xlabel(\"Decrease in accuracy score\")\n",
    "# ax[1].figure.tight_layout()\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making up the x-plot for train dataset\n",
    "\n",
    "# df_gbr_total = pd.concat([df_gbr_train, df_gbr_test])\n",
    "# print('wells total:', df_gbr_train.shape[0])\n",
    "# print('wells unpredicted:', df_gbr_train['qc'].value_counts()['out'], (df_gbr_train['qc'].value_counts()['out']/df_gbr_train.shape[0]).round(3), 'v/v')\n",
    "# print('wells predicted:', df_gbr_train['qc'].value_counts()['in'], (df_gbr_train['qc'].value_counts()['in']/df_gbr_train.shape[0]).round(3), 'v/v')\n",
    "# max_val = 14000\n",
    "# fig1_ml = px.scatter(gbr_split_notphit['result_df'][gbr_split_notphit['result_df']['dataset']=='train'], x='actual', y='predict', \n",
    "#                      color='qc', \n",
    "#                      hover_data=['well'], \n",
    "#                      width=800, height=400,\n",
    "#                      color_discrete_sequence=[\"green\", \"red\"])\n",
    "# fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "# fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "# fig2_1_ml=px.line(x=[0,max_val], y=[0,max_val*1.25])\n",
    "# fig2_2_ml=px.line(x=[0,max_val], y=[0,max_val*0.75])\n",
    "# fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "# fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "# fig3_ml.update_layout(title = 'Comparison Actual vs Pred Train',width=600,height=400, xaxis_title='actual', yaxis_title='predict',\n",
    "#                       margin=dict(l=10,r=10,b=10,t=40))\n",
    "# fig3_ml.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making up the x-plot for test dataset\n",
    "# fig1_ml = px.scatter(gbr_split_notphit['result_df'][gbr_split_notphit['result_df']['dataset']=='test'], x='actual', y='predict', \n",
    "#                      color='qc', \n",
    "#                      hover_data=['well'], \n",
    "#                      width=400, height=400,\n",
    "#                      color_discrete_sequence=[\"red\", \"green\"])\n",
    "# fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "# fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "# fig2_1_ml=px.line(x=[0,max_val], y=[0,max_val*1.25])\n",
    "# fig2_2_ml=px.line(x=[0,max_val], y=[0,max_val*0.75])\n",
    "# fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "# fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "# fig3_ml.update_layout(title = 'Comparison Actual vs Pred',width=600,height=400, xaxis_title='actual', yaxis_title='predict',\n",
    "#                       margin=dict(l=10,r=10,b=10,t=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Starting of the loop\n",
    "# y_test_lst = []\n",
    "# y_pred_test_lst = []\n",
    "# well_exclude_lst = []\n",
    "# gs_settings_lst = []\n",
    "# metrics_r2_mae_lst = []\n",
    "# df_dist_kh_bal_shahriayr_gbr = df_allf_v2.sample(frac = 1).reset_index().drop('index', axis=1)\n",
    "# for i in tqdm(range(len(df_dist_kh_bal_shahriayr_gbr))):\n",
    "#     #Making up the feature and target datasets\n",
    "#     df_wo_well = df_dist_kh_bal_shahriayr_gbr.drop([i])\n",
    "#     well_exclude = df_dist_kh_bal_shahriayr_gbr.iloc[i]['well']\n",
    "#     well_exclude_lst.append(well_exclude)\n",
    "#     y_train = np.array(df_wo_well['kavg_htst_lg10'])\n",
    "#     x_train = np.array(df_wo_well[[ 'X', 'Y', 'TVD_SCS', 'interv_tst', 'res_tst', 'vsh_wavg',\n",
    "#                                     'kh1_lg10', 'kh2_lg10', 'kh3_lg10',\n",
    "#                                     'FORMATION_up_Balakhany VIII', 'FORMATION_up_Balakhany X']])\n",
    "#     well_train = np.array(df_wo_well['well'])\n",
    "#     y_test = np.array(df_dist_kh_bal_shahriayr_gbr.iloc[i]['kavg_htst_lg10'])\n",
    "#     y_test_lst.append(y_test)\n",
    "#     x_test = np.array(df_dist_kh_bal_shahriayr_gbr.iloc[i][[    'X', 'Y', 'TVD_SCS', 'interv_tst', 'res_tst', 'vsh_wavg',\n",
    "#                                                                 'kh1_lg10', 'kh2_lg10', 'kh3_lg10',\n",
    "#                                                                 'FORMATION_up_Balakhany VIII', 'FORMATION_up_Balakhany X']])\n",
    "#     # GridSearch for ML-model\n",
    "#     # {'learning_rate': 0.1, 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100, 'random_state': 90}\n",
    "#     grid_param_GBR = {\n",
    "#                         \"n_estimators\": [100],\n",
    "#                         \"max_depth\": [10],\n",
    "#                         \"learning_rate\": [0.1],\n",
    "#                         \"max_features\": [\"sqrt\"],\n",
    "#                         'random_state': [90]\n",
    "#                                     }\n",
    "#     GS_setting = grid_param_GBR\n",
    "#     gs_settings_lst.append((GS_setting['n_estimators'],\n",
    "#                             GS_setting['max_depth'], \n",
    "#                             GS_setting['learning_rate'],\n",
    "#                             GS_setting['max_features'],\n",
    "#                             GS_setting['random_state']))\n",
    "#     # Statement of ML-model\n",
    "#     gbr = Pipeline([(\"scaler\",StandardScaler()),(\"gbr\",GradientBoostingRegressor(   n_estimators = GS_setting['n_estimators'][0], \n",
    "#                                                                                     max_depth = GS_setting['max_depth'][0],  \n",
    "#                                                                                     learning_rate = GS_setting['learning_rate'][0], \n",
    "#                                                                                     max_features = GS_setting['max_features'][0],\n",
    "#                                                                                     random_state = GS_setting['random_state'][0]))])\n",
    "#     # Fitting the ML-model\n",
    "#     gbr.fit(x_train, y_train)\n",
    "#     y_pred_train = gbr.predict(x_train)\n",
    "#     y_pred_test = gbr.predict([x_test])\n",
    "#     # Block of data naturalization\n",
    "#     # y_pred_test_nat = sc_y.inverse_transform(y_pred_test.reshape(-1, 1))\n",
    "#     y_pred_test_lst.append(y_pred_test[0])\n",
    "#     # Metrics computation for the ML-model\n",
    "#     r2_train = r2(y_train, y_pred_train).round(5)\n",
    "#     mae_train = mae(y_train, y_pred_train)\n",
    "#     # mae_train_nat = sc_y.inverse_transform([[mae_train]])\n",
    "#     metrics_r2_mae_lst.append((r2_train, mae_train.round(5)))\n",
    "\n",
    "# # Building up of dataframe\n",
    "# res_gbr_sha = pd.DataFrame(zip(y_test_lst,y_pred_test_lst,well_exclude_lst, gs_settings_lst), \n",
    "#                    columns = ['test','predict','well', 'gs_setting',])\n",
    "# res_gbr_sha.test = res_gbr_sha.test.astype('float')\n",
    "# res_gbr_sha['l_test'] = np.log10((10**res_gbr_sha.test)*0.75)\n",
    "# res_gbr_sha['h_test'] = np.log10((10**res_gbr_sha.test)*1.25)\n",
    "# res_gbr_sha['qc'] = 'out'\n",
    "# res_gbr_sha.loc[(res_gbr_sha.predict >= res_gbr_sha.l_test) & (res_gbr_sha.predict <= res_gbr_sha.h_test), 'qc'] = 'in'\n",
    "# print('wells total:', res_gbr_sha.shape[0])\n",
    "# print('wells unpredicted:', res_gbr_sha['qc'].value_counts()['out'], (res_gbr_sha['qc'].value_counts()['out']/res_gbr_sha.shape[0]).round(3), 'v/v')\n",
    "# print('wells predicted:', res_gbr_sha['qc'].value_counts()['in'], (res_gbr_sha['qc'].value_counts()['in']/res_gbr_sha.shape[0]).round(3), 'v/v')\n",
    "# mae_df_xy = mae(10**res_gbr_sha.test, 10**res_gbr_sha.predict).round(5)\n",
    "# r2_df_xy = r2(res_gbr_sha.test, res_gbr_sha.predict).round(0)\n",
    "# print('mae:', mae_df_xy, 'mDm')\n",
    "# print('r2:', r2_df_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Making up the final x-plot\n",
    "# max_val = np.log10(20000)\n",
    "# fig1_ml = px.scatter(res_gbr_sha, x='test', y='predict', \n",
    "#                      color='qc', \n",
    "#                      hover_data=['well'], \n",
    "#                      width=400, height=400,\n",
    "#                      color_discrete_sequence=[\"green\", \"red\"])\n",
    "# fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "# fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "# fig2_1_ml=px.line(x=[0,max_val], y=[0,np.log10((10**max_val)*1.25)])\n",
    "# fig2_2_ml=px.line(x=[0,max_val], y=[0,np.log10((10**max_val)*0.75)])\n",
    "# fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "# fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "# fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "# fig3_ml.update_layout(title = 'Comparison Actual vs Pred Shahriyar GBR',width=600,height=400, xaxis_title='test', yaxis_title='predict',\n",
    "#                       margin=dict(l=10,r=10,b=10,t=40), xaxis_type=\"log\", yaxis_type=\"log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
