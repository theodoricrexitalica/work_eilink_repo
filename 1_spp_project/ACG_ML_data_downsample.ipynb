{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libs \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import io\n",
    "import base64\n",
    "from PIL import Image\n",
    "import statistics as st\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gmean\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as go_offline\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, mapping\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score as r2 \n",
    "from sklearn.metrics import mean_absolute_error as mae \n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from yellowbrick.regressor import PredictionError\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime\n",
    "import random\n",
    "from dash import Dash, dcc, html, Input, Output, no_update, callback\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata, distribution wells per Platforms and all the that.\n",
    "def metadata_parquet_loading():\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    metadata_init = pd.read_csv(path + 'ACG_wells_metadata.csv', sep=',')\n",
    "    metadata = metadata_init.copy()\n",
    "    metadata = metadata.rename(columns={'X':'X_wellhead', 'Y':'Y_wellhead'})\n",
    "    metadata.Status = metadata.Status.str.strip()\n",
    "    metadata.Status = metadata.Status.str.lower()\n",
    "    metadata.loc[metadata.Status == 'oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'oil producer', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'produiction oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production_oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned production oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned  oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned oi', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'injector  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injector water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injetor  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'abandoned injector - water per b', 'Status' ] = 'abandoned injector - water'\n",
    "    metadata.loc[metadata.Status == 'plugged and abandoned', 'Status' ] = 'p&a'\n",
    "    metadata.loc[metadata.X_wellhead==118.270, 'X_wellhead'] = 526258.84\n",
    "    metadata.loc[metadata.Y_wellhead==526261.510, 'Y_wellhead'] = 4435802.01\n",
    "    metadata.loc[metadata.well=='C39', 'X_wellhead'] = 526258.840\n",
    "    metadata.loc[metadata.well=='C39', 'Y_wellhead'] = 4435802.010\n",
    "    metadata.loc[metadata.field=='West Azeri', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.field=='COP', 'field'] = 'WEST CHIRAG'\n",
    "    metadata.loc[metadata.well=='AZERI2', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='AZERI3', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='B31', 'field'] = 'CENTRAL AZERI'\n",
    "    metadata.loc[metadata.well=='J28_bpQIP', 'field'] = 'WEST CHIRAG'\n",
    "\n",
    "    #Read data from parquet\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    df_prq = pd.read_parquet(path + 'ACG_wells_JOINT_BEST_v10.parquet.gzip')\n",
    "    df_prq.rename(columns={'wellName':'well'}, inplace=True)\n",
    "    df_prq = df_prq.set_index('well').join(metadata.set_index('well')).reset_index()\n",
    "    # print('wells in df totally:', len(df_prq.well.unique()))\n",
    "    # Filter data with bad_well_list \n",
    "    bad_well_list = ['E10Z','Predrill_J01Z', 'Predrill_J08', 'J28_bpQIP', 'A01W_2']\n",
    "    df_prq = df_prq[~df_prq.well.isin(bad_well_list)]\n",
    "    #Assign any Fluidcode_mod number by variable gross_pay=1 and gross_pay=0 if Fluidcode_mod as NaN\n",
    "    df_prq.loc[df_prq.FLUIDS>0, 'FLUIDS_int'] = 1\n",
    "    df_prq.loc[df_prq.FLUIDS<=0, 'FLUIDS_int'] = 0\n",
    "    df_prq.FLUIDS_int = df_prq.FLUIDS_int.astype('int')\n",
    "    # Unite of FU for each formation\n",
    "\n",
    "    df_bal = df_prq[df_prq.FORMATION.str.contains('Balakhany')]\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "    df_bal = df_bal[df_bal.FORMATION_up.notna()]\n",
    "    #Getting XY mean coords of Balakhany formation\n",
    "    xy_coord_mean = df_bal[['well', 'FORMATION_up', 'X', 'Y']]\n",
    "    xy_coord_mean = xy_coord_mean.groupby(['well', 'FORMATION_up']).agg({'X': 'mean', 'Y':'mean'}).reset_index()\n",
    "    xy_coord_mean = xy_coord_mean.rename(columns={'X':'X_mean', 'Y':'Y_mean'})\n",
    "    xy_coord_mean = xy_coord_mean[xy_coord_mean.FORMATION_up.str.contains('Balakhany') & (xy_coord_mean.X_mean>0) & (xy_coord_mean.Y_mean>0)]\n",
    "    df_bal.rename(columns={'X':'X_traj', 'Y':'Y_traj'}, inplace=True)\n",
    "    df_bal = df_bal.set_index(['well', 'FORMATION_up']).join(xy_coord_mean.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "    return df_bal\n",
    "df_bal = metadata_parquet_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display in TST well plots with logging curves\n",
    "def well_display_khtst( dataset, wellname, fmname, net_var, comments, \n",
    "                        ref_depth, fm_flag, depth_step, kh_include, print):\n",
    "    \"\"\"\n",
    "    dataset = df_bal or something else\n",
    "    net_var = NET or FLUIDS_int\n",
    "    comments = put what you want\n",
    "    ref_depth = MD or TST\n",
    "    fm_flag = 1 if you need a FORMATION_up, 0 if just a simple FORMATION\n",
    "    depth_step = step for ticks on the diagramm\n",
    "    kh_include = 1 if we have KHtst in dataset, 0 if there is not KHtst\n",
    "    print = 1 if we want to print the plot\n",
    "    \"\"\"\n",
    "    if fm_flag == 0:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION == fmname)]\n",
    "    if fm_flag == 1:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    depth = data[ref_depth]\n",
    "    grn = data['GR_N']\n",
    "    vsh = data['VSH']\n",
    "    rhob = data['RHOB'] \n",
    "    npss = data['NPSS']\n",
    "    rdeep = data['RDEEP']\n",
    "    phit = data['PHIT'] \n",
    "    net = data[net_var]\n",
    "    perm = data['LPERM']\n",
    "    if kh_include == 1:\n",
    "        kh = data['KHtst']\n",
    "    else:\n",
    "        data['KHtst'] = 0\n",
    "        kh = data['KHtst']\n",
    "    fig, ax = plt.subplots(1,4, figsize=(7,7), sharey=True)\n",
    "    well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "    ax[0].plot(grn, depth, color='lightgreen', lw=3, zorder=10)\n",
    "    ax[0].invert_yaxis() \n",
    "    ax[0].set_xlim(0, 150) \n",
    "    ax[0].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "    twin0 = ax[0].twiny()\n",
    "    twin0.plot(vsh, depth, color='black', alpha=0.5, zorder=5)\n",
    "    twin0.set_xlim(0, 1.5)\n",
    "    ax[1].plot(rhob, depth, color='red') \n",
    "    ax[1].invert_yaxis() \n",
    "    ax[1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "    ax[1].set_xlim(1.65, 2.65)\n",
    "    ax[1].grid(axis='y'), ax[1].grid(axis='x')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[1].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "        xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "        ax[1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "    twin1 = ax[1].twiny()\n",
    "    twin1.plot(npss, depth, color='blue')\n",
    "    twin1.set_xlim(0.6, 0)\n",
    "    # ax[2].plot(rdeep, depth, color='black'), ax[2].set_xscale('log'), ax[2].set_xlim(0.1, 50), ax[2].invert_yaxis(), ax[2].grid(axis='x', which='both')\n",
    "    ax[2].plot(phit, depth, color='green', linestyle='dashed'), ax[2].set_xlim(0.3, 0), ax[2].grid(axis='x') \n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].grid(axis='y')\n",
    "    ax[2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "    twin2 = ax[2].twiny()\n",
    "    twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "    twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "    twin2.set_xlim(0, 1)\n",
    "    twin2.set_ylim(min(depth), max(depth))\n",
    "    ax[3].plot(perm, depth, color='purple', alpha=0.66), ax[3].set_xscale('log'), ax[3].set_xlim(0.1, 1000)\n",
    "    ax[3].invert_yaxis()\n",
    "    ax[3].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.66)\n",
    "    twin4 = ax[3].twiny()\n",
    "    twin4.plot(kh, depth, color='black', alpha=1)\n",
    "    fig.suptitle(wellname + ' ' + fmname + ' ' + ref_depth + ' ' + str(round(max(kh.dropna()),0)) + ' ' + str(comments), fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    if print == 1:\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\wellplots\\\\'\n",
    "        fig.savefig(path + fmname.replace(' ','') + '_' + wellname + '.png')\n",
    "    else:\n",
    "        pass\n",
    "# Draw a map\n",
    "def map_value_2plots(metadata, dataset, formation, value, color, multi_chr = 0.001, multi_azr = 0.001):\n",
    "    \"\"\"\n",
    "    metadata, \n",
    "    dataset = dataset with X & Y, \n",
    "    formation = 'Balakhany VIII',  \n",
    "    value = for example 'KHtst' or 'tst_interv'\n",
    "    multi_chr = 0.001, multi_azr = 0.001\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=('crg: ' + str(multi_chr), 'azr: ' + str(multi_azr)), \n",
    "                        vertical_spacing = 0.025)\n",
    "    azr_lst = ['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']\n",
    "    chg_lst = ['CHIRAG', 'DWG', 'DDGG', 'WEST CHIRAG']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    field_avg_coord_chg = field_avg_coord[field_avg_coord.field.isin(chg_lst)]\n",
    "    field_avg_coord_azr = field_avg_coord[field_avg_coord.field.isin(azr_lst)] \n",
    "    df_chg = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(chg_lst))]\n",
    "    df_azr = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(azr_lst))]\n",
    "    fig.add_trace(go.Scatter(x=df_chg.X, y=df_chg.Y, customdata = df_chg[['well', value, color]],\n",
    "                            marker=dict(color=df_chg[color], size=df_chg[value]*multi_chr, colorscale='Viridis_r',  showscale=True,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_chg.X_wellhead, y=field_avg_coord_chg.Y_wellhead, customdata = field_avg_coord_chg[['field']],\n",
    "                            text=field_avg_coord_chg['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_azr.X, y=df_azr.Y, customdata = df_azr[['well', value, color]],\n",
    "                            marker=dict(color=df_azr[color], size=df_azr[value]*multi_azr, colorscale='Viridis_r',  showscale=False,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_azr.X_wellhead, y=field_avg_coord_azr.Y_wellhead, customdata = field_avg_coord_azr[['field']],\n",
    "                            text=field_avg_coord_azr['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.update_layout(  title_text= ('formation: ' + str(formation) + ' value: ' + str(value) + ' color: ' + str(color)),\n",
    "                        autosize=True, width=1300, height=1400, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Calculation NTD\n",
    "def ntd_calculation_big(dataset, desired_fm, net_var='NET'):\n",
    "    df_lst = []\n",
    "    for well_in_loop in tqdm(dataset.well.unique()[:]):\n",
    "        well_lst = []\n",
    "        data = dataset[(dataset.well==well_in_loop)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        for k in range(len(tst_top)):\n",
    "            if (round(tst_top[k],1) == round(tst_bot[k],1)):\n",
    "                h_tst = 0 \n",
    "            elif (round(tst_bot[k],1) == round(tst_top[k]+0.1,1)):\n",
    "                h_tst = 0\n",
    "            else:\n",
    "                h_tst = (round((tst_bot[k] - tst_top[k]),1))\n",
    "                md_perm = []\n",
    "                md_phit = []\n",
    "                md_vsh = []\n",
    "                for i in range(len(data)):\n",
    "                    if round(data.iloc[i]['TST'],1) >= round(tst_top[k],1) and round(data.iloc[i]['TST'],1) <= round(tst_bot[k],1):\n",
    "                        md_perm.append(data.iloc[i]['LPERM'])\n",
    "                        md_phit.append(data.iloc[i]['PHIT'])\n",
    "                        md_vsh.append(data.iloc[i]['VSH'])\n",
    "                if len(md_perm) == 0:\n",
    "                    md_perm.append(0)\n",
    "                if len(md_phit) == 0:\n",
    "                    md_phit.append(0)\n",
    "                if len(md_vsh) == 0:\n",
    "                    md_vsh.append(0)\n",
    "                well_lst.append([data.iloc[0]['well'], h_tst, tst_top[k], tst_bot[k], round(mean(md_perm),0), round(mean(md_phit),2), round(mean(md_vsh),2)])\n",
    "            df_tst = pd.DataFrame(well_lst, columns = ['well', 'h_tst', 'top_tst', 'bot_tst', 'md_perm_avg', 'md_phit_avg', 'md_vsh_avg'])\n",
    "        df_lst.append(df_tst)\n",
    "    ntd_bal = pd.concat(df_lst)\n",
    "    ntd_bal['FORMATION_up'] = desired_fm\n",
    "    return ntd_bal\n",
    "def ntd_calculation_brief(dataset,well,desired_fm, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "    data.iloc[0, 3] = 0\n",
    "    data.iloc[-1, 3] = 0\n",
    "    tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "    tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "    tops = zip(tst_top, tst_bot)\n",
    "    df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "    df_htst['FORMATION_up'] = desired_fm\n",
    "    df_htst['well'] = well\n",
    "    df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "    df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "    return df_htst\n",
    "# Calculation NTD zero\n",
    "def ntd_calculation_zero(dataset,well,formation, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==formation)]\n",
    "    data.iloc[0, 3] = 1\n",
    "    data.iloc[-1, 3] = 1\n",
    "    tst_zero_top = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i-1][net_var] == 1)]\n",
    "    tst_zero_bot = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1) \n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i+1][net_var] == 1)]\n",
    "    tops_zero = zip(tst_zero_top, tst_zero_bot)\n",
    "    df_zero_htst = pd.DataFrame(tops_zero, columns=['tst_zero_top', 'tst_zero_bot'])\n",
    "    df_zero_htst['FORMATION_up'] = formation\n",
    "    df_zero_htst['well'] = well\n",
    "    df_zero_htst['h_tst_zero'] = df_zero_htst.tst_zero_bot - df_zero_htst.tst_zero_top\n",
    "    df_zero_htst = df_zero_htst[['well','FORMATION_up','tst_zero_top','tst_zero_bot','h_tst_zero']]\n",
    "    return df_zero_htst\n",
    "# Print numerical table with layers\n",
    "def ntd_numerical(dataset, wellname, fmname):\n",
    "    \"\"\"\n",
    "    dataset = ntd_final\n",
    "    \"\"\"\n",
    "    df = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname) ][['well','h_tst','top_tst', 'bot_tst','FORMATION_up']]\n",
    "    q50 = df['h_tst'].quantile(q=0.5, interpolation='nearest')\n",
    "    df['q50'] = q50\n",
    "    return df\n",
    "#Cleaning NET variable and making up NET_clp with clipped data\n",
    "def ntd_htst_cleaning(dataset, cutoff):\n",
    "    \"\"\"\n",
    "    dataset - any updated dataset like df_bal...\n",
    "    cutoff - value in TST to remove layers with thickness below cutoff\n",
    "    \"\"\"\n",
    "    df_list_ntd = []\n",
    "    for well in tqdm(dataset.well.unique()):\n",
    "        ntd_well = dataset[(dataset.well ==well)]\n",
    "        ntd_well_cutoff = ntd_well[ntd_well.h_tst >= cutoff]\n",
    "        well_short = df_bal[['well', 'FORMATION_up', 'MD', 'TST', 'GR_N', 'NET', 'FORMATION']]\n",
    "        net_well = well_short[(well_short.well==well)]\n",
    "        net_well['NET_clp'] = 0\n",
    "        for j in range(len(ntd_well_cutoff.well)):\n",
    "            ntd_top = ntd_well_cutoff.iloc[j, 2].round(3)\n",
    "            ntd_bot = ntd_well_cutoff.iloc[j, 3].round(3)\n",
    "            for i in range(len(net_well.TST)):\n",
    "                well_tst = net_well['TST'].iloc[i].round(3)\n",
    "                if well_tst >= ntd_top and well_tst <= ntd_bot:\n",
    "                    net_well['NET_clp'].iloc[i] = 1\n",
    "        df_list_ntd.append(net_well)\n",
    "    net_clp = pd.concat(df_list_ntd)\n",
    "    return net_clp\n",
    "# Cleaning NET_clp variable from zero values with zero_samples <=cutoff\n",
    "def ntd_htst_zero_cleaning(dataset_zero, dataset, cutoff, net_var1, net_var2):\n",
    "    df_list_ntd_zero = []\n",
    "    for well in tqdm(dataset_zero.well.unique()):\n",
    "        ntd_well_zero = dataset_zero[(dataset_zero.well ==well)]\n",
    "        ntd_well_zero_sel = ntd_well_zero[ntd_well_zero.h_tst_zero <= cutoff]\n",
    "        well_zero_short = dataset[['well','FORMATION_up','MD','TST', net_var1, 'GR_N', 'NET', 'FORMATION']]\n",
    "        well_zero_short[net_var2] = well_zero_short[net_var1]\n",
    "        well_zero_sel = well_zero_short[(well_zero_short.well==well)]\n",
    "        for j in range(len(ntd_well_zero_sel.well)):\n",
    "            ntd_zero_top = ntd_well_zero_sel.iloc[j, 2].round(3)\n",
    "            ntd_zero_bot = ntd_well_zero_sel.iloc[j, 3].round(3)\n",
    "            for i in range(len(well_zero_sel.TST)):\n",
    "                well_zero_tst = well_zero_sel['TST'].iloc[i].round(3)\n",
    "                if well_zero_tst >= ntd_zero_top and well_zero_tst <= ntd_zero_bot:\n",
    "                    well_zero_sel[net_var2].iloc[i] = 1\n",
    "        df_list_ntd_zero.append(well_zero_sel)\n",
    "    result = pd.concat(df_list_ntd_zero)\n",
    "    return result\n",
    "# View desired TST-interval\n",
    "def net_view1(dataset, well, top, bot):\n",
    "    dataset = dataset[dataset.well==well][['well','TST','GR_N', 'RHOB', 'NET','NET_clp']]\n",
    "    return dataset[(dataset.TST >= top) & (dataset.TST <= bot)].head(50)\n",
    "#TST sampling & TST KH curve calculation per formation/well\n",
    "def proph_calculation(dataset, net_var):\n",
    "    df_smpl_lst = []\n",
    "    print('TST sampling calculation')\n",
    "    for well_smpl in tqdm(dataset.well.unique()[:]):\n",
    "        tst_sampl = dataset[dataset.well==well_smpl]['TST'].diff()\n",
    "        df_new = dataset[dataset.well==well_smpl].join(tst_sampl, rsuffix='_smpl')    \n",
    "        df_smpl_lst.append(df_new)\n",
    "    df_bal_tst_smpl = pd.concat(df_smpl_lst)\n",
    "    df_kh_lst_fm = []\n",
    "    print('KHtst calculation')\n",
    "    for fm_kh in ['Balakhany VIII', 'Balakhany X']:\n",
    "        df_kh_lst = []\n",
    "        for well_kh in tqdm(dataset.well.unique()[:]):\n",
    "            well_tst_perm = df_bal_tst_smpl[(df_bal_tst_smpl.well==well_kh) & \n",
    "                                            (df_bal_tst_smpl.FORMATION_up==fm_kh)].sort_values(by='MD', ascending=False)\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'LPERM'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'PHIT'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'VSH'] = 0\n",
    "            well_tst_perm['khtst'] = well_tst_perm.LPERM*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['phithtst'] = well_tst_perm.PHIT*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['vshhtst'] = well_tst_perm.VSH*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['KHtst'] = well_tst_perm.khtst.cumsum()\n",
    "            well_tst_perm['PHITHtst'] = well_tst_perm.phithtst.cumsum()\n",
    "            well_tst_perm['VSHHtst'] = well_tst_perm.vshhtst.cumsum()\n",
    "            well_tst_perm = well_tst_perm.sort_values(by='MD')\n",
    "            df_kh_lst.append(well_tst_perm)\n",
    "        df_khlst = pd.concat(df_kh_lst)\n",
    "        df_kh_lst_fm.append(df_khlst)\n",
    "    df_khlst_fm = pd.concat(df_kh_lst_fm)\n",
    "    # df_khlst_fm = df_khlst_fm.dropna()\n",
    "    return df_khlst_fm[['well', 'FORMATION_up', 'MD', 'TST', 'TST_smpl','KHtst','PHITHtst','VSHHtst']]\n",
    "# Comparison NET_clp and NET_clp2\n",
    "def well_display_net(dataset, well, formation, net1='NET_clp', net2_flag=0, net2='NET_clp_v2'):\n",
    "    well_sel = dataset[(dataset.well == well) & (dataset.FORMATION_up == formation)]\n",
    "    depth = well_sel['TST']\n",
    "    grn = well_sel['GR_N']\n",
    "    net = well_sel['NET']\n",
    "    net_clp = well_sel[net1]\n",
    "    if net2_flag == 0:\n",
    "        fig, ax = plt.subplots(1,3, figsize=(4.5,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange'), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange'), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    if net2_flag == 1:\n",
    "        net_clp2 = well_sel[net2]\n",
    "        fig, ax = plt.subplots(1,4, figsize=(6,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange', lw=0.25), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange', lw=0.25), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        ax[3].plot(net_clp2, depth, color='orange', lw=0.25), ax[3].set_xlim(0, 1), ax[3].grid(axis='y')\n",
    "        ax[3].fill_betweenx(depth,net_clp2, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    return fig.show()\n",
    "# Run RFR model with train/test split\n",
    "def rfr_train_test_split(train_dataset, gs_set, scorer, target='KHtst', rng=0.25, margin=0.005):\n",
    "    \"\"\"\n",
    "    'train_ds', \n",
    "    'metrics: r2_train, r2_test, mae_train, mae_test, test_in', \n",
    "    'grid_search', \n",
    "    'result_df', \n",
    "    'train_df', \n",
    "    'test_df'\n",
    "    --------\n",
    "    scorer = make_scorer(mse, greater_is_better=False) <- format scorer like this\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','FORMATION_up',target]])\n",
    "    x = np.array(train_dataset.drop(target, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    # num=42\n",
    "    train_dataset_list.append(train_dataset.drop(['FORMATION_up', target], axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    # x_train_wells = x_train_init[:,2]\n",
    "    # x_test_wells = x_test_init[:,2]\n",
    "    y_train_wells = y_train_init[:,0:2]\n",
    "    y_test_wells = y_test_init[:,0:2]\n",
    "    x_train = x_train_init[:,2:]\n",
    "    x_test = x_test_init[:,2:]\n",
    "    y_train = y_train_init[:,2]\n",
    "    y_test = y_test_init[:,2]\n",
    "    # GridSearch for ML-model\n",
    "    grid_rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "    grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = gs_set, scoring=scorer, cv = 5)\n",
    "    grid_calc_rfr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_rfr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    print('Grid_search: ', grid_rfr)\n",
    "    # Applying Pipeline for ML-model\n",
    "    rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**gd_sr_setting, n_jobs=-1, random_state=42))])\n",
    "    rfr.fit(x_train, y_train)\n",
    "    y_pred_train = rfr.predict(x_train)\n",
    "    y_pred_test = rfr.predict(x_test)\n",
    "    # Reporting\n",
    "    print('Pipeline: ', rfr.steps[1][1])\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    well_fm_train = pd.DataFrame(y_train_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_train = pd.DataFrame(zip(y_train, y_pred_train), columns=['actual','predict'])\n",
    "    df_rfr_train = well_fm_train.join(rfr_train)\n",
    "    df_rfr_train['l_limit'] = df_rfr_train.actual*dwn_range - margin\n",
    "    df_rfr_train['h_limit'] = df_rfr_train.actual*up_range + margin\n",
    "    df_rfr_train['qc'] = 'out'\n",
    "    df_rfr_train['dataset'] = 'train'\n",
    "    df_rfr_train.loc[(df_rfr_train.predict >= df_rfr_train.l_limit) & (df_rfr_train.predict <= df_rfr_train.h_limit), 'qc'] = 'in'\n",
    "    well_fm_test = pd.DataFrame(y_test_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_test = pd.DataFrame(zip(y_test, y_pred_test), columns=['actual','predict'])\n",
    "    df_rfr_test = well_fm_test.join(rfr_test)\n",
    "    df_rfr_test['l_limit'] = df_rfr_test.actual*dwn_range - margin\n",
    "    df_rfr_test['h_limit'] = df_rfr_test.actual*up_range + margin\n",
    "    df_rfr_test['qc'] = 'out'\n",
    "    df_rfr_test['dataset'] = 'test'\n",
    "    df_rfr_test.loc[(df_rfr_test.predict >= df_rfr_test.l_limit) & (df_rfr_test.predict <= df_rfr_test.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_result = pd.concat([df_rfr_train,df_rfr_test])\n",
    "    df_rfr_result['diff'] = (df_rfr_result.actual - df_rfr_result.predict).round(3)\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'train_in':     df_rfr_train['qc'].value_counts(normalize=True)['in'].round(2),\n",
    "                        'test_in':      df_rfr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=train_dataset_list[0]).sort_values(ascending=True)\n",
    "    return {'train_ds':train_dataset_list[0], \n",
    "            'metrics':metrics_dict, \n",
    "            'grid_search' : grids_setting_list, \n",
    "            'result_df' : df_rfr_result,\n",
    "            'train_df' : df_rfr_train,\n",
    "            'test_df' : df_rfr_test,\n",
    "            'feature_imp' : feature_imp}\n",
    "# Run RFR model with loop\n",
    "def rfr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        # Statement of ML-model\n",
    "        rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**hyperdict, n_jobs=-1, random_state=42))])                                                                                  \n",
    "        # Fitting the ML-model\n",
    "        rfr.fit(x_train, y_train)\n",
    "        y_pred_train = rfr.predict(x_train)\n",
    "        y_pred_test = rfr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "    # Building up of dataframe\n",
    "    print(rfr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin\n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff', 'l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 3, 'predict': 3, 'diff': 3})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Run XGBR model with loop \n",
    "def xgbr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        xgbr = Pipeline([(\"scaler\",StandardScaler()),(\"xgbr\",XGBRegressor(**hyperdict, n_jobs=-1, random_state=42))])\n",
    "        # Fitting the ML-model\n",
    "        xgbr.fit(x_train, y_train)\n",
    "        y_pred_train = xgbr.predict(x_train)\n",
    "        y_pred_test = xgbr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(xgbr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "\n",
    "    # Building up of dataframe\n",
    "    print(xgbr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin \n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff','l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 0, 'predict': 0, 'diff': 0})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Display results of ML-modeling\n",
    "def xplot_qc(dataset, dataframe, max_val, rng=0.25):\n",
    "    fig1_ml = px.scatter(dataset[dataframe], x='actual', y='predict', \n",
    "                        color='qc', \n",
    "                        hover_data=['well'], \n",
    "                        width=400, height=400,\n",
    "                        #  color_discrete_sequence=[\"red\", \"green\"]\n",
    "                        )\n",
    "    up_range = rng+1\n",
    "    dwn_range = 1- rng\n",
    "    fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "    fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "    fig2_1_ml=px.line(x=[0,max_val], y=[0,max_val*up_range])\n",
    "    fig2_2_ml=px.line(x=[0,max_val], y=[0,max_val*dwn_range])\n",
    "    fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "    fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "    fig3_ml.update_layout(  title = 'Comparison Actual vs Pred' + \n",
    "                                    ' QC_train: ' + str(dataset['metrics']['train_in']) +\n",
    "                                    ' QC_test: ' + str(dataset['metrics']['test_in']),\n",
    "                            width=600,height=400, xaxis_title='actual', yaxis_title='predict',\n",
    "                            margin=dict(l=10,r=10,b=10,t=40))\n",
    "    return fig3_ml.show()\n",
    "# Calculate weighted avg properties \n",
    "def avg_prop_calculation(dataset_ntd, dataset, formation):\n",
    "    well_data = []\n",
    "    well_formation = formation\n",
    "    for well in tqdm(dataset_ntd.well.unique()):\n",
    "        # print(well)\n",
    "        ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "        well_avgprop_sel = dataset[(dataset.well==well)]\n",
    "        well_phit = []\n",
    "        well_phit10 = []\n",
    "        well_phit50 = []\n",
    "        well_phit90 = []\n",
    "        well_vsh = []\n",
    "        well_vsh10 = []\n",
    "        well_vsh50 = []\n",
    "        well_vsh90 = []\n",
    "        well_gperm = []\n",
    "        well_h = []\n",
    "        for layers in range(len(ntd_well_avgprop.well)):\n",
    "            ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "            ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "            ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "            phit_lst = []\n",
    "            vsh_lst = []\n",
    "            perm_lst = []\n",
    "            for depth in range(len(well_avgprop_sel.TST)):\n",
    "                well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                    phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                    vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                    perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "            well_phit.append(mean(phit_lst)*ntd_h)\n",
    "            well_phit10.append(np.quantile(phit_lst, 0.1)*ntd_h)\n",
    "            well_phit50.append(np.quantile(phit_lst, 0.5)*ntd_h)\n",
    "            well_phit90.append(np.quantile(phit_lst, 0.9)*ntd_h)\n",
    "            well_vsh.append(mean(vsh_lst)*ntd_h)\n",
    "            well_vsh10.append(np.quantile(vsh_lst, 0.1)*ntd_h)\n",
    "            well_vsh50.append(np.quantile(vsh_lst, 0.5)*ntd_h)\n",
    "            well_vsh90.append(np.quantile(vsh_lst, 0.9)*ntd_h)\n",
    "            well_gperm.append(gmean(perm_lst)*ntd_h)\n",
    "            well_h.append(ntd_h)\n",
    "        well_phit_wavg = sum(well_phit)/sum(well_h)\n",
    "        well_phit10_wavg = sum(well_phit10)/sum(well_h)\n",
    "        well_phit50_wavg = sum(well_phit50)/sum(well_h)\n",
    "        well_phit90_wavg = sum(well_phit90)/sum(well_h)\n",
    "        well_vsh_wavg = sum(well_vsh)/sum(well_h)\n",
    "        well_vsh10_wavg = sum(well_vsh10)/sum(well_h)\n",
    "        well_vsh50_wavg = sum(well_vsh50)/sum(well_h)\n",
    "        well_vsh90_wavg = sum(well_vsh90)/sum(well_h)\n",
    "        well_perm_wavg = sum(well_gperm)/sum(well_h)\n",
    "        well_hmax = max(well_h)\n",
    "        well_h_p50 = np.quantile(well_h, 0.5)\n",
    "        well_layers_count =len(well_h)\n",
    "        well_hsum = sum(well_h)\n",
    "        well_data.append([  well, well_formation, \n",
    "                            well_hmax, well_h_p50, well_layers_count, well_hsum,\n",
    "                            well_phit_wavg, well_phit10_wavg, well_phit50_wavg, well_phit90_wavg,\n",
    "                            well_vsh_wavg, well_vsh10_wavg, well_vsh50_wavg, well_vsh90_wavg,\n",
    "                            well_perm_wavg])\n",
    "    result = pd.DataFrame(well_data, columns=[  'well','FORMATION_up',\n",
    "                                                'htst_max', 'htst_p50','htst_count', 'htst_sum',            \n",
    "                                                'phit_wavg', 'phit10_wavg','phit50_wavg','phit90_wavg',\n",
    "                                                'vsh_wavg', 'vsh10_wavg', 'vsh50_wavg', 'vsh90_wavg',\n",
    "                                                'perm_wavg'])\n",
    "    return result\n",
    "# Euclidian dist calculation with prop\n",
    "def dist_prop_calc(dataset, dist_formation, dist_cutoff, value):\n",
    "    \"\"\"\n",
    "    dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "    \"\"\"\n",
    "    data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "    row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "    distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean', 'TVD_SCS']]), columns=list(data.well))\n",
    "    distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "    distance_fm_well.reset_index(inplace=True)\n",
    "    def well_kh_accum(wells, dataset, kh_formation):\n",
    "        well_kh_accum = []\n",
    "        well_x_accum = []\n",
    "        well_y_accum = []\n",
    "        for i in wells:\n",
    "            well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "            well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "            well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "        well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "        well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "        well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "        well_x3.columns = ['x1','x2','x3']\n",
    "        well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "        well_y3.columns = ['y1','y2','y3']\n",
    "        final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                            well_x3.reset_index().drop('index',axis=1), \n",
    "                            well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "        return final\n",
    "    df_collect = []\n",
    "    for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "        well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "        well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "        well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "        well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "        well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "        well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "        well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "        well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "        concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "        result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "        df_collect.append(result)     \n",
    "    df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "    df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "    return df_well_kh_dist\n",
    "# Feature importance bar chart for 1-to-all algorithm\n",
    "def feature_imp_loop(dataset, wellname, fmname, xsize, ysize):\n",
    "    # dataset = test['result_df']\n",
    "    data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    ftr_imp = data['features_imp'].values[0]\n",
    "    f, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ftr_imp.plot.barh()\n",
    "    ax.set_title('RFR feature imp  ' + wellname + ' ' + fmname)\n",
    "    ax.tick_params(axis='y', labelsize=8, rotation=0)\n",
    "    return f.show()\n",
    "# Save datafram to csv\n",
    "def save_tocsv(dataframe, filename, flag):\n",
    "    if flag == 1:\n",
    "        # Saving avg_prop dataframe to .csv\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\'\n",
    "        dataframe.to_csv(path + filename)\n",
    "    else:\n",
    "        pass\n",
    "# Feature importance bar chart for split dataframe\n",
    "def feature_imp_split(dataset, xsize, ysize):\n",
    "    fig, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ax = dataset.plot.barh()\n",
    "    ax.set_title(\"RFR Feature Importances\")\n",
    "    ax.tick_params(axis='y', labelsize=9, rotation=0)\n",
    "    ax.figure.tight_layout()\n",
    "    return fig.show()\n",
    "# Logging results of ml\n",
    "def write_res_file(finename, comments, target, trainds, metrics, gridsearch):\n",
    "    with open(finename, 'a') as file:\n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.now()\n",
    "        # Write the result to the file\n",
    "        file.write(f'\\n{current_datetime} \\n {comments} target: {target}')\n",
    "        file.write(f'\\n training_ds_{trainds} \\n metrics_{[metrics]} \\n grid_search_{gridsearch}')\n",
    "    file.close()\n",
    "# Remover categorical values from datasets\n",
    "def cat_finder(dataset):\n",
    "    \"\"\"\n",
    "    cat_list: categorical columns to drop out\n",
    "    get_dum_list: categorical columns to run via pd.get_dummies\n",
    "    \"\"\"\n",
    "    cat_list = []\n",
    "    gm_list = []\n",
    "    for col in dataset.columns:\n",
    "        # print(i)\n",
    "        if dataset[col].dtype == 'string':\n",
    "            cat_list.append(col)\n",
    "            if col != 'well':\n",
    "                gm_list.append(col)\n",
    "    # return {'cat_list':cat_list,\n",
    "    #         'get_dum_list': gm_list}\n",
    "    return cat_list, gm_list\n",
    "# Display results of ML-modeling ver2\n",
    "def xplot_qc2(data, max_val, rng, margin, round):\n",
    "    data = data.round({'actual': round, 'predict': round, 'diff': round})\n",
    "    ds_train = data[data.dataset == 'train']\n",
    "    ds_test = data[data.dataset == 'test']\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "    qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "    scatter_train = go.Scatter( x=ds_train.actual, y=ds_train.predict,\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_tr, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_train[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    scatter_test = go.Scatter(  x=ds_test.actual, y=ds_test.predict, \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_ts, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_test[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('train ds', 'test ds'))\n",
    "    fig.add_trace(scatter_train,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "    fig.add_trace(scatter_test,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "    fig.update_layout(  title_text= ('rfr_train_test_split'), width=900, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling ver2 via loop    \n",
    "def xplot_qc2_loop(data, max_val, rng, margin=0.005):\n",
    "    data = data.round({'actual': 3, 'predict': 3, 'diff ': 3})\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors = [colors[qc] for qc in data.qc]\n",
    "    scatter = go.Scatter( x=data.actual, y=data.predict,\n",
    "                            mode='markers',\n",
    "                            marker=dict(color=qc_colors, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = data[['well','actual','predict', 'diff', 'FORMATION_up']],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                            )\n",
    "    fig = go.Figure()\n",
    "    line_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line_up)\n",
    "    fig.add_trace(line_dw)\n",
    "    fig.update_xaxes(title_text='actual')\n",
    "    fig.update_yaxes(title_text='predict')\n",
    "    fig.update_layout(  title_text= ('rfr_loop'), width=450, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling on map\n",
    "def map_qc(metadata, data, fmname, scale):\n",
    "    data['diff'] = abs(data['diff'])\n",
    "    data = data[data.FORMATION_up == fmname]\n",
    "    data_in = data[data.qc=='in']\n",
    "    data_out = data[data.qc=='out']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    platform  = go.Scatter(         x=field_avg_coord.X_wellhead, y=field_avg_coord.Y_wellhead, customdata = field_avg_coord[['field']],\n",
    "                                    text=field_avg_coord['field'], textposition=\"middle right\",\n",
    "                                    marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                                    mode='markers+text', \n",
    "                                    marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_in = go.Scatter(   x=data_in.X, y=data_in.Y,\n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='circle', color='green', size=data_in['actual']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)\n",
    "                                    ),\n",
    "                                    customdata = data_in[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_out = go.Scatter(  x=data_out.X, y=data_out.Y, \n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='diamond', color='red', size=data_out['diff']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                    customdata = data_out[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(platform)\n",
    "    fig.add_trace(scatter_data_in)\n",
    "    fig.add_trace(scatter_data_out)\n",
    "    fig.update_layout(title_text= ('rfr_train_test_split'),autosize=True, width=1000, height=600, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Pairplot new version\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "# Columns reorder for better display of variables\n",
    "def columns_reorder(dataset, selected_column):\n",
    "    new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "    dataset = dataset[new_order]\n",
    "    return dataset\n",
    "# Just simple x-plot for 1 dataframe\n",
    "def log_map_plot(dataframe, x_var, y_var, min_val, max_val):\n",
    "    fig = go.Figure()\n",
    "    scatter = go.Scatter(   x=dataframe[x_var], y=dataframe[y_var], \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='orange', size=10, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = dataframe[['well',x_var,y_var]],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},x:%{customdata[1]}, y:%{customdata[2]}<extra></extra>\"])\n",
    "                            )\n",
    "    line = go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', line=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line)\n",
    "    fig.update_layout(  title_text= ('scatter plot'), width=600, height=600, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Joining main and additional dataframes for predictions\n",
    "def join_add_df_prediction(base_dataframe, add_dataframe, target_var):\n",
    "    \"\"\"\n",
    "    Both dataframes have contain 'well' & 'FORMATION_up' for joining\n",
    "    \"\"\"\n",
    "    join_dataframe = base_dataframe.set_index(['well','FORMATION_up']).join(add_dataframe.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    col_names, gm_list = cat_finder(join_dataframe)\n",
    "    df_corr = join_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(join_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    join_dataframe_gm = pd.concat([join_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, join_dataframe_gm\n",
    "# Preparation dataframes for pairplot and for predictions\n",
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            # print(i)\n",
    "            if dataset[col].dtype == 'string':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        # return {'cat_list':cat_list,\n",
    "        #         'get_dum_list': gm_list}\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "# Function to calculate grid_search via train_split\n",
    "def run_rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin, logtxt_name, comment, xplot_flag, ftr_imp_flag):\n",
    "    model_res = rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, \n",
    "                    model_res['train_ds'], model_res['metrics'], model_res['grid_search'])\n",
    "    print('train_ds: ', model_res['train_ds'])\n",
    "    print('metrics: ', model_res['metrics'])\n",
    "    print('grid_search: ', model_res['grid_search'])\n",
    "    model_res_hyper_par = model_res['grid_search'][0]\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2(dataset['result_df'], 0.3, 0.05, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(dataset['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return model_res_hyper_par\n",
    "# Function to calculate target via 1-to-all\n",
    "def run_rfr_1_to_all(dataset, hyperdict, target, rng, margin, logtxt_name, comment, xplot_flag, max_val, ftr_imp_flag):\n",
    "    loop_res = rfr_loop(dataset, 'FORMATION_up', target, hyperdict, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, loop_res['train_ds'], loop_res['metrics'], loop_res['grid_search'])\n",
    "    loop_res_pred = loop_res['result_df']\n",
    "    print('train_ftrs: ',loop_res['train_ftrs'])\n",
    "    print('metrics: ',loop_res['metrics'])\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2_loop(loop_res['result_df'], max_val, rng, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(loop_res['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return loop_res_pred\n",
    "# Just display 2 df side by side\n",
    "def display_2df_side_side(df1, df2):\n",
    "    df_combined = pd.concat([df1, df2], axis=1)\n",
    "    display(HTML(df_combined.to_html(index=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetThicknessDistribution upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal_net2_kh = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\archiv\\df_bal_net2_kh.csv').drop('Unnamed: 0', axis=1)\n",
    "df_dist_kh_bal_fin = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\archiv\\df_dist_kh_bal_fin.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bal_net2_kh.to_csv('df_bal_net2_kh.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 offsets wells\n",
    "def display_3offset_wells(well, formation, dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh):\n",
    "    \"\"\"\n",
    "    Pay attention dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh\n",
    "    well:       just well name\n",
    "    formation:  just formation\n",
    "    \"\"\"\n",
    "    def well_offset_selection(dataset_dist, fmname, well_target):\n",
    "        try:\n",
    "            well_df = dataset_dist[(dataset_dist.well == well_target) & (dataset_dist.FORMATION_up == fmname)][['well', 'well1', 'well2', 'well3',\n",
    "                                                                                                                        'dist1', 'dist2', 'dist3',\n",
    "                                                                                                                'KHtst','KHtst_1', 'KHtst_2', 'KHtst_3']]\n",
    "            well1 = well_df['well1'].iloc[0]\n",
    "            well2 = well_df['well2'].iloc[0]\n",
    "            well3 = well_df['well3'].iloc[0]\n",
    "            dist1 = well_df['dist1'].astype('int').iloc[0]\n",
    "            dist2 = well_df['dist2'].astype('int').iloc[0]\n",
    "            dist3 = well_df['dist3'].astype('int').iloc[0]\n",
    "            kh = well_df['KHtst'].astype('int').iloc[0]\n",
    "            kh1 = well_df['KHtst_1'].astype('int').iloc[0]\n",
    "            kh2 = well_df['KHtst_2'].astype('int').iloc[0]\n",
    "            kh3 = well_df['KHtst_3'].astype('int').iloc[0]\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return {'target': well_target, 'w1':well1, 'w2':well2, 'w3':well3, \n",
    "                'dist': 0,'d1':dist1, 'd2':dist2,'d3':dist3,\n",
    "                'kh':kh,'kh1':kh1, 'kh2':kh2, 'kh3':kh3}\n",
    "    def display_tracks(dataset, wellname, fmname, ref_depth, depth_step, r, c, kh_value, dist):\n",
    "        try:\n",
    "            data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "            depth = data[ref_depth]\n",
    "            grn = data['GR_N']\n",
    "            vsh = data['VSH']\n",
    "            rhob = data['RHOB'] \n",
    "            npss = data['NPSS']\n",
    "            rdeep = data['RDEEP']\n",
    "            phit = data['PHIT'] \n",
    "            net = data['NET_clp2']\n",
    "            perm = data['LPERM']\n",
    "            kh = data['KHtst']\n",
    "            well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            ax[r,c].plot(grn, depth, color='lightgreen', lw=2, zorder=10)\n",
    "            ax[r,c].set_xlim(0, 150) \n",
    "            ax[r,c].grid(axis='y')\n",
    "            ax[r,c].invert_yaxis()\n",
    "            ax[r,c].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].tick_params(axis='y', labelsize=8)\n",
    "            ax[r,c].set_title(wellname + ' ' + fmname + ' kh:' + str(kh_value) + ' dist:' + str(dist), fontsize=12) \n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                # ax[r,c].text(10, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            ax[r,c+1].plot(rhob, depth, color='red')\n",
    "            ax[r,c+1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "            ax[r,c+1].set_xlim(1.65, 2.65)\n",
    "            ax[r,c+1].grid(axis='y')\n",
    "            ax[r,c+1].grid(axis='x')\n",
    "            ax[r,c+1].invert_yaxis()\n",
    "            ax[r,c+1].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+1].set_xticks([])\n",
    "            ax[r,c+1].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+1].hlines( well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                ax[r,c+1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            twin1 = ax[r,c+1].twiny()\n",
    "            twin1.plot(npss, depth, color='blue')\n",
    "            twin1.set_xlim(0.6, 0)\n",
    "            twin1.set_xticks([])\n",
    "            ax[r,c+2].plot(phit, depth, color='green', linestyle='dashed')\n",
    "            ax[r,c+2].set_xlim(0.3, 0)\n",
    "            ax[r,c+2].grid(axis='x')\n",
    "            ax[r,c+2].grid(axis='y')\n",
    "            ax[r,c+2].invert_yaxis()\n",
    "            ax[r,c+2].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+2].set_xticks([])\n",
    "            ax[r,c+2].set_yticks([])\n",
    "            ax[r,c+2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+2].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "            twin2 = ax[r,c+2].twiny()\n",
    "            twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "            twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "            twin2.set_xlim(0, 1)\n",
    "            twin2.set_xticks([])\n",
    "            ax[r,c+3].plot(perm, depth, color='purple', alpha=0.66)\n",
    "            ax[r,c+3].set_xscale('log')\n",
    "            ax[r,c+3].set_xlim(0.1, 1000)\n",
    "            ax[r,c+3].grid(axis='y')\n",
    "            ax[r,c+3].grid(axis='x')\n",
    "            ax[r,c+3].invert_yaxis()\n",
    "            ax[r,c+3].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+3].set_xticks([])\n",
    "            ax[r,c+3].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.5)\n",
    "            twin4 = ax[r,c+3].twiny()\n",
    "            twin4.plot(kh, depth, color='black', alpha=1)\n",
    "            twin4.set_xticks([])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return fig.show()\n",
    "    def display_subplots():\n",
    "        try:\n",
    "            well_dist_dict = well_offset_selection(dataset_dist, fmname, well_target)\n",
    "            display_tracks(dataset_logs, well_dist_dict['target'], fmname,'TST', 10, 0,0,well_dist_dict['kh'], well_dist_dict['dist'])\n",
    "            display_tracks(dataset_logs, well_dist_dict['w1'], fmname,'TST', 10 ,0,4, well_dist_dict['kh1'], well_dist_dict['d1'])  \n",
    "            display_tracks(dataset_logs, well_dist_dict['w2'], fmname,'TST', 10,1,0, well_dist_dict['kh2'], well_dist_dict['d2'])      \n",
    "            display_tracks(dataset_logs, well_dist_dict['w3'], fmname,'TST', 10,1,4, well_dist_dict['kh3'], well_dist_dict['d3'])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "    well_target = well\n",
    "    fmname = formation\n",
    "    fig, ax = plt.subplots(2,8, figsize=(9,8), constrained_layout=True)\n",
    "    return display_subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_3offset_wells('C21','Balakhany VIII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data interpolation & downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_by_depth_fm(dataset_logs, formation_name, step):\n",
    "    def interpolate_by_depth(one_well, formation_name, step):\n",
    "        one_well = one_well.sort_values(by='TST')\n",
    "        well_name = one_well[\"well\"].iloc[0]\n",
    "        data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "        starting_tst = one_well[\"TST\"].iloc[0]\n",
    "        new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "        interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_VSH = interp1d(one_well['TST'], one_well['VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_GR = interp1d(one_well['TST'], one_well['GR_N'], kind='linear', fill_value=\"extrapolate\")\n",
    "        # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "        new_data = {\n",
    "            'well': [well_name for _ in range(len(new_TST_values))],\n",
    "            'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "            'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "            'TST': new_TST_values,\n",
    "            'X_traj': interp_X(new_TST_values),\n",
    "            'Y_traj': interp_Y(new_TST_values),\n",
    "            'PHIT': interp_PHIT(new_TST_values),\n",
    "            'TVD_SCS': interp_TVD(new_TST_values),\n",
    "            'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "            'LPERM': interp_LPERM(new_TST_values),\n",
    "            'KHtst': interp_KHtst(new_TST_values),\n",
    "            'VSH': interp_VSH(new_TST_values),\n",
    "            'GR_N': interp_GR(new_TST_values),\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        return new_df\n",
    "    df_lst = []\n",
    "    print(f'Start interpolation of {formation_name}')\n",
    "    for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "        well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "        well_interp = interpolate_by_depth(well_sel, formation_name, step)\n",
    "        df_lst.append(well_interp)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "well_bal8 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany VIII')]\n",
    "well_bal10 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany X')]\n",
    "well_bal8_interp = interpolate_by_depth_fm(well_bal8, 'Balakhany VIII', 0.1)\n",
    "well_bal10_interp = interpolate_by_depth_fm(well_bal10, 'Balakhany X', 0.1)\n",
    "\n",
    "def data_downsampling_run(dataset, fm, desired_length):\n",
    "    print(f'Start downsamling of {fm}')\n",
    "    def data_downsampling(dataset, fm, wellname, desired_length):\n",
    "        tst_init = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fm)]['TST'].values\n",
    "        var_init = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fm)]['PHIT'].values\n",
    "        tst_downsample = np.linspace(tst_init.min(), tst_init.max(), desired_length)\n",
    "        var_list = ['X_traj', 'Y_traj', 'PHIT','TVD_SCS', 'NET_clp2', 'LPERM', 'KHtst', 'VSH', 'GR_N']\n",
    "        var_downsampled_list = []\n",
    "        for var in var_list:\n",
    "            var_values = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fm)][var].values\n",
    "            f = interp1d(tst_init, var_values, kind='linear', fill_value=\"extrapolate\")\n",
    "            var_downsample = f(tst_downsample)  \n",
    "            var_downsampled_list.append(pd.DataFrame({var:var_downsample}))\n",
    "        var_df = pd.concat(var_downsampled_list, axis=1)\n",
    "        var_df['TST'] = tst_downsample\n",
    "        var_df['well'] = wellname\n",
    "        var_df['FORMATION_up'] = fm\n",
    "\n",
    "        # plt.figure(figsize=(15, 4))\n",
    "        # plt.plot(tst_init, var_init, label='Original Curve')\n",
    "        # plt.plot(var_df.TST, var_df.PHIT, label='Downsampled Curve')\n",
    "        # plt.show()\n",
    "\n",
    "        return var_df\n",
    "    df_lst = []\n",
    "    for wellname in tqdm(dataset.well.unique()):\n",
    "        res = data_downsampling(dataset, fm, wellname, desired_length)\n",
    "        df_lst.append(res)\n",
    "    df_full = pd.concat(df_lst)\n",
    "    df_full.insert(0, 'well', df_full.pop('well'))\n",
    "    df_full.insert(1, 'FORMATION_up', df_full.pop('FORMATION_up'))\n",
    "    return df_full\n",
    "well_bal8_interp_downsampled = data_downsampling_run(well_bal8_interp, 'Balakhany VIII', 500)\n",
    "well_bal10_interp_downsampled = data_downsampling_run(well_bal10_interp, 'Balakhany X', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_counting(dataset, fm):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        samples = dataset[(dataset.well == wellname) & (dataset.FORMATION_up==fm)].shape[0]\n",
    "        data = pd.DataFrame({'well':[wellname], 'samples':[samples]})\n",
    "        df_lst.append(data)\n",
    "    result = pd.concat(df_lst).reset_index(drop=True)\n",
    "    result = result[result.samples != 0].sort_values(by='samples')\n",
    "    return result\n",
    "result = samples_counting(well_bal8_interp, 'Balakhany VIII')\n",
    "print(result.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(well_bal8_interp_downsampled.columns)\n",
    "print(well_bal8_interp_downsampled[well_bal8_interp_downsampled.well == 'A11Z'].shape)\n",
    "print(df_bal_net2_kh[df_bal_net2_kh.well=='A11Z'].shape)\n",
    "well_bal8_interp_downsampled[well_bal8_interp_downsampled.well == 'A11Z'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gr_phit(dataset, wellname, fm, comment):\n",
    "    well_selected = dataset[(dataset.well==wellname) & (dataset.FORMATION_up==fm)]\n",
    "    well_selected['PHIT_clipped'] = well_selected['PHIT']\n",
    "    well_selected.loc[well_selected.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "\n",
    "    y = well_selected.TST\n",
    "    gr = well_selected.GR_N\n",
    "    phit_avg = well_selected.PHIT\n",
    "    phit_cliped = well_selected.PHIT_clipped\n",
    "    net = well_selected.NET_clp2\n",
    "    kh = well_selected.KHtst\n",
    "    print(  'KH orig:', kh.iloc[0].round(0))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(4, 7))\n",
    "    ax[0].plot(gr, y, color='lightgreen', label='gr_n')\n",
    "    ax[0].set_xlim(0, 100)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].legend(fontsize=10)\n",
    "    ax[1].plot(phit_cliped, y, color='red', label='phit_cliped')\n",
    "    ax[1].plot(phit_avg, y, color='green',ls='--', label='phit')\n",
    "    ax[1].plot(net, y, color='orange', alpha=0.33)\n",
    "    ax[1].fill_betweenx(y,net, color='orange', alpha=0.33)\n",
    "    ax[1].set_xlim(0, 0.3)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].legend(fontsize=10)\n",
    "    plt.suptitle(wellname +' '+ fm + ' ' + comment)\n",
    "    fig.show()\n",
    "display_gr_phit(df_bal_net2_kh, 'A05', 'Balakhany VIII',  'data downsampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gr_phit_kh(dataset, wellname, fm, comment):\n",
    "    well_a01w = dataset[(dataset.well==wellname) & (dataset.FORMATION_up==fm)]\n",
    "\n",
    "    well_a01w['PHIT_clipped'] = well_a01w['PHIT']\n",
    "    well_a01w.loc[well_a01w.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "    well_a01w['LPERM_avg'] = 0.00000002*(np.exp(well_a01w.PHIT*105.56))\n",
    "    well_a01w.loc[well_a01w['PHIT'] >= 0.2, 'LPERM_avg'] = (7.7925*((well_a01w.PHIT*100)**2))-(29881.0*well_a01w.PHIT)+2891.8\n",
    "    well_a01w.loc[well_a01w['PHIT'] < 0.16, 'LPERM_avg'] = 0.0159*(np.exp(well_a01w.PHIT*21.27))\n",
    "    well_a01w['khtst'] = well_a01w.LPERM_avg*0.1\n",
    "    well_a01w['KHtst_avg'] = well_a01w.loc[::-1, 'khtst'].cumsum()[::-1]\n",
    "\n",
    "    y = well_a01w.TST\n",
    "    # phit_orig = well_a01w.PHIT_orig\n",
    "    gr = well_a01w.GR_N\n",
    "    phit_avg = well_a01w.PHIT\n",
    "    phit_cliped = well_a01w.PHIT_clipped\n",
    "    net = well_a01w.NET_clp2\n",
    "    perm = well_a01w.LPERM\n",
    "    perm_avg = well_a01w.LPERM_avg\n",
    "    kh = well_a01w.KHtst\n",
    "    # kh_avg = well_a01w.KHtst_avg\n",
    "    print(  'KH orig:', kh.iloc[0].round(0))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(6, 7))\n",
    "    # ax[0].plot(phit_orig, y, color='green', label='phit')\n",
    "    ax[0].plot(gr, y, color='lightgreen', label='gr_n')\n",
    "    ax[0].set_xlim(0, 100)\n",
    "    ax[0].invert_yaxis()\n",
    "    # ax[0].set_title(wellname +' '+ fm)\n",
    "    ax[0].legend(fontsize=10)\n",
    "    ax[1].plot(phit_cliped, y, color='red', label='phit_cliped')\n",
    "    ax[1].plot(phit_avg, y, color='green',ls='--', label='phit')\n",
    "    ax[1].plot(net, y, color='orange', alpha=0.33)\n",
    "    ax[1].fill_betweenx(y,net, color='orange', alpha=0.33)\n",
    "    ax[1].set_xlim(0, 0.3)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].legend(fontsize=10)\n",
    "\n",
    "    ax[2].plot(perm, y, color='purple', lw=2, label='perm')\n",
    "    # ax[2].plot(perm_avg, y, color='yellow', label='perm_avg')\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_xscale('log')\n",
    "    ax[2].legend(fontsize=8)\n",
    "    ax[3].plot(kh, y, color='black')\n",
    "    # ax[3].plot(kh_avg, y, color='gray')\n",
    "    ax[3].invert_yaxis()\n",
    "    plt.suptitle(wellname +' '+ fm + ' ' + comment)\n",
    "    fig.show()\n",
    "display_gr_phit_kh(well_bal8_interp_downsampled, 'A05', 'Balakhany VIII',  'data downsampled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML data downsampled prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_for_spatial_prediction_downsampled(dataset_full, fm, offset_qty):\n",
    "        \n",
    "        def joining_coordinates(dataset_full):\n",
    "            coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            return coordinates\n",
    "        coordinates = joining_coordinates(dataset_full)\n",
    "        coordinates = coordinates[~coordinates.well.isin(['A14Y','CHIRAG6','GCA1', 'GCA2', 'GCA5', 'GCA5Z', 'GCA6', 'GCA6Y', 'GCA6Z', 'GCA7'])]\n",
    "\n",
    "        def well_distance_calculation(coordinates, fm):\n",
    "            coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "            df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "            well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "            result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "            return result\n",
    "        well_dist_crosstable = well_distance_calculation(coordinates, fm)\n",
    "\n",
    "        def offset_well_names_dist(dataset, offset_qty):\n",
    "            df_lst = []\n",
    "            for ind in range(len(dataset.well.unique())):\n",
    "                off_well_series = dataset.iloc[ind]\n",
    "                off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "                off_well_selected['well'] = off_well_selected.columns[0]\n",
    "                off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "                dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "                well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "                col_names = []\n",
    "                for i in range(len(off_well_selected.columns[:-1])):\n",
    "                    col = off_well_selected.columns[i]\n",
    "                    col_names.append(col)\n",
    "                    off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "                off_well_names = pd.DataFrame(col_names).T\n",
    "                col_names = []\n",
    "                for i in range(len(off_well_names.columns)):\n",
    "                    col = off_well_names.columns[i]\n",
    "                    col_names.append(col)\n",
    "                    off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "                \n",
    "                concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "                df_lst.append(concat_well_data)\n",
    "            result = pd.concat(df_lst).reset_index(drop=True)\n",
    "            return result\n",
    "        well_dist_data = offset_well_names_dist(well_dist_crosstable, offset_qty)\n",
    "        well_dist_data['FORMATION_up'] = fm\n",
    "        result = {'well_dist':well_dist_data, 'coordinates':coordinates, 'well_dist_crosstable':well_dist_crosstable}\n",
    "        return result\n",
    "well_dist_init = dataset_for_spatial_prediction_downsampled(df_bal_net2_kh, 'Balakhany VIII', 3)\n",
    "well_dist = well_dist_init['well_dist']\n",
    "\n",
    "def target_feature_collection_downsampled(dataset, dataset_dist):\n",
    "    df_lst = []\n",
    "    for wellname in tqdm(dataset_dist.well.unique()[:]):\n",
    "        data = dataset_dist[dataset_dist.well == wellname]\n",
    "        cc = 0\n",
    "        feature_list = []\n",
    "        for j in data.columns:\n",
    "            if 'well_' in j:\n",
    "                cc += 1\n",
    "                offset_wellname = data[j].values[0]\n",
    "                offset_dist = data['dist_' + str(cc)].values[0]\n",
    "                data_offset = dataset[(dataset.well == offset_wellname)]\n",
    "                well_name = 'well_' + str(cc)\n",
    "                well_ = pd.DataFrame({well_name:[offset_wellname for i in range(len(data_offset))]})\n",
    "                feature_list.append(well_)\n",
    "                dist_name = 'dist_' + str(cc)\n",
    "                dist_ = pd.DataFrame({dist_name:[offset_dist for i in range(len(data_offset))]})\n",
    "                feature_list.append(dist_)\n",
    "                phit_name = 'phit_' + str(cc)\n",
    "                phit = pd.DataFrame({phit_name:data_offset['PHIT']}).reset_index(drop=True)\n",
    "                feature_list.append(phit)\n",
    "                vsh_name = 'vsh_' + str(cc)\n",
    "                vsh = pd.DataFrame({vsh_name:data_offset['VSH']}).reset_index(drop=True)\n",
    "                feature_list.append(vsh)\n",
    "                net_name = 'net2_' + str(cc)\n",
    "                net = pd.DataFrame({net_name:data_offset['NET_clp2']}).reset_index(drop=True)\n",
    "                feature_list.append(net)\n",
    "                x_name = 'x_traj_' + str(cc)\n",
    "                xtraj = pd.DataFrame({x_name:data_offset['X_traj']}).reset_index(drop=True)\n",
    "                feature_list.append(xtraj)\n",
    "                y_name = 'y_traj_' + str(cc)\n",
    "                ytraj = pd.DataFrame({y_name:data_offset['Y_traj']}).reset_index(drop=True)\n",
    "                feature_list.append(ytraj)\n",
    "                tvdscs_name = 'tvd_scs_' + str(cc)\n",
    "                tvdscs = pd.DataFrame({tvdscs_name:data_offset['TVD_SCS']}).reset_index(drop=True)\n",
    "                feature_list.append(tvdscs)\n",
    "        result_features = pd.concat(feature_list, axis=1)\n",
    "\n",
    "        var_target = 'PHIT'\n",
    "        target_list = []\n",
    "        target = dataset[dataset.well == wellname][var_target]\n",
    "        fm = dataset[dataset.well == wellname]['FORMATION_up'].iloc[0]\n",
    "        target_list.append(target)\n",
    "        well_target = pd.DataFrame({'well':[wellname for i in range(len(target))]})\n",
    "        target_list.append(well_target)\n",
    "        fm_target = pd.DataFrame({'FORMATION_up':[fm for i in range(len(target))]})\n",
    "        target_list.append(fm_target)\n",
    "        result_target = pd.concat(target_list, axis=1)\n",
    "        result_target = result_target.rename(columns={var_target:var_target + '_target'})\n",
    "        result_target.insert(0, 'well', result_target.pop('well'))\n",
    "        result_target.insert(1, 'FORMATION_up', result_target.pop('FORMATION_up'))\n",
    "        result_well = pd.concat([result_target, result_features], axis=1)\n",
    "\n",
    "        df_lst.append(result_well)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "input_bal8 = target_feature_collection_downsampled(well_bal8_interp_downsampled, well_dist)\n",
    "input_bal8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_list = input_bal8.well.unique()\n",
    "random.shuffle(well_list)\n",
    "well_list_test = well_list[:10]\n",
    "\n",
    "X_train = input_bal8[~input_bal8.well.isin(well_list_test)].drop(['well','FORMATION_up', 'PHIT_target', 'well_1', 'well_2', 'well_3'], axis=1)\n",
    "y_train = input_bal8[~input_bal8.well.isin(well_list_test)]['PHIT_target']\n",
    "X_test = input_bal8[input_bal8.well.isin(well_list_test)].drop(['well','FORMATION_up', 'PHIT_target', 'well_1', 'well_2', 'well_3'], axis=1)\n",
    "y_test = input_bal8[input_bal8.well.isin(well_list_test)]['PHIT_target']\n",
    "y_test_wellname = input_bal8[input_bal8.well.isin(well_list_test)][['well','FORMATION_up']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# BayesianRidge()\n",
    "# XGBRegressor(n_jobs=-1, random_state=42, verbosity=0)\n",
    "# CatBoostRegressor(random_state=42, verbose=False)\n",
    "# AdaBoostRegressor(random_state=42)\n",
    "# LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': randint(2, 300),\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_depth': randint(1, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "selected_model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "# model_search = RandomizedSearchCV(estimator=selected_model, param_distributions=param_dist, n_iter=5, cv=5, random_state=42, n_jobs=-1, verbose=4)\n",
    "# model_search.fit(X_train, y_train)\n",
    "# best_model = model_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "test = pd.concat([y_test_wellname.reset_index(drop=True), test], axis=1)\n",
    "\n",
    "tolerance = 0.05\n",
    "test['up'] = test['y_orig']*(1 + tolerance)\n",
    "test['down'] = test['y_orig']*(1 - tolerance)\n",
    "test['qc'] = 'out'\n",
    "test['dataset'] = 'test'\n",
    "test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "testqc = test.qc.value_counts(normalize=True)\n",
    "testqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = PredictionError(model)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred(dataset, wellname, comment):\n",
    "    # plt.style.use('_mpl-gallery-nogrid')\n",
    "    well = dataset[(dataset.well==wellname)]\n",
    "    well_1 = input_bal8[input_bal8.well == wellname][['well_1']].iloc[0][0]\n",
    "    well_2 = input_bal8[input_bal8.well == wellname][['well_2']].iloc[0][0]\n",
    "    well_3 = input_bal8[input_bal8.well == wellname][['well_3']].iloc[0][0]\n",
    "    data_1 = well_bal8_interp_downsampled[(well_bal8_interp_downsampled.well==well_1)]\n",
    "    data_2 = well_bal8_interp_downsampled[(well_bal8_interp_downsampled.well==well_2)]\n",
    "    data_3 = well_bal8_interp_downsampled[(well_bal8_interp_downsampled.well==well_3)]\n",
    "    y = [i for i in range(len(well))]\n",
    "    x_orig = well.y_orig\n",
    "    x_pred = well.y_pred\n",
    "    x_1 = data_1['PHIT']\n",
    "    x_2 = data_2['PHIT']\n",
    "    x_3 = data_3['PHIT']\n",
    "\n",
    "    fig, ax = plt.subplots(1,4, figsize=(12, 8))\n",
    "    ax[0].plot(x_orig, y, color='green', label='phit')\n",
    "    ax[0].plot(x_pred, y, color='red', label='phit_pred', alpha=0.75)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_xlim(0, 0.3)\n",
    "    ax[0].legend(fontsize=10, facecolor='lightgray', edgecolor='gray')\n",
    "    ax[0].set_title(wellname)\n",
    "\n",
    "    ax[1].plot(x_1, y, color='green', label='phit')\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].set_xlim(0, 0.3)\n",
    "    ax[1].set_title(well_1)\n",
    "\n",
    "    ax[2].plot(x_2, y, color='green', label='phit')\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_xlim(0, 0.3)\n",
    "    ax[2].set_title(well_2)\n",
    "\n",
    "    ax[3].plot(x_3, y, color='green', label='phit')\n",
    "    ax[3].invert_yaxis()\n",
    "    ax[3].set_xlim(0, 0.3)\n",
    "    ax[3].set_title(well_3)\n",
    "    plt.suptitle(wellname + ' ' + comment)\n",
    "    fig.show()\n",
    "phit_orig_pred(test, test.well.unique()[6], 'data downsampled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PHIT & GRcube - martix plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_cube_upload():\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    vsh_cube_log = pd.read_parquet(path + 'ACG_GRcube_VSH_v3.parquet.gzip')\n",
    "    vsh_cube_log = vsh_cube_log.replace(-9999.000, np.nan)\n",
    "    vsh_cube_log = vsh_cube_log.dropna()\n",
    "    vsh_cube_log.loc[vsh_cube_log.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "    vsh_cube_log.loc[vsh_cube_log.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "    vsh_cube_log = vsh_cube_log[vsh_cube_log.FORMATION_up.isin(['Balakhany VIII', 'Balakhany X'])]\n",
    "    vsh_grcube = vsh_cube_log[['wellName', 'DEPT','VSH_GRcube', 'FORMATION_up']]\n",
    "    vsh_grcube = vsh_grcube.rename(columns={'wellName':'well', 'DEPT':'MD'})\n",
    "    return vsh_grcube\n",
    "vsh_grcube = gr_cube_upload()\n",
    "df_bal_net2_kh['MD'] = df_bal_net2_kh.MD.round(1)\n",
    "df_bal_net2_kh_cube = df_bal_net2_kh.set_index(['well','MD', 'FORMATION_up']).join(vsh_grcube.set_index(['well','MD', 'FORMATION_up'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsh_gr_cube_recalc(dataset):\n",
    "    def interpolate_by_depth_fm(dataset_logs, formation_name, step):\n",
    "        def interpolate_by_depth(one_well, formation_name, step):\n",
    "            one_well = one_well.sort_values(by='TST')\n",
    "            well_name = one_well[\"well\"].iloc[0]\n",
    "            data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "            starting_tst = one_well[\"TST\"].iloc[0]\n",
    "            new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "            interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_VSH_GRcube = interp1d(one_well['TST'], one_well['VSH_GRcube'], kind='linear', fill_value=\"extrapolate\")\n",
    "            # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "            new_data = {\n",
    "                'well': [well_name for _ in range(len(new_TST_values))],\n",
    "                'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "                'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "                'TST': new_TST_values,\n",
    "                'X_traj': interp_X(new_TST_values),\n",
    "                'Y_traj': interp_Y(new_TST_values),\n",
    "                'PHIT': interp_PHIT(new_TST_values),\n",
    "                'TVD_SCS': interp_TVD(new_TST_values),\n",
    "                'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "                'LPERM': interp_LPERM(new_TST_values),\n",
    "                'KHtst': interp_KHtst(new_TST_values),\n",
    "                'VSH_GRcube':interp_VSH_GRcube(new_TST_values)\n",
    "            }\n",
    "            new_df = pd.DataFrame(new_data)\n",
    "            return new_df\n",
    "        df_lst = []\n",
    "        print(f'Start interpolation of {formation_name}')\n",
    "        for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "            well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "            well_interp = interpolate_by_depth(well_sel, formation_name, step)\n",
    "            df_lst.append(well_interp)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    well_bal8 = dataset[(dataset.FORMATION_up == 'Balakhany VIII')]\n",
    "    well_bal10 = dataset[(dataset.FORMATION_up == 'Balakhany X')]\n",
    "    well_bal8_interp = interpolate_by_depth_fm(well_bal8, 'Balakhany VIII', 0.1)\n",
    "    well_bal10_interp = interpolate_by_depth_fm(well_bal10, 'Balakhany X', 0.1)\n",
    "    well_bal8_interp_rn = well_bal8_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "    well_bal10_interp_rn = well_bal10_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "\n",
    "    def phit_rolling_averaging(input_dataset, samples_per_window):\n",
    "        df_lst = []\n",
    "        avg_report = []\n",
    "        fmname = input_dataset['FORMATION_up'].iloc[0] \n",
    "        print(f'Start rolling averaging of {fmname}')\n",
    "        for wellname in tqdm(input_dataset.well.unique()):\n",
    "            dataset = input_dataset[input_dataset.well == wellname]\n",
    "            window_size = int(len(dataset) / samples_per_window)\n",
    "            dataset['PHIT'] = dataset['PHIT_orig'].rolling(window=window_size, center=True).mean()\n",
    "            dataset =  dataset.dropna(subset=['PHIT'])\n",
    "            df_lst.append(dataset)\n",
    "            avg_report.append((wellname, len(dataset), window_size, samples_per_window))\n",
    "        result = pd.concat(df_lst)\n",
    "        avg_report_df = pd.DataFrame(avg_report, columns=['well','lenght_ds','window_size','samples_per_window'])\n",
    "        return result, avg_report_df\n",
    "    samples_per_window = 100\n",
    "    well_bal8_interp_phavg, avg_report_df8 = phit_rolling_averaging(well_bal8_interp_rn, samples_per_window)\n",
    "    well_bal10_interp_phavg, avg_report_df10 = phit_rolling_averaging(well_bal10_interp_rn, samples_per_window)\n",
    "    well_bal8_interp_phavg['PHIT_clp'] = well_bal8_interp_phavg['PHIT']\n",
    "    well_bal10_interp_phavg['PHIT_clp'] = well_bal10_interp_phavg['PHIT']\n",
    "    well_bal8_interp_phavg['LPERM_clp'] = well_bal8_interp_phavg['LPERM']\n",
    "    well_bal10_interp_phavg['LPERM_clp'] = well_bal10_interp_phavg['LPERM']\n",
    "    well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "    well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "    well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "    well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "    return well_bal8_interp_phavg, well_bal10_interp_phavg\n",
    "well_bal8_interp_phavg, well_bal10_interp_phavg = vsh_gr_cube_recalc(df_bal_net2_kh_cube)\n",
    "def well_letter_def(dataset):\n",
    "    wells_letter = [wellname[0] for wellname in dataset.well.unique()]\n",
    "    return set(wells_letter)\n",
    "well_letter_def(well_bal8_interp_phavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_plots_phit_vsh_matrix(dataset, platform, variable, flag, max_var, comment):\n",
    "    \"\"\"\n",
    "    flag = 'phit' or 'perm'\n",
    "    \"\"\"\n",
    "    rows = 4\n",
    "    columns = 9\n",
    "    wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(platform)]\n",
    "    fig, ax = plt.subplots(rows,columns, figsize=(16,rows*3))\n",
    "    counter = 0\n",
    "    y_real_list = []\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                data = dataset[dataset.well==wells_letter[counter]]\n",
    "                y_real_list.append(len(data))\n",
    "                counter +=1\n",
    "    max_ind = max(y_real_list)\n",
    "    counter = 0\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                well_data = dataset[dataset.well==wells_letter[counter]]\n",
    "                ind = well_data[variable]\n",
    "                y_real = [k for k in range(len(ind))]\n",
    "                y_desired = [k for k in range(max_ind)]\n",
    "                y_diff = len(y_desired) - len(y_real)\n",
    "                values_to_add = [0.12 for k in range(y_diff)]\n",
    "                x = well_data[variable]\n",
    "                x_gr = well_data['VSH_GRcube']\n",
    "                x_new = pd.concat([x, pd.Series(values_to_add)])\n",
    "                x_gr_new = pd.concat([x_gr, pd.Series(values_to_add)])          \n",
    "                if flag == 'phit':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='green', lw=1.5, alpha=1, zorder=1)\n",
    "                    ax[j,i].set_xlim(0.1, 0.35)\n",
    "                    twin = ax[j,i].twiny()\n",
    "                    twin.plot(x_gr_new, y_desired, color='blue', lw=2, alpha=0.5, zorder=0)\n",
    "                    twin.set_xlim(0, 1)\n",
    "                if flag == 'perm':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='purple', lw=2, alpha=0.75)\n",
    "                    ax[j,i].set_xscale('log')\n",
    "                    ax[j,i].set_xlim(0.1, max_var)\n",
    "                ax[j,i].set_title(wells_letter[counter] + comment)\n",
    "                ax[j,i].invert_yaxis()\n",
    "                ax[j,i].grid()\n",
    "                counter +=1\n",
    "\n",
    "    return plt.tight_layout()\n",
    "# for letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']:\n",
    "for letter in ['A','B']:\n",
    "    well_plots_phit_vsh_matrix(well_bal8_interp_phavg, letter, 'PHIT_clp', 'phit', 0.35, ' bal8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering top_phi_bot layering v2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_data_calculation(dataset):\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[df_net2_bal8.FORMATION_up=='Balakhany VIII']\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[df_net2_bal10.FORMATION_up=='Balakhany X']\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            well_khtst = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                khtst_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                        khtst_lst.append(well_avgprop_sel['KHtst'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_khtst.append(khtst_lst[0] - khtst_lst[-1])\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_khtst, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'khtst',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'khtst','htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "    def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "        row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "        row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "        return row_change\n",
    "    df_list = []\n",
    "    for wellname in dataset_ntd.well.unique():\n",
    "        df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "        df_list.append(df)\n",
    "    res_df_list = pd.concat(df_list)\n",
    "    result = pd.concat([dataset_ntd, res_df_list])\n",
    "    result = result.sort_values(by=['well','top_tst'])\n",
    "    result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "    return result_final\n",
    "ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')\n",
    "\n",
    "def top_phit_bot_clustering(dataset):\n",
    "    print('Top & bot calculation')\n",
    "    def top_phit_bot_collection_run(dataset):\n",
    "        def top_phit_bot_collection(dataset, wellname):\n",
    "            data = dataset[dataset.well == wellname]\n",
    "            data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "            data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "            data['bot_htst'] = data['not_htst']\n",
    "            data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'khtst',\n",
    "                         'top_htst','htst','bot_htst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            return data\n",
    "        df_lst = []\n",
    "        for wellname in tqdm(dataset.well.unique()):\n",
    "            res_df = top_phit_bot_collection(dataset, wellname)\n",
    "            df_lst.append(res_df)\n",
    "        top_phi_bot_cluster = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster\n",
    "    top_phi_bot_cluster = top_phit_bot_collection_run(dataset)\n",
    "\n",
    "    def top_phit_bot_ntg_run(dataset):\n",
    "        def top_phit_bot_ntg(dataset, wellname):\n",
    "            ntg = []\n",
    "            data = dataset[dataset.well == wellname].reset_index(drop=True)\n",
    "            for ind, row in data.iterrows():\n",
    "                if ind == 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst']))\n",
    "                if ind != 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst'] + row['top_htst']))\n",
    "                if ind == len(data):\n",
    "                    ntg.append(row['htst']/(row['top_htst'] + row['htst']))\n",
    "            result = pd.concat([data, pd.DataFrame({'ntg':ntg})], axis=1)\n",
    "            return result\n",
    "        df_lst = []\n",
    "        for wellname in dataset.well.unique():\n",
    "            df = top_phit_bot_ntg(dataset, wellname)\n",
    "            df_lst.append(df)\n",
    "        top_phi_bot_cluster_ntg = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster_ntg\n",
    "    top_phi_bot_cluster_ntg = top_phit_bot_ntg_run(top_phi_bot_cluster)\n",
    "    \n",
    "    return top_phi_bot_cluster_ntg\n",
    "top_phi_bot_cluster8 = top_phit_bot_clustering(ntd_val_final8_clean)\n",
    "top_phi_bot_cluster10 = top_phit_bot_clustering(ntd_val_final10_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clustering(dataset, feature_list, scaler, cluster_num):\n",
    "    \"\"\"\n",
    "    MinMaxScaler(), StandardScaler()\n",
    "    \"\"\"\n",
    "    data = dataset[feature_list]\n",
    "    normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "    kmeans_labels = kmeans.fit_predict(normalized_data)\n",
    "    kmeans_labels = pd.DataFrame(kmeans_labels, columns=['kmeans'])\n",
    "\n",
    "    gmm = GaussianMixture(n_components=cluster_num, random_state=42)\n",
    "    gmm.fit(normalized_data)\n",
    "    gmm_labels = gmm.predict(normalized_data)\n",
    "    gmm_labels = pd.DataFrame(gmm_labels, columns=['gmm'])\n",
    "\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=cluster_num)\n",
    "    agglomerative_labels = agglomerative.fit_predict(normalized_data)\n",
    "    agglomerative_labels = pd.DataFrame(agglomerative_labels, columns=['agglomer'])\n",
    "    result = pd.concat([dataset, kmeans_labels, gmm_labels, agglomerative_labels], axis=1)\n",
    "    return result\n",
    "data_clustered8 = data_clustering(top_phi_bot_cluster8, ['phit_avg', 'htst'], StandardScaler(), 3)\n",
    "data_clustered10 = data_clustering(top_phi_bot_cluster10, ['phit_avg', 'vsh_avg'], StandardScaler(), 3)\n",
    "data_clustered8.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo_clustering(dataset, clustering, comment):\n",
    "    data = dataset[dataset.phit_avg !=0]\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(18, 4))\n",
    "    custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "    sns.histplot(data=data, x='phit_avg', hue=clustering, ax=ax[0], kde=True,  palette=custom_palette)\n",
    "    ax[0].grid(True, axis='x'), ax[0].set_xticks(np.arange(0.12, 0.32, 0.02)), ax[0].tick_params(axis='both', which='major', labelsize=8)\n",
    "    sns.histplot(data=data[data.htst < 30], x='htst', hue=clustering, ax=ax[1], kde=True,  palette=custom_palette)\n",
    "    ax[1].grid(True, axis='x'), ax[1].set_xticks(np.arange(0, 30, 3)), ax[1].tick_params(axis='both', which='major', labelsize=8)\n",
    "    sns.histplot(data=data, x='ntg', hue=clustering, ax=ax[2], kde=True,  palette=custom_palette)\n",
    "    ax[2].grid(True, axis='x'), ax[2].set_xticks(np.arange(0, 1, 0.1)), ax[2].tick_params(axis='both', which='major', labelsize=8)\n",
    "    sns.histplot(data=data, x='vsh_avg', hue=clustering, ax=ax[3], kde=True,  palette=custom_palette)\n",
    "    ax[3].grid(True, axis='x'), ax[3].set_xticks(np.arange(0, 0.6, 0.1)), ax[3].tick_params(axis='both', which='major', labelsize=8)\n",
    "    fig.suptitle(comment)\n",
    "histo_clustering(data_clustered8, 'kmeans', 'Kmeans Bal VIII')\n",
    "histo_clustering(data_clustered10, 'kmeans', 'Kmeans Bal X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xplot_clustering(dataset, clustering, comment):\n",
    "    data = dataset[dataset.phit_avg !=0]\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "    sns.scatterplot(data=data, x='phit_avg', y='htst', ax=ax, hue=clustering, style=clustering, markers=['o', 's', 'D'], palette=custom_palette, alpha=0.5)\n",
    "    fig.suptitle(comment)\n",
    "xplot_clustering(data_clustered8, 'kmeans', 'Kmeans Bal VIII')\n",
    "xplot_clustering(data_clustered10, 'kmeans', 'Kmeans Bal X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters_top_phi_bot_v2(dataset, clustering, fm):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        bot_lst = []\n",
    "        ntg_lst = []\n",
    "        vsh_lst = []\n",
    "        cluster_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            cluster_lst.append(np.nan)\n",
    "            cluster_lst.append(row[clustering])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "            \n",
    "            bot_lst.append(row['bot_htst'])\n",
    "\n",
    "            ntg_lst.append(0)\n",
    "            ntg_lst.append(row['ntg'])\n",
    "\n",
    "            vsh_lst.append(0)\n",
    "            vsh_lst.append(row['vsh_avg'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        cluster_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "        well_collect_cluster_short = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, ntg_lst, vsh_lst, cluster_lst ), columns=[  'well','phit', 'htst', \n",
    "                                                                                                                                'ntg', 'vsh', 'cluster'])\n",
    "        well_last_row = pd.DataFrame({'well':[well_lst[-1]], 'phit':[0], 'htst': [bot_lst[-1]], 'ntg':[0], 'vsh':[0], 'cluster':[cluster_lst[-1]]})\n",
    "        well_collect_cluster = pd.concat([well_collect_cluster_short, well_last_row]).reset_index(drop=True)\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    result['FORMATION_up'] = fm\n",
    "    return result\n",
    "tpb8_kmeans = well_collecting_clusters_top_phi_bot_v2(data_clustered8, 'kmeans', 'Balakhany VIII')\n",
    "tpb10_kmeans = well_collecting_clusters_top_phi_bot_v2(data_clustered10, 'agglomer', 'Balakhany VIII')\n",
    "\n",
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    \"\"\"\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']\n",
    "    \"\"\"\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.suptitle(clustering)\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "coloring_clusters_matrix_tpb3(tpb8_kmeans, ['B'], 4, 9, 'kmeans bal8', 'dontprint')\n",
    "coloring_clusters_matrix_tpb3(tpb10_kmeans, ['A'], 4, 9, 'kmeans bal10', 'dontprint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data_to_map(dataset, dataset_coord, fm, comment):\n",
    "    def dataset_groupby(dataset):\n",
    "        result = dataset.groupby(['well','cluster'])['htst'].sum().reset_index()\n",
    "        return result\n",
    "    tpb_test_v3_piechart = dataset_groupby(dataset)\n",
    "\n",
    "    def cluster_transpose(dataset, wellname):\n",
    "        result = dataset[dataset.well==wellname]\n",
    "        result.loc[result.cluster == 0, 'cluster_0'] = result.htst\n",
    "        result.loc[result.cluster == 1, 'cluster_1'] = result.htst\n",
    "        result.loc[result.cluster == 2, 'cluster_2'] = result.htst\n",
    "        result.fillna(0)\n",
    "        result = result.groupby('well').sum().reset_index()\n",
    "        result = result[['well', 'cluster_0', 'cluster_1','cluster_2']]\n",
    "        return result\n",
    "    df_lst = []\n",
    "    for wellname in tpb_test_v3_piechart.well.unique():\n",
    "        df = cluster_transpose(tpb_test_v3_piechart, wellname)\n",
    "        df_lst.append(df)\n",
    "    data_transpose = pd.concat(df_lst).reset_index(drop=True)\n",
    "\n",
    "    def coordinates_calc(dataset_coord, fm):\n",
    "        dataset_coord = dataset_coord[dataset_coord.FORMATION_up == fm]\n",
    "        result = dataset_coord.groupby('well')[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        return result \n",
    "    coord = coordinates_calc(dataset_coord, fm)\n",
    "    data_transpose_coord = data_transpose.set_index('well').join(coord.set_index('well'), rsuffix='_coord').reset_index()\n",
    "\n",
    "    def piechart_map(dataset_map):\n",
    "        fig, ax = plt.subplots(figsize=(13,10))\n",
    "        for ind, row in dataset_map.iterrows():\n",
    "                ax.pie([row['cluster_0'], row['cluster_1'], row['cluster_2']], \n",
    "                        radius=0.3, center=(row['X_mean']/1000, row['Y_mean']/1000), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.75},\n",
    "                        colors=['blue', 'green', 'red'], frame=True)\n",
    "        # plt.grid()\n",
    "        plt.title(comment)     \n",
    "    piechart_map(data_transpose_coord)\n",
    "    return data_transpose_coord\n",
    "cluster_kmeans8 = cluster_data_to_map(tpb8_kmeans, df_bal_net2_kh, 'Balakhany VIII', 'Well Bal8 clustering by Kmeans & Bal8 1510 geobodies')\n",
    "cluster_kmeans10 = cluster_data_to_map(tpb10_kmeans, df_bal_net2_kh, 'Balakhany X', 'Well Bal8 clustering by Kmeans & Bal8 1510 geobodies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow for prediction khtst from phit_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_clustering(data_clustered8, 'kmeans', 'Kmeans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLuster 0,1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khtst_workflow(cluster_list):\n",
    "\n",
    "    def dataset_for_spatial_prediction(dataset_full, dataset_cluster, offset_qty, cluster_algo, cluster_list):\n",
    "            \n",
    "            def joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list):\n",
    "                coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "                dataset_cluster = dataset_cluster[(dataset_cluster[cluster_algo].isin(cluster_list))]\n",
    "                result = dataset_cluster.set_index(['well','FORMATION_up']).join(coordinates.set_index(['well','FORMATION_up'])).reset_index()\n",
    "                coordinates = result[['well','FORMATION_up', 'X_mean', 'Y_mean']].groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "                return coordinates, result\n",
    "            coordinates, dataset_cluster_xy = joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list)\n",
    "            coordinates = coordinates[~coordinates.well.isin(['A14Y'])]\n",
    "\n",
    "            def well_distance_calculation(coordinates, fm):\n",
    "                coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "                df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "                well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "                result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "                return result\n",
    "            well_dist_crosstable_8 = well_distance_calculation(coordinates, 'Balakhany VIII')\n",
    "\n",
    "            def offset_well_names_dist(dataset, offset_qty):\n",
    "                df_lst = []\n",
    "                for ind in range(len(dataset.well.unique())):\n",
    "                    off_well_series = dataset.iloc[ind]\n",
    "                    off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "                    off_well_selected['well'] = off_well_selected.columns[0]\n",
    "                    off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "                    dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "                    well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_selected.columns[:-1])):\n",
    "                        col = off_well_selected.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "                    off_well_names = pd.DataFrame(col_names).T\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_names.columns)):\n",
    "                        col = off_well_names.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "                    \n",
    "                    concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "                    df_lst.append(concat_well_data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_dist_data8 = offset_well_names_dist(well_dist_crosstable_8, offset_qty)\n",
    "\n",
    "            def offset_wells_features_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    data = dataset_dist[dataset_dist.well == wellname]\n",
    "                    cc = 0\n",
    "                    for j in data.columns:\n",
    "                        if 'well_' in j:\n",
    "                            cc += 1\n",
    "                            offset_wellname = data[j].values[0]\n",
    "                            data_cluster = dataset_clusters[(dataset_clusters.well == offset_wellname) & \n",
    "                                                                (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                            var_name = 'phit_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['phit_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'vsh_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['vsh_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'htst_sum_' + str(cc)\n",
    "                            data[var_name] = data_cluster['htst'].sum()                \n",
    "                    df_lst.append(data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                result['FORMATION_up'] = fm\n",
    "                return result\n",
    "            well_features8 = offset_wells_features_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "\n",
    "            def target_wells_variable_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    df = pd.DataFrame({'well': [wellname], 'FORMATION_up': [fm], 'phit_wavg_target': [0]})\n",
    "                    data = dataset_clusters[(dataset_clusters.well == wellname) & \n",
    "                                            (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                    df['phit_wavg_target'] = ((data['phit_avg'] * data['htst']).sum()) / (data['htst'].sum())\n",
    "                    df_lst.append(df)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_target8 = target_wells_variable_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "            \n",
    "            dataset8 = well_target8.set_index(['well','FORMATION_up']).join(well_features8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "            result = {'dataset8':dataset8, 'cluster_xy':dataset_cluster_xy, 'well_dist8':well_dist_data8, 'coordinates':coordinates,\n",
    "                    'target8':well_target8, 'feature8':well_features8, 'dist_crosstable8':well_dist_crosstable_8}\n",
    "            return result\n",
    "    input_ph8 = dataset_for_spatial_prediction(df_bal_net2_kh, data_clustered8, 3, 'kmeans', cluster_list)['dataset8']\n",
    "    print(f'Dataset features {textwrap.fill(str(list(input_ph8.columns)), width=150)}')\n",
    "\n",
    "    def run_phit_pred_split(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            # model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print('features dataset: \\n', list(X_train.columns))\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('test \"in\":', '{:.2f}'.format(result['testqc'].round(2)),'\\t', model_name)\n",
    "            return result\n",
    "        def xplot_qc2(data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_train = data[data.dataset == 'train']\n",
    "            ds_test = data[data.dataset == 'test']\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "            qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}'))\n",
    "            fig.add_trace(scatter_train,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.add_trace(scatter_test,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "            fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')   \n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_ph = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_ph = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_ph = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_ph = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_ph = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        xplot_qc2(model1_ph['result'], model1_ph['trainqc'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_ph['result'], model2_ph['trainqc'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_ph['result'], model3_ph['trainqc'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc2(model4_ph['result'], model4_ph['trainqc'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_ph['result'], model5_ph['trainqc'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_ph['result'], model6_ph['trainqc'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model_split = run_phit_pred_split(input_ph8, cluster_list, tolerance=0.05)['result']\n",
    "\n",
    "    def run_phit_pred_1_to_all(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_1_to_all(dataset, selected_model, target, tolerance, model_name):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "            print(model_name)\n",
    "            df_lst = []\n",
    "            for wellname in tqdm(dataset.well.unique()[:]):\n",
    "                train = dataset[dataset.well != wellname]\n",
    "                X_train_init = train.drop(target, axis=1)\n",
    "                y_train_init = train[['well','FORMATION_up', target]]\n",
    "                X_train = X_train_init.drop(drop_lst_X, axis=1)\n",
    "                y_train = y_train_init.drop(drop_lst_y, axis=1)\n",
    "                model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                test = dataset[dataset.well == wellname]\n",
    "                y_test_wnames = test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "                X_test_init = test.drop(target, axis=1)\n",
    "                y_test_init = test[['well','FORMATION_up', target]]\n",
    "                X_test = X_test_init.drop(drop_lst_X, axis=1)\n",
    "                y_test = y_test_init.drop(drop_lst_y, axis=1).values[0]\n",
    "                y_pred = model.predict(X_test)\n",
    "                test = pd.DataFrame(zip(y_test, y_pred), columns=['y_orig', 'y_pred'])\n",
    "                test = pd.concat([y_test_wnames, test], axis=1)\n",
    "                df_lst.append(test)\n",
    "                \n",
    "            result = pd.concat(df_lst).reset_index(drop=True)\n",
    "            result['up'] = result['y_orig']*(1 + tolerance)\n",
    "            result['down'] = result['y_orig']*(1 - tolerance)\n",
    "            result['qc'] = 'out'\n",
    "            result.loc[(result['y_pred'] <= result.up) & (result['y_pred'] >= result.down), 'qc'] = 'in'\n",
    "            resultqc = result.qc.value_counts(normalize=True)\n",
    "\n",
    "            phit_pred = result[['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "            dataset_pred = dataset.set_index(['well','FORMATION_up']).join(phit_pred.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "            result_dict = {'result':result, 'res_full':dataset_pred, 'testqc':resultqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            return result_dict\n",
    "        def xplot_qc_1_to_all(data, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_test = data\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_test = go.Scatter( x=ds_test['y_orig'], y=ds_test['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=1, subplot_titles=(f'test qc {qc_test}',))\n",
    "            fig.add_trace(scatter_test,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.update_layout(  title_text= (comment), width=350, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_1_to_all(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance,'RandomForestRegressor')\n",
    "        # model2_ph = model_prediction_1_to_all(dataset, BayesianRidge(), target, 0.05, 'BayesianRidge')\n",
    "        # model3_ph = model_prediction_1_to_all(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, 0.05, 'XGBRegressor')\n",
    "        # model4_ph = model_prediction_1_to_all(dataset, CatBoostRegressor(random_state=42, verbose=False), target, 0.05,'CatBoostRegressor')\n",
    "        # model5_ph = model_prediction_1_to_all(dataset, AdaBoostRegressor(random_state=42), target, 0.05, 'AdaBoostRegressor')\n",
    "        # model6_ph = model_prediction_1_to_all(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, 0.05, 'LGBMRegressor')\n",
    "\n",
    "        xplot_qc_1_to_all(model1_ph['result'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model2_ph['result'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model3_ph['result'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model4_ph['result'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model5_ph['result'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model6_ph['result'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model = run_phit_pred_1_to_all(input_ph8, cluster_list, tolerance=0.05)\n",
    "\n",
    "    def concat_prediction_to_khtst_df(data_pred, data_khtst, data_main, cluster_algo):\n",
    "        phit_pred8 = data_pred['result'][['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "        khtst8 = data_khtst[data_khtst[cluster_algo].isin(cluster_list)].groupby(['well','FORMATION_up'])['khtst'].sum().reset_index()\n",
    "\n",
    "        khtst8_phit_pred8 = khtst8.set_index(['well','FORMATION_up']).join(phit_pred8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "        phitpred_khtst = khtst8_phit_pred8.set_index(['well','FORMATION_up']).join(data_main.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "        phitpred_khtst.insert(19, 'phit_pred', phitpred_khtst.pop('phit_pred'))\n",
    "        phitpred_khtst.insert(19, 'phit_wavg_target', phitpred_khtst.pop('phit_wavg_target'))\n",
    "        phitpred_khtst.insert(19, 'khtst', phitpred_khtst.pop('khtst'))\n",
    "        return phitpred_khtst\n",
    "    phitpred_khtst = concat_prediction_to_khtst_df(model, data_clustered8, input_ph8, 'kmeans')\n",
    "    print(f'Concat dataset features {textwrap.fill(str(list(phitpred_khtst.columns)), width=150)}')\n",
    "\n",
    "    print('\\nPrediction KHtst: ')\n",
    "    def run_khtst_pred_split(dataset, cluster_list, tolerance):\n",
    "\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            \"\"\"\n",
    "            'well', 'FORMATION_up', 'well_1', 'well_2', 'well_3', 'dist_1',\n",
    "            'dist_2', 'dist_3', 'phit_wavg_1', 'vsh_wavg_1', 'htst_sum_1',\n",
    "            'phit_wavg_2', 'vsh_wavg_2', 'htst_sum_2', 'phit_wavg_3', 'vsh_wavg_3',\n",
    "            'htst_sum_3', 'phit_pred', 'phit_wavg_target', 'khtst'\n",
    "            \"\"\"\n",
    "            drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3', 'dist_1', 'dist_2','dist_3', 'phit_wavg_target']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            # model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print(f'features dataset: {list(X_train.columns)}')\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            return result\n",
    "\n",
    "        def xplot_qc2(data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_train = data[data.dataset == 'train']\n",
    "            ds_test = data[data.dataset == 'test']\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "            qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = data[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}'))\n",
    "            fig.add_trace(scatter_train,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.add_trace(scatter_test,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "            fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')\n",
    "        target = 'khtst'\n",
    "        model1_kh = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_kh = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_kh = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_kh = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_kh = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_kh = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        # xplot_qc2(model1_kh['result'], model1_kh['trainqc'], model1_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_kh['result'], model2_kh['trainqc'], model2_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_kh['result'], model3_kh['trainqc'], model3_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        xplot_qc2(model4_kh['result'], model4_kh['trainqc'], model4_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_kh['result'], model5_kh['trainqc'], model5_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_kh['result'], model6_kh['trainqc'], model6_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model4_kh\n",
    "    model_khtst = run_khtst_pred_split(phitpred_khtst, cluster_list, 0.25)\n",
    "    result = {'khtst_pred':model_khtst['result'], 'khtst_data':phitpred_khtst, 'phit_pred':model['result'], 'input':input_ph8}\n",
    "    return result\n",
    "test_full = khtst_workflow(cluster_list = [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3',\n",
    "                                        'dist_1', 'dist_2', 'dist_3']\n",
    "pairplot_special(test_full['khtst_data'].drop(drop_lst_X, axis=1), 12, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khtst_workflow(cluster_list):\n",
    "\n",
    "    def dataset_for_spatial_prediction(dataset_full, dataset_cluster, offset_qty, cluster_algo, cluster_list):\n",
    "            \n",
    "            def joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list):\n",
    "                coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "                dataset_cluster = dataset_cluster[(dataset_cluster[cluster_algo].isin(cluster_list))]\n",
    "                result = dataset_cluster.set_index(['well','FORMATION_up']).join(coordinates.set_index(['well','FORMATION_up'])).reset_index()\n",
    "                coordinates = result[['well','FORMATION_up', 'X_mean', 'Y_mean']].groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "                return coordinates, result\n",
    "            coordinates, dataset_cluster_xy = joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list)\n",
    "            coordinates = coordinates[~coordinates.well.isin(['A14Y'])]\n",
    "\n",
    "            def well_distance_calculation(coordinates, fm):\n",
    "                coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "                df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "                well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "                result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "                return result\n",
    "            well_dist_crosstable_8 = well_distance_calculation(coordinates, 'Balakhany VIII')\n",
    "\n",
    "            def offset_well_names_dist(dataset, offset_qty):\n",
    "                df_lst = []\n",
    "                for ind in range(len(dataset.well.unique())):\n",
    "                    off_well_series = dataset.iloc[ind]\n",
    "                    off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "                    off_well_selected['well'] = off_well_selected.columns[0]\n",
    "                    off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "                    dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "                    well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_selected.columns[:-1])):\n",
    "                        col = off_well_selected.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "                    off_well_names = pd.DataFrame(col_names).T\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_names.columns)):\n",
    "                        col = off_well_names.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "                    \n",
    "                    concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "                    df_lst.append(concat_well_data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_dist_data8 = offset_well_names_dist(well_dist_crosstable_8, offset_qty)\n",
    "\n",
    "            def offset_wells_features_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    data = dataset_dist[dataset_dist.well == wellname]\n",
    "                    cc = 0\n",
    "                    for j in data.columns:\n",
    "                        if 'well_' in j:\n",
    "                            cc += 1\n",
    "                            offset_wellname = data[j].values[0]\n",
    "                            data_cluster = dataset_clusters[(dataset_clusters.well == offset_wellname) & \n",
    "                                                                (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                            var_name = 'phit_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['phit_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'vsh_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['vsh_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'htst_sum_' + str(cc)\n",
    "                            data[var_name] = data_cluster['htst'].sum()                \n",
    "                    df_lst.append(data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                result['FORMATION_up'] = fm\n",
    "                return result\n",
    "            well_features8 = offset_wells_features_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "\n",
    "            def target_wells_variable_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    df = pd.DataFrame({'well': [wellname], 'FORMATION_up': [fm], 'phit_wavg_target': [0]})\n",
    "                    data = dataset_clusters[(dataset_clusters.well == wellname) & \n",
    "                                            (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                    df['phit_wavg_target'] = ((data['phit_avg'] * data['htst']).sum()) / (data['htst'].sum())\n",
    "                    df_lst.append(df)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_target8 = target_wells_variable_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "            \n",
    "            dataset8 = well_target8.set_index(['well','FORMATION_up']).join(well_features8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "            result = {'dataset8':dataset8, 'cluster_xy':dataset_cluster_xy, 'well_dist8':well_dist_data8, 'coordinates':coordinates,\n",
    "                    'target8':well_target8, 'feature8':well_features8, 'dist_crosstable8':well_dist_crosstable_8}\n",
    "            return result\n",
    "    input_ph8 = dataset_for_spatial_prediction(df_bal_net2_kh, data_clustered8, 3, 'kmeans', cluster_list)['dataset8']\n",
    "    print(f'Dataset features {textwrap.fill(str(list(input_ph8.columns)), width=150)}')\n",
    "\n",
    "    def run_phit_pred_split(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            # model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print('features dataset: \\n', list(X_train.columns))\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('test \"in\":', '{:.2f}'.format(result['testqc'].round(2)),'\\t', model_name)\n",
    "            return result\n",
    "        def xplot_qc2(data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_train = data[data.dataset == 'train']\n",
    "            ds_test = data[data.dataset == 'test']\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "            qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}'))\n",
    "            fig.add_trace(scatter_train,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.add_trace(scatter_test,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "            fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')   \n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_ph = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_ph = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_ph = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_ph = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_ph = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        xplot_qc2(model1_ph['result'], model1_ph['trainqc'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_ph['result'], model2_ph['trainqc'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_ph['result'], model3_ph['trainqc'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc2(model4_ph['result'], model4_ph['trainqc'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_ph['result'], model5_ph['trainqc'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_ph['result'], model6_ph['trainqc'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model_split = run_phit_pred_split(input_ph8, cluster_list, tolerance=0.05)['result']\n",
    "\n",
    "    def run_phit_pred_1_to_all(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_1_to_all(dataset, selected_model, target, tolerance, model_name):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "            print(model_name)\n",
    "            df_lst = []\n",
    "            for wellname in tqdm(dataset.well.unique()[:]):\n",
    "                train = dataset[dataset.well != wellname]\n",
    "                X_train_init = train.drop(target, axis=1)\n",
    "                y_train_init = train[['well','FORMATION_up', target]]\n",
    "                X_train = X_train_init.drop(drop_lst_X, axis=1)\n",
    "                y_train = y_train_init.drop(drop_lst_y, axis=1)\n",
    "                model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                test = dataset[dataset.well == wellname]\n",
    "                y_test_wnames = test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "                X_test_init = test.drop(target, axis=1)\n",
    "                y_test_init = test[['well','FORMATION_up', target]]\n",
    "                X_test = X_test_init.drop(drop_lst_X, axis=1)\n",
    "                y_test = y_test_init.drop(drop_lst_y, axis=1).values[0]\n",
    "                y_pred = model.predict(X_test)\n",
    "                test = pd.DataFrame(zip(y_test, y_pred), columns=['y_orig', 'y_pred'])\n",
    "                test = pd.concat([y_test_wnames, test], axis=1)\n",
    "                df_lst.append(test)\n",
    "                \n",
    "            result = pd.concat(df_lst).reset_index(drop=True)\n",
    "            result['up'] = result['y_orig']*(1 + tolerance)\n",
    "            result['down'] = result['y_orig']*(1 - tolerance)\n",
    "            result['qc'] = 'out'\n",
    "            result.loc[(result['y_pred'] <= result.up) & (result['y_pred'] >= result.down), 'qc'] = 'in'\n",
    "            resultqc = result.qc.value_counts(normalize=True)\n",
    "\n",
    "            phit_pred = result[['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "            dataset_pred = dataset.set_index(['well','FORMATION_up']).join(phit_pred.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "            result_dict = {'result':result, 'res_full':dataset_pred, 'testqc':resultqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            return result_dict\n",
    "        def xplot_qc_1_to_all(data, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_test = data\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_test = go.Scatter( x=ds_test['y_orig'], y=ds_test['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=1, subplot_titles=(f'test qc {qc_test}',))\n",
    "            fig.add_trace(scatter_test,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.update_layout(  title_text= (comment), width=350, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_1_to_all(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance,'RandomForestRegressor')\n",
    "        # model2_ph = model_prediction_1_to_all(dataset, BayesianRidge(), target, 0.05, 'BayesianRidge')\n",
    "        # model3_ph = model_prediction_1_to_all(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, 0.05, 'XGBRegressor')\n",
    "        # model4_ph = model_prediction_1_to_all(dataset, CatBoostRegressor(random_state=42, verbose=False), target, 0.05,'CatBoostRegressor')\n",
    "        # model5_ph = model_prediction_1_to_all(dataset, AdaBoostRegressor(random_state=42), target, 0.05, 'AdaBoostRegressor')\n",
    "        # model6_ph = model_prediction_1_to_all(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, 0.05, 'LGBMRegressor')\n",
    "\n",
    "        xplot_qc_1_to_all(model1_ph['result'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model2_ph['result'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model3_ph['result'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model4_ph['result'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model5_ph['result'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model6_ph['result'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model = run_phit_pred_1_to_all(input_ph8, cluster_list, tolerance=0.05)\n",
    "\n",
    "    def concat_prediction_to_khtst_df(data_pred, data_khtst, data_main, cluster_algo):\n",
    "        phit_pred8 = data_pred['result'][['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "        khtst8 = data_khtst[data_khtst[cluster_algo].isin(cluster_list)].groupby(['well','FORMATION_up'])['khtst'].sum().reset_index()\n",
    "\n",
    "        khtst8_phit_pred8 = khtst8.set_index(['well','FORMATION_up']).join(phit_pred8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "        phitpred_khtst = khtst8_phit_pred8.set_index(['well','FORMATION_up']).join(data_main.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "        phitpred_khtst.insert(19, 'phit_pred', phitpred_khtst.pop('phit_pred'))\n",
    "        phitpred_khtst.insert(19, 'phit_wavg_target', phitpred_khtst.pop('phit_wavg_target'))\n",
    "        phitpred_khtst.insert(19, 'khtst', phitpred_khtst.pop('khtst'))\n",
    "        return phitpred_khtst\n",
    "    phitpred_khtst = concat_prediction_to_khtst_df(model, data_clustered8, input_ph8, 'kmeans')\n",
    "    print(f'Concat dataset features {textwrap.fill(str(list(phitpred_khtst.columns)), width=150)}')\n",
    "\n",
    "    print('\\nPrediction KHtst: ')\n",
    "    def run_khtst_pred_split(dataset, cluster_list, tolerance):\n",
    "\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            \"\"\"\n",
    "            'well', 'FORMATION_up', 'well_1', 'well_2', 'well_3', 'dist_1',\n",
    "            'dist_2', 'dist_3', 'phit_wavg_1', 'vsh_wavg_1', 'htst_sum_1',\n",
    "            'phit_wavg_2', 'vsh_wavg_2', 'htst_sum_2', 'phit_wavg_3', 'vsh_wavg_3',\n",
    "            'htst_sum_3', 'phit_pred', 'phit_wavg_target', 'khtst'\n",
    "            \"\"\"\n",
    "            drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3', 'dist_1', 'dist_2','dist_3', 'phit_wavg_target']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            # model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print(f'features dataset: {list(X_train.columns)}')\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            return result\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')\n",
    "        target = 'khtst'\n",
    "        model1_kh = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_kh = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_kh = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_kh = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_kh = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_kh = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        # xplot_qc2(model1_kh['result'], model1_kh['trainqc'], model1_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_kh['result'], model2_kh['trainqc'], model2_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_kh['result'], model3_kh['trainqc'], model3_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        xplot_qc2(model4_kh['result'], model4_kh['trainqc'], model4_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_kh['result'], model5_kh['trainqc'], model5_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_kh['result'], model6_kh['trainqc'], model6_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model4_kh\n",
    "    model_khtst = run_khtst_pred_split(phitpred_khtst, cluster_list, 0.25)\n",
    "    result = {'khtst_pred':model_khtst, 'khtst_data':phitpred_khtst, 'phit_pred':model['result']}\n",
    "    return result\n",
    "test_0 = khtst_workflow(cluster_list = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3',\n",
    "                                        'dist_1', 'dist_2', 'dist_3', 'vsh_wavg_1', 'htst_sum_1', 'vsh_wavg_2', 'htst_sum_2', 'vsh_wavg_3', 'htst_sum_3', 'cluster']\n",
    "pairplot_special(test_0['khtst_data'].drop(drop_lst_X, axis=1), 7, 7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khtst_workflow(cluster_list):\n",
    "\n",
    "    def dataset_for_spatial_prediction(dataset_full, dataset_cluster, offset_qty, cluster_algo, cluster_list):\n",
    "            \n",
    "            def joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list):\n",
    "                coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "                dataset_cluster = dataset_cluster[(dataset_cluster[cluster_algo].isin(cluster_list))]\n",
    "                result = dataset_cluster.set_index(['well','FORMATION_up']).join(coordinates.set_index(['well','FORMATION_up'])).reset_index()\n",
    "                coordinates = result[['well','FORMATION_up', 'X_mean', 'Y_mean']].groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "                return coordinates, result\n",
    "            coordinates, dataset_cluster_xy = joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list)\n",
    "            coordinates = coordinates[~coordinates.well.isin(['A14Y'])]\n",
    "\n",
    "            def well_distance_calculation(coordinates, fm):\n",
    "                coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "                df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "                well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "                result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "                return result\n",
    "            well_dist_crosstable_8 = well_distance_calculation(coordinates, 'Balakhany VIII')\n",
    "\n",
    "            def offset_well_names_dist(dataset, offset_qty):\n",
    "                df_lst = []\n",
    "                for ind in range(len(dataset.well.unique())):\n",
    "                    off_well_series = dataset.iloc[ind]\n",
    "                    off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "                    off_well_selected['well'] = off_well_selected.columns[0]\n",
    "                    off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "                    dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "                    well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_selected.columns[:-1])):\n",
    "                        col = off_well_selected.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "                    off_well_names = pd.DataFrame(col_names).T\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_names.columns)):\n",
    "                        col = off_well_names.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "                    \n",
    "                    concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "                    df_lst.append(concat_well_data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_dist_data8 = offset_well_names_dist(well_dist_crosstable_8, offset_qty)\n",
    "\n",
    "            def offset_wells_features_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    data = dataset_dist[dataset_dist.well == wellname]\n",
    "                    cc = 0\n",
    "                    for j in data.columns:\n",
    "                        if 'well_' in j:\n",
    "                            cc += 1\n",
    "                            offset_wellname = data[j].values[0]\n",
    "                            data_cluster = dataset_clusters[(dataset_clusters.well == offset_wellname) & \n",
    "                                                                (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                            var_name = 'phit_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['phit_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'vsh_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['vsh_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'htst_sum_' + str(cc)\n",
    "                            data[var_name] = data_cluster['htst'].sum()                \n",
    "                    df_lst.append(data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                result['FORMATION_up'] = fm\n",
    "                return result\n",
    "            well_features8 = offset_wells_features_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "\n",
    "            def target_wells_variable_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    df = pd.DataFrame({'well': [wellname], 'FORMATION_up': [fm], 'phit_wavg_target': [0]})\n",
    "                    data = dataset_clusters[(dataset_clusters.well == wellname) & \n",
    "                                            (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                    df['phit_wavg_target'] = ((data['phit_avg'] * data['htst']).sum()) / (data['htst'].sum())\n",
    "                    df_lst.append(df)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_target8 = target_wells_variable_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "            \n",
    "            dataset8 = well_target8.set_index(['well','FORMATION_up']).join(well_features8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "            result = {'dataset8':dataset8, 'cluster_xy':dataset_cluster_xy, 'well_dist8':well_dist_data8, 'coordinates':coordinates,\n",
    "                    'target8':well_target8, 'feature8':well_features8, 'dist_crosstable8':well_dist_crosstable_8}\n",
    "            return result\n",
    "    input_ph8 = dataset_for_spatial_prediction(df_bal_net2_kh, data_clustered8, 3, 'kmeans', cluster_list)['dataset8']\n",
    "    print(f'Dataset features {textwrap.fill(str(list(input_ph8.columns)), width=150)}')\n",
    "\n",
    "    def run_phit_pred_split(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            # model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print('features dataset: \\n', list(X_train.columns))\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('test \"in\":', '{:.2f}'.format(result['testqc'].round(2)),'\\t', model_name)\n",
    "            return result\n",
    "        def xplot_qc2(data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_train = data[data.dataset == 'train']\n",
    "            ds_test = data[data.dataset == 'test']\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "            qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}'))\n",
    "            fig.add_trace(scatter_train,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.add_trace(scatter_test,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "            fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')   \n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_ph = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_ph = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_ph = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_ph = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_ph = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        xplot_qc2(model1_ph['result'], model1_ph['trainqc'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_ph['result'], model2_ph['trainqc'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_ph['result'], model3_ph['trainqc'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc2(model4_ph['result'], model4_ph['trainqc'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_ph['result'], model5_ph['trainqc'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_ph['result'], model6_ph['trainqc'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model_split = run_phit_pred_split(input_ph8, cluster_list, tolerance=0.05)['result']\n",
    "\n",
    "    def run_phit_pred_1_to_all(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_1_to_all(dataset, selected_model, target, tolerance, model_name):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "            print(model_name)\n",
    "            df_lst = []\n",
    "            for wellname in tqdm(dataset.well.unique()[:]):\n",
    "                train = dataset[dataset.well != wellname]\n",
    "                X_train_init = train.drop(target, axis=1)\n",
    "                y_train_init = train[['well','FORMATION_up', target]]\n",
    "                X_train = X_train_init.drop(drop_lst_X, axis=1)\n",
    "                y_train = y_train_init.drop(drop_lst_y, axis=1)\n",
    "                model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                test = dataset[dataset.well == wellname]\n",
    "                y_test_wnames = test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "                X_test_init = test.drop(target, axis=1)\n",
    "                y_test_init = test[['well','FORMATION_up', target]]\n",
    "                X_test = X_test_init.drop(drop_lst_X, axis=1)\n",
    "                y_test = y_test_init.drop(drop_lst_y, axis=1).values[0]\n",
    "                y_pred = model.predict(X_test)\n",
    "                test = pd.DataFrame(zip(y_test, y_pred), columns=['y_orig', 'y_pred'])\n",
    "                test = pd.concat([y_test_wnames, test], axis=1)\n",
    "                df_lst.append(test)\n",
    "                \n",
    "            result = pd.concat(df_lst).reset_index(drop=True)\n",
    "            result['up'] = result['y_orig']*(1 + tolerance)\n",
    "            result['down'] = result['y_orig']*(1 - tolerance)\n",
    "            result['qc'] = 'out'\n",
    "            result.loc[(result['y_pred'] <= result.up) & (result['y_pred'] >= result.down), 'qc'] = 'in'\n",
    "            resultqc = result.qc.value_counts(normalize=True)\n",
    "\n",
    "            phit_pred = result[['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "            dataset_pred = dataset.set_index(['well','FORMATION_up']).join(phit_pred.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "            result_dict = {'result':result, 'res_full':dataset_pred, 'testqc':resultqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            return result_dict\n",
    "        def xplot_qc_1_to_all(data, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_test = data\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_test = go.Scatter( x=ds_test['y_orig'], y=ds_test['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=1, subplot_titles=(f'test qc {qc_test}',))\n",
    "            fig.add_trace(scatter_test,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.update_layout(  title_text= (comment), width=350, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_1_to_all(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance,'RandomForestRegressor')\n",
    "        # model2_ph = model_prediction_1_to_all(dataset, BayesianRidge(), target, 0.05, 'BayesianRidge')\n",
    "        # model3_ph = model_prediction_1_to_all(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, 0.05, 'XGBRegressor')\n",
    "        # model4_ph = model_prediction_1_to_all(dataset, CatBoostRegressor(random_state=42, verbose=False), target, 0.05,'CatBoostRegressor')\n",
    "        # model5_ph = model_prediction_1_to_all(dataset, AdaBoostRegressor(random_state=42), target, 0.05, 'AdaBoostRegressor')\n",
    "        # model6_ph = model_prediction_1_to_all(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, 0.05, 'LGBMRegressor')\n",
    "\n",
    "        xplot_qc_1_to_all(model1_ph['result'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model2_ph['result'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model3_ph['result'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model4_ph['result'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model5_ph['result'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model6_ph['result'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model = run_phit_pred_1_to_all(input_ph8, cluster_list, tolerance=0.05)\n",
    "\n",
    "    def concat_prediction_to_khtst_df(data_pred, data_khtst, data_main, cluster_algo):\n",
    "        phit_pred8 = data_pred['result'][['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "        khtst8 = data_khtst[data_khtst[cluster_algo].isin(cluster_list)].groupby(['well','FORMATION_up'])['khtst'].sum().reset_index()\n",
    "\n",
    "        khtst8_phit_pred8 = khtst8.set_index(['well','FORMATION_up']).join(phit_pred8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "        phitpred_khtst = khtst8_phit_pred8.set_index(['well','FORMATION_up']).join(data_main.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "        phitpred_khtst.insert(19, 'phit_pred', phitpred_khtst.pop('phit_pred'))\n",
    "        phitpred_khtst.insert(19, 'phit_wavg_target', phitpred_khtst.pop('phit_wavg_target'))\n",
    "        phitpred_khtst.insert(19, 'khtst', phitpred_khtst.pop('khtst'))\n",
    "        return phitpred_khtst\n",
    "    phitpred_khtst = concat_prediction_to_khtst_df(model, data_clustered8, input_ph8, 'kmeans')\n",
    "    print(f'Concat dataset features {textwrap.fill(str(list(phitpred_khtst.columns)), width=150)}')\n",
    "\n",
    "    print('\\nPrediction KHtst: ')\n",
    "    def run_khtst_pred_split(dataset, cluster_list, tolerance):\n",
    "\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            \"\"\"\n",
    "            'well', 'FORMATION_up', 'well_1', 'well_2', 'well_3', 'dist_1',\n",
    "            'dist_2', 'dist_3', 'phit_wavg_1', 'vsh_wavg_1', 'htst_sum_1',\n",
    "            'phit_wavg_2', 'vsh_wavg_2', 'htst_sum_2', 'phit_wavg_3', 'vsh_wavg_3',\n",
    "            'htst_sum_3', 'phit_pred', 'phit_wavg_target', 'khtst'\n",
    "            \"\"\"\n",
    "            drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3', 'dist_1', 'dist_2','dist_3', 'phit_wavg_target']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            # model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print(f'features dataset: {list(X_train.columns)}')\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            return result\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')\n",
    "        target = 'khtst'\n",
    "        model1_kh = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_kh = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_kh = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_kh = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_kh = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_kh = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        # xplot_qc2(model1_kh['result'], model1_kh['trainqc'], model1_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_kh['result'], model2_kh['trainqc'], model2_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_kh['result'], model3_kh['trainqc'], model3_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        xplot_qc2(model4_kh['result'], model4_kh['trainqc'], model4_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_kh['result'], model5_kh['trainqc'], model5_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_kh['result'], model6_kh['trainqc'], model6_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model4_kh\n",
    "    model_khtst = run_khtst_pred_split(phitpred_khtst, cluster_list, 0.25)\n",
    "    result = {'khtst_pred':model_khtst, 'khtst_data':phitpred_khtst, 'phit_pred':model['result']}\n",
    "    return result\n",
    "test_1 = khtst_workflow(cluster_list = [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3',\n",
    "                                        'dist_1', 'dist_2', 'dist_3', 'vsh_wavg_1', 'htst_sum_1', 'vsh_wavg_2', 'htst_sum_2', 'vsh_wavg_3', 'htst_sum_3', 'cluster']\n",
    "pairplot_special(test_1['khtst_data'].drop(drop_lst_X, axis=1), 7, 7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def khtst_workflow(cluster_list):\n",
    "\n",
    "    def dataset_for_spatial_prediction(dataset_full, dataset_cluster, offset_qty, cluster_algo, cluster_list):\n",
    "            \n",
    "            def joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list):\n",
    "                coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "                dataset_cluster = dataset_cluster[(dataset_cluster[cluster_algo].isin(cluster_list))]\n",
    "                result = dataset_cluster.set_index(['well','FORMATION_up']).join(coordinates.set_index(['well','FORMATION_up'])).reset_index()\n",
    "                coordinates = result[['well','FORMATION_up', 'X_mean', 'Y_mean']].groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "                return coordinates, result\n",
    "            coordinates, dataset_cluster_xy = joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list)\n",
    "            coordinates = coordinates[~coordinates.well.isin(['A14Y'])]\n",
    "\n",
    "            def well_distance_calculation(coordinates, fm):\n",
    "                coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "                df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "                well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "                result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "                return result\n",
    "            well_dist_crosstable_8 = well_distance_calculation(coordinates, 'Balakhany VIII')\n",
    "\n",
    "            def offset_well_names_dist(dataset, offset_qty):\n",
    "                df_lst = []\n",
    "                for ind in range(len(dataset.well.unique())):\n",
    "                    off_well_series = dataset.iloc[ind]\n",
    "                    off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "                    off_well_selected['well'] = off_well_selected.columns[0]\n",
    "                    off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "                    dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "                    well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_selected.columns[:-1])):\n",
    "                        col = off_well_selected.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "                    off_well_names = pd.DataFrame(col_names).T\n",
    "                    col_names = []\n",
    "                    for i in range(len(off_well_names.columns)):\n",
    "                        col = off_well_names.columns[i]\n",
    "                        col_names.append(col)\n",
    "                        off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "                    \n",
    "                    concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "                    df_lst.append(concat_well_data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_dist_data8 = offset_well_names_dist(well_dist_crosstable_8, offset_qty)\n",
    "\n",
    "            def offset_wells_features_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    data = dataset_dist[dataset_dist.well == wellname]\n",
    "                    cc = 0\n",
    "                    for j in data.columns:\n",
    "                        if 'well_' in j:\n",
    "                            cc += 1\n",
    "                            offset_wellname = data[j].values[0]\n",
    "                            data_cluster = dataset_clusters[(dataset_clusters.well == offset_wellname) & \n",
    "                                                                (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                            var_name = 'phit_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['phit_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'vsh_wavg_' + str(cc)\n",
    "                            data[var_name] = ((data_cluster['vsh_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                            var_name = 'htst_sum_' + str(cc)\n",
    "                            data[var_name] = data_cluster['htst'].sum()                \n",
    "                    df_lst.append(data)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                result['FORMATION_up'] = fm\n",
    "                return result\n",
    "            well_features8 = offset_wells_features_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "\n",
    "            def target_wells_variable_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "                df_lst = []\n",
    "                for wellname in dataset_dist.well.unique():\n",
    "                    df = pd.DataFrame({'well': [wellname], 'FORMATION_up': [fm], 'phit_wavg_target': [0]})\n",
    "                    data = dataset_clusters[(dataset_clusters.well == wellname) & \n",
    "                                            (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                    df['phit_wavg_target'] = ((data['phit_avg'] * data['htst']).sum()) / (data['htst'].sum())\n",
    "                    df_lst.append(df)\n",
    "                result = pd.concat(df_lst).reset_index(drop=True)\n",
    "                return result\n",
    "            well_target8 = target_wells_variable_calculation(well_dist_data8, dataset_cluster, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "            \n",
    "            dataset8 = well_target8.set_index(['well','FORMATION_up']).join(well_features8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "            result = {'dataset8':dataset8, 'cluster_xy':dataset_cluster_xy, 'well_dist8':well_dist_data8, 'coordinates':coordinates,\n",
    "                    'target8':well_target8, 'feature8':well_features8, 'dist_crosstable8':well_dist_crosstable_8}\n",
    "            return result\n",
    "    input_ph8 = dataset_for_spatial_prediction(df_bal_net2_kh, data_clustered8, 3, 'kmeans', cluster_list)['dataset8']\n",
    "    print(f'Dataset features {textwrap.fill(str(list(input_ph8.columns)), width=150)}')\n",
    "\n",
    "    def run_phit_pred_split(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            # model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print('features dataset: \\n', list(X_train.columns))\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('test \"in\":', '{:.2f}'.format(result['testqc'].round(2)),'\\t', model_name)\n",
    "            return result\n",
    "        def xplot_qc2(data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_train = data[data.dataset == 'train']\n",
    "            ds_test = data[data.dataset == 'test']\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "            qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=2, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}'))\n",
    "            fig.add_trace(scatter_train,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.add_trace(scatter_test,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "            fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')   \n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_ph = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_ph = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_ph = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_ph = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_ph = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        xplot_qc2(model1_ph['result'], model1_ph['trainqc'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_ph['result'], model2_ph['trainqc'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_ph['result'], model3_ph['trainqc'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc2(model4_ph['result'], model4_ph['trainqc'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_ph['result'], model5_ph['trainqc'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_ph['result'], model6_ph['trainqc'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model_split = run_phit_pred_split(input_ph8, cluster_list, tolerance=0.05)['result']\n",
    "\n",
    "    def run_phit_pred_1_to_all(dataset, cluster_list, tolerance):\n",
    "        def model_prediction_1_to_all(dataset, selected_model, target, tolerance, model_name):\n",
    "            drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "            print(model_name)\n",
    "            df_lst = []\n",
    "            for wellname in tqdm(dataset.well.unique()[:]):\n",
    "                train = dataset[dataset.well != wellname]\n",
    "                X_train_init = train.drop(target, axis=1)\n",
    "                y_train_init = train[['well','FORMATION_up', target]]\n",
    "                X_train = X_train_init.drop(drop_lst_X, axis=1)\n",
    "                y_train = y_train_init.drop(drop_lst_y, axis=1)\n",
    "                model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                test = dataset[dataset.well == wellname]\n",
    "                y_test_wnames = test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "                X_test_init = test.drop(target, axis=1)\n",
    "                y_test_init = test[['well','FORMATION_up', target]]\n",
    "                X_test = X_test_init.drop(drop_lst_X, axis=1)\n",
    "                y_test = y_test_init.drop(drop_lst_y, axis=1).values[0]\n",
    "                y_pred = model.predict(X_test)\n",
    "                test = pd.DataFrame(zip(y_test, y_pred), columns=['y_orig', 'y_pred'])\n",
    "                test = pd.concat([y_test_wnames, test], axis=1)\n",
    "                df_lst.append(test)\n",
    "                \n",
    "            result = pd.concat(df_lst).reset_index(drop=True)\n",
    "            result['up'] = result['y_orig']*(1 + tolerance)\n",
    "            result['down'] = result['y_orig']*(1 - tolerance)\n",
    "            result['qc'] = 'out'\n",
    "            result.loc[(result['y_pred'] <= result.up) & (result['y_pred'] >= result.down), 'qc'] = 'in'\n",
    "            resultqc = result.qc.value_counts(normalize=True)\n",
    "\n",
    "            phit_pred = result[['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "            dataset_pred = dataset.set_index(['well','FORMATION_up']).join(phit_pred.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "            result_dict = {'result':result, 'res_full':dataset_pred, 'testqc':resultqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            return result_dict\n",
    "        def xplot_qc_1_to_all(data, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "            data = data.round({y_orig: round, y_pred: round})\n",
    "            ds_test = data\n",
    "            up_range = rng + 1\n",
    "            dwn_range = 1 - rng\n",
    "            colors = {'in': 'green', 'out': 'red'}\n",
    "            qc_colors = [colors[qc] for qc in ds_test.qc]\n",
    "            scatter_test = go.Scatter( x=ds_test['y_orig'], y=ds_test['y_pred'],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color=qc_colors, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                        customdata = ds_test[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                        hovertemplate=\"\".join(\n",
    "                                        [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                        )\n",
    "            line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "            line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "            fig = make_subplots(rows=1, cols=1, subplot_titles=(f'test qc {qc_test}',))\n",
    "            fig.add_trace(scatter_test,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "            fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "            fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "            fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "            fig.update_layout(  title_text= (comment), width=350, height=350, \n",
    "                                margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "            return fig.show()\n",
    "\n",
    "        target = 'phit_wavg_target'\n",
    "        model1_ph = model_prediction_1_to_all(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance,'RandomForestRegressor')\n",
    "        # model2_ph = model_prediction_1_to_all(dataset, BayesianRidge(), target, 0.05, 'BayesianRidge')\n",
    "        # model3_ph = model_prediction_1_to_all(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, 0.05, 'XGBRegressor')\n",
    "        # model4_ph = model_prediction_1_to_all(dataset, CatBoostRegressor(random_state=42, verbose=False), target, 0.05,'CatBoostRegressor')\n",
    "        # model5_ph = model_prediction_1_to_all(dataset, AdaBoostRegressor(random_state=42), target, 0.05, 'AdaBoostRegressor')\n",
    "        # model6_ph = model_prediction_1_to_all(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, 0.05, 'LGBMRegressor')\n",
    "\n",
    "        xplot_qc_1_to_all(model1_ph['result'], model1_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model2_ph['result'], model2_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model3_ph['result'], model3_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model4_ph['result'], model4_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model5_ph['result'], model5_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc_1_to_all(model6_ph['result'], model6_ph['testqc'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model1_ph\n",
    "    model = run_phit_pred_1_to_all(input_ph8, cluster_list, tolerance=0.05)\n",
    "\n",
    "    def concat_prediction_to_khtst_df(data_pred, data_khtst, data_main, cluster_algo):\n",
    "        phit_pred8 = data_pred['result'][['well','FORMATION_up','y_pred']].rename(columns={'y_pred':'phit_pred'})\n",
    "        khtst8 = data_khtst[data_khtst[cluster_algo].isin(cluster_list)].groupby(['well','FORMATION_up'])['khtst'].sum().reset_index()\n",
    "\n",
    "        khtst8_phit_pred8 = khtst8.set_index(['well','FORMATION_up']).join(phit_pred8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "        phitpred_khtst = khtst8_phit_pred8.set_index(['well','FORMATION_up']).join(data_main.set_index(['well','FORMATION_up']), how='inner').reset_index()\n",
    "\n",
    "        phitpred_khtst.insert(19, 'phit_pred', phitpred_khtst.pop('phit_pred'))\n",
    "        phitpred_khtst.insert(19, 'phit_wavg_target', phitpred_khtst.pop('phit_wavg_target'))\n",
    "        phitpred_khtst.insert(19, 'khtst', phitpred_khtst.pop('khtst'))\n",
    "        return phitpred_khtst\n",
    "    phitpred_khtst = concat_prediction_to_khtst_df(model, data_clustered8, input_ph8, 'kmeans')\n",
    "    print(f'Concat dataset features {textwrap.fill(str(list(phitpred_khtst.columns)), width=150)}')\n",
    "\n",
    "    print('\\nPrediction KHtst: ')\n",
    "    def run_khtst_pred_split(dataset, cluster_list, tolerance):\n",
    "\n",
    "        def model_prediction_split(dataset, selected_model, target, tolerance, model_name, display_flag='display'):\n",
    "            \"\"\"\n",
    "            'well', 'FORMATION_up', 'well_1', 'well_2', 'well_3', 'dist_1',\n",
    "            'dist_2', 'dist_3', 'phit_wavg_1', 'vsh_wavg_1', 'htst_sum_1',\n",
    "            'phit_wavg_2', 'vsh_wavg_2', 'htst_sum_2', 'phit_wavg_3', 'vsh_wavg_3',\n",
    "            'htst_sum_3', 'phit_pred', 'phit_wavg_target', 'khtst'\n",
    "            \"\"\"\n",
    "            drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3', 'dist_1', 'dist_2','dist_3', 'phit_wavg_target']\n",
    "            drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "            X = dataset.drop(target, axis=1)\n",
    "            y = dataset[['well','FORMATION_up', target]]\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "            y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "            y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "            X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "            X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "            y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "            y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "            # model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "            model = selected_model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            y_train = np.array(y_train).flatten()\n",
    "            y_test = np.array(y_test).flatten()\n",
    "            train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "            train = pd.concat([y_train_wnames, train], axis=1)\n",
    "            test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "            test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "            train['up'] = train['y_orig']*(1 + tolerance)\n",
    "            train['down'] = train['y_orig']*(1 - tolerance)\n",
    "            train['qc'] = 'out'\n",
    "            train['dataset'] = 'train'\n",
    "            train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "            trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "            test['up'] = test['y_orig']*(1 + tolerance)\n",
    "            test['down'] = test['y_orig']*(1 - tolerance)\n",
    "            test['qc'] = 'out'\n",
    "            test['dataset'] = 'test'\n",
    "            test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "            testqc = test.qc.value_counts(normalize=True)\n",
    "            df = pd.concat([train, test])\n",
    "            df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "            result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "            if display_flag == 'display':\n",
    "                print(f'features dataset: {list(X_train.columns)}')\n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            else: \n",
    "                print('train \"in\":', '{:.2f}'.format(result['trainqc'].round(2)),\n",
    "                    'test \"in\":', '{:.2f}'.format(result['testqc'].round(2)), \n",
    "                    '\\t', model_name)\n",
    "            return result\n",
    "\n",
    "        print(f'Cluster list is: {cluster_list}')\n",
    "        target = 'khtst'\n",
    "        model1_kh = model_prediction_split(dataset, RandomForestRegressor(n_jobs=-1, random_state=42), target, tolerance, 'RandomForestRegressor','display')\n",
    "        model2_kh = model_prediction_split(dataset, BayesianRidge(), target, tolerance, 'BayesianRidge', 'dont_display')\n",
    "        model3_kh = model_prediction_split(dataset, XGBRegressor(n_jobs=-1, random_state=42, verbosity=0), target, tolerance, 'XGBRegressor', 'dont_display')\n",
    "        model4_kh = model_prediction_split(dataset, CatBoostRegressor(random_state=42, verbose=False), target, tolerance, 'CatBoostRegressor', 'dont_display')\n",
    "        model5_kh = model_prediction_split(dataset, AdaBoostRegressor(random_state=42), target, tolerance, 'AdaBoostRegressor', 'dont_display')\n",
    "        model6_kh = model_prediction_split(dataset, LGBMRegressor(n_jobs=-1, random_state=42, verbose=0, verbosity=-1), target, tolerance, 'LGBMRegressor', 'dont_display')\n",
    "\n",
    "        # xplot_qc2(model1_kh['result'], model1_kh['trainqc'], model1_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'RandomForestRegressor {cluster_list}')\n",
    "        # xplot_qc2(model2_kh['result'], model2_kh['trainqc'], model2_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'BayesianRidge {cluster_list}')\n",
    "        # xplot_qc2(model3_kh['result'], model3_kh['trainqc'], model3_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'XGBRegressor {cluster_list}')\n",
    "        xplot_qc2(model4_kh['result'], model4_kh['trainqc'], model4_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'CatBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model5_kh['result'], model5_kh['trainqc'], model5_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'AdaBoostRegressor {cluster_list}')\n",
    "        # xplot_qc2(model6_kh['result'], model6_kh['trainqc'], model6_kh['testqc'], 'y_orig', 'y_pred', 27000, tolerance, 0, 3, f'LGBMRegressor {cluster_list}')\n",
    "        return model4_kh\n",
    "    model_khtst = run_khtst_pred_split(phitpred_khtst, cluster_list, 0.25)\n",
    "    result = {'khtst_pred':model_khtst, 'khtst_data':phitpred_khtst, 'phit_pred':model['result']}\n",
    "    return result\n",
    "test_2 = khtst_workflow(cluster_list = [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "drop_lst_X = ['well','FORMATION_up',    'well_1', 'well_2', 'well_3',\n",
    "                                        'dist_1', 'dist_2', 'dist_3', 'vsh_wavg_1', 'htst_sum_1', 'vsh_wavg_2', 'htst_sum_2', 'vsh_wavg_3', 'htst_sum_3','cluster']\n",
    "pairplot_special(test_2['khtst_data'].drop(drop_lst_X, axis=1), 7, 7, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_linestringz_polygon(dataset):\n",
    "    geom = [x for x in dataset.geometry]\n",
    "    df_lst = []\n",
    "    for i in range(len(geom)):\n",
    "        all_coords = mapping(geom[i])['coordinates']\n",
    "        lats = [x[1] for x in all_coords]\n",
    "        lons = [x[0] for x in all_coords]\n",
    "        polyg = Polygon(zip(lons, lats))\n",
    "        df = gpd.GeoDataFrame(index=[0], crs='EPSG:2499', geometry=[polyg])\n",
    "        df_lst.append(df)\n",
    "    result = pd.concat(df_lst).reset_index(drop=True)\n",
    "    return result \n",
    "\n",
    "bal8_1510_3 = gpd.read_file(r'C:\\jupyter\\SPP\\surfaces\\petrel\\BalakhanyVIII_1510_base_3.shp').set_crs('EPSG:2499')\n",
    "bal8_20_3 = gpd.read_file(r'C:\\jupyter\\SPP\\surfaces\\petrel\\BalakhanyVIII_20_base_3.shp').set_crs('EPSG:2499')\n",
    "bal8_sand_3 = gpd.read_file(r'C:\\jupyter\\SPP\\surfaces\\petrel\\BalakhanyVIII_30_base_3.shp').set_crs('EPSG:2499')\n",
    "\n",
    "bal8_1510_3_polygon = convert_linestringz_polygon(bal8_1510_3)\n",
    "bal8_20_3_polygon = convert_linestringz_polygon(bal8_20_3)\n",
    "bal8_sand_3_polygon = convert_linestringz_polygon(bal8_sand_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CHIRAG', 'CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI', 'DWG', 'DDGG', 'WEST CHIRAG'\n",
    "def polygon_by_field(dataset, field, buffer):\n",
    "    data = dataset[dataset.FORMATION_up == 'Balakhany VIII'][['well','X_mean','Y_mean','field']]\n",
    "    data = data[data.field == field]\n",
    "    data = data.drop('field', axis=1).groupby('well').mean().reset_index()\n",
    "    geometry_fld = [Point(xy) for xy in zip(data['X_mean'], data['Y_mean'])]\n",
    "    data = gpd.GeoDataFrame(data, geometry=geometry_fld).drop(['X_mean','Y_mean'], axis=1)\n",
    "    buffers_fld = data.buffer(buffer)\n",
    "    buffers_fld = gpd.GeoDataFrame(geometry=buffers_fld)\n",
    "    data = data.join(buffers_fld, rsuffix='_polygon')\n",
    "    data = gpd.GeoDataFrame(data, geometry='geometry_polygon').set_crs('EPSG:2499')\n",
    "    field_polygon = gpd.GeoSeries(data['geometry_polygon'].unary_union.convex_hull)\n",
    "    return field_polygon\n",
    "dwg = polygon_by_field(df_bal_net2_kh, 'DWG', 500)\n",
    "chirag = polygon_by_field(df_bal_net2_kh, 'CHIRAG', 500)\n",
    "wchirag = polygon_by_field(df_bal_net2_kh, 'WEST CHIRAG', 500)\n",
    "cazeri = polygon_by_field(df_bal_net2_kh, 'CENTRAL AZERI', 500)\n",
    "wazeri = polygon_by_field(df_bal_net2_kh, 'WEST AZERI', 500)\n",
    "eazeri = polygon_by_field(df_bal_net2_kh, 'EAST AZERI', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_polygons_n_points(dataset, fm, field_list):\n",
    "    bdl8_xy = dataset[(dataset.FORMATION_up == fm) & (dataset.field.isin(field_list))][['well','X_mean','Y_mean']]\n",
    "    bdl8_xy = bdl8_xy.groupby('well').mean().reset_index()\n",
    "    geometry = [Point(xy) for xy in zip(bdl8_xy['X_mean'], bdl8_xy['Y_mean'])]\n",
    "    bdl8_xy_gpd = gpd.GeoDataFrame(bdl8_xy, geometry=geometry).drop(['X_mean','Y_mean'], axis=1)\n",
    "\n",
    "    buffers = bdl8_xy_gpd.buffer(250)\n",
    "    buffers = gpd.GeoDataFrame(geometry=buffers)\n",
    "    bdl8_xy_gpd = bdl8_xy_gpd.join(buffers, rsuffix='_polygon')\n",
    "    bdl8_xy_buff = gpd.GeoDataFrame(bdl8_xy_gpd, geometry='geometry_polygon').drop('geometry', axis=1).set_crs('EPSG:2499')\n",
    "    bdl8_xy_points = gpd.GeoDataFrame(bdl8_xy_gpd, geometry='geometry').drop('geometry_polygon', axis=1).set_crs('EPSG:2499')\n",
    "\n",
    "    fields_polyg_hull = gpd.GeoSeries(bdl8_xy_buff.unary_union.convex_hull)\n",
    "    return bdl8_xy_buff, bdl8_xy_points, fields_polyg_hull\n",
    "bdl8_xy_buff, bdl8_xy_points, fields_polyg_hull = draw_polygons_n_points(df_bal_net2_kh, 'Balakhany VIII',['WEST AZERI','CENTRAL AZERI','EAST AZERI','CHIRAG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_khtst_by_fu():\n",
    "    well_lst = df_bal_net2_kh[(df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) & \n",
    "                            (df_bal_net2_kh.FORMATION.str.contains('Balakhany VIII')) & (df_bal_net2_kh.KHtst.notna())]\n",
    "    result_well_lst = well_lst.groupby(['well','FORMATION'])['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    result_well_lst_sum = result_well_lst.groupby('FORMATION')['KHtst'].sum().reset_index()\n",
    "    result_well_lst_sum = result_well_lst_sum.sort_values(by='KHtst', ascending=False)\n",
    "    return result_well_lst_sum\n",
    "# calc_khtst_by_fu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_into_bal8_sand = gpd.sjoin(bdl8_xy_points, bal8_sand_3_polygon, op='within')\n",
    "\n",
    "def gpd_polygons_wells(geobody_polygons, wells_points_df, title):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    geobody_polygons.plot(ax=ax, color='yellow', label='bal8_1510_3', alpha=0.66)\n",
    "\n",
    "    wells_points_df[['well','geometry']].plot(ax=ax, color='black', marker='*', markersize = 50, alpha=0.5, ec='black')\n",
    "    # bdl8_xy_buff.plot(ax=ax, color='green', label='wells', alpha=0.5)\n",
    "    bdl8_xy_points.plot(ax=ax, markersize = 1, color='black', label='wells', alpha=1)\n",
    "\n",
    "    # fields_polyg_hull.plot(ax=ax, alpha=0.25, label='ACG polygon')\n",
    "    \n",
    "    # dwg.plot(ax=ax, alpha=0.25, color = 'orange', label='dwg')\n",
    "    chirag.plot(ax=ax, alpha=0.25, color = 'red', label='chirag')\n",
    "    # wchirag.plot(ax=ax, alpha=0.25, color = 'purple', label='chirag')\n",
    "    cazeri.plot(ax=ax, alpha=0.25, color = 'green', label='c azeri')\n",
    "    wazeri.plot(ax=ax, alpha=0.25, color = 'blue', label='w azeri')\n",
    "    eazeri.plot(ax=ax, alpha=0.25, color = 'turquoise', label='e azeri')\n",
    "    # ax.grid()\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "gpd_polygons_wells(bal8_sand_3_polygon, well_into_bal8_sand, 'Polygons of Balakhany VIII sand body #3 & wells (buffer 250m)')\n",
    "\n",
    "def wells_in_out_polygon(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "\n",
    "    well_lst = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))]\n",
    "    well_in_geob =  well_lst[well_lst.well.isin(geobody_well_lst)]\n",
    "    well_out_geob =  well_lst[~well_lst.well.isin(geobody_well_lst)]\n",
    "\n",
    "    well_in_geob_khtst = well_in_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_in_geob_khtst['geobody'] = 'in'\n",
    "    well_out_geob_khtst = well_out_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_out_geob_khtst['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_in_geob_khtst, well_out_geob_khtst])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x='KHtst', hue='geobody', log_scale=False, palette=custom_palette, ax=ax[0])\n",
    "    sns.boxplot(concat_df, x=\"geobody\", y=\"KHtst\", palette=custom_palette, ax=ax[1])\n",
    "    ax[0].set_title(title)\n",
    "    ax[1].set_title(title)\n",
    "    ax[0].grid(which='both')\n",
    "    return wells_points_df, concat_df\n",
    "well_into_bal8_sand_3, concat_df_25 = wells_in_out_polygon(df_bal_net2_kh, well_into_bal8_sand,['Balakhany VIII sand'],\n",
    "                                                        'bal8_30_3 polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_into_bal8_20_3 = gpd.sjoin(bdl8_xy_points, bal8_20_3_polygon, op='within')\n",
    "\n",
    "def gpd_polygons_wells(geobody_polygons, wells_points_df, title):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    geobody_polygons.plot(ax=ax, color='orange', label='bal8_1510_3', alpha=0.66)\n",
    "\n",
    "    wells_points_df[['well','geometry']].plot(ax=ax, color='black', marker='*', markersize = 50, alpha=0.5, ec='black')\n",
    "    # bdl8_xy_buff.plot(ax=ax, color='green', label='wells', alpha=0.5)\n",
    "    bdl8_xy_points.plot(ax=ax, markersize = 1, color='black', label='wells', alpha=1)\n",
    "\n",
    "    # fields_polyg_hull.plot(ax=ax, alpha=0.25, label='ACG polygon')\n",
    "    \n",
    "    # dwg.plot(ax=ax, alpha=0.25, color = 'orange', label='dwg')\n",
    "    chirag.plot(ax=ax, alpha=0.25, color = 'red', label='chirag')\n",
    "    # wchirag.plot(ax=ax, alpha=0.25, color = 'purple', label='chirag')\n",
    "    cazeri.plot(ax=ax, alpha=0.25, color = 'green', label='c azeri')\n",
    "    wazeri.plot(ax=ax, alpha=0.25, color = 'blue', label='w azeri')\n",
    "    eazeri.plot(ax=ax, alpha=0.25, color = 'turquoise', label='e azeri')\n",
    "    # ax.grid()\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "gpd_polygons_wells(bal8_20_3_polygon, well_into_bal8_20_3, 'Polygons of Balakhany VIII 20 body #3 & wells (buffer 250m)')\n",
    "\n",
    "def wells_in_out_polygon(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "\n",
    "    well_lst = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))]\n",
    "    well_in_geob =  well_lst[well_lst.well.isin(geobody_well_lst)]\n",
    "    well_out_geob =  well_lst[~well_lst.well.isin(geobody_well_lst)]\n",
    "\n",
    "    well_in_geob_khtst = well_in_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_in_geob_khtst['geobody'] = 'in'\n",
    "    well_out_geob_khtst = well_out_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_out_geob_khtst['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_in_geob_khtst, well_out_geob_khtst])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x='KHtst', hue='geobody', log_scale=False, palette=custom_palette, ax=ax[0])\n",
    "    sns.boxplot(concat_df, x=\"geobody\", y=\"KHtst\", palette=custom_palette, ax=ax[1])\n",
    "    ax[0].set_title(title)\n",
    "    ax[1].set_title(title)\n",
    "    ax[0].grid(which='both')\n",
    "    return wells_points_df\n",
    "well_into_bal8_20_3 = wells_in_out_polygon(df_bal_net2_kh, well_into_bal8_20_3,['Balakhany VIII 20'],'bal8_20_3 polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_into_bal8_1510_3 = gpd.sjoin(bdl8_xy_points, bal8_1510_3_polygon, op='within')\n",
    "\n",
    "def gpd_polygons_wells(geobody_polygons, wells_points_df, title):\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    geobody_polygons.plot(ax=ax, color='red', label='bal8_1510_3', alpha=0.66)\n",
    "\n",
    "    wells_points_df[['well','geometry']].plot(ax=ax, color='black', marker='*', markersize = 50, alpha=0.5, ec='black')\n",
    "    # bdl8_xy_buff.plot(ax=ax, color='green', label='wells', alpha=0.5)\n",
    "    bdl8_xy_points.plot(ax=ax, markersize = 1, color='black', label='wells', alpha=1)\n",
    "\n",
    "    # fields_polyg_hull.plot(ax=ax, alpha=0.25, label='ACG polygon')\n",
    "    \n",
    "    # dwg.plot(ax=ax, alpha=0.25, color = 'orange', label='dwg')\n",
    "    chirag.plot(ax=ax, alpha=0.25, color = 'red', label='chirag')\n",
    "    # wchirag.plot(ax=ax, alpha=0.25, color = 'purple', label='chirag')\n",
    "    cazeri.plot(ax=ax, alpha=0.25, color = 'green', label='c azeri')\n",
    "    wazeri.plot(ax=ax, alpha=0.25, color = 'blue', label='w azeri')\n",
    "    eazeri.plot(ax=ax, alpha=0.25, color = 'turquoise', label='e azeri')\n",
    "    # ax.grid()\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "gpd_polygons_wells(bal8_1510_3_polygon, well_into_bal8_1510_3,'Polygons of Balakhany VIII 15 10 body #3 & wells (buffer 250m)')\n",
    "\n",
    "def wells_in_out_polygon(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "\n",
    "    well_lst = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))]\n",
    "    well_in_geob =  well_lst[well_lst.well.isin(geobody_well_lst)]\n",
    "    well_out_geob =  well_lst[~well_lst.well.isin(geobody_well_lst)]\n",
    "\n",
    "    well_in_geob_khtst = well_in_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_in_geob_khtst['geobody'] = 'in'\n",
    "    well_out_geob_khtst = well_out_geob.groupby('well')['KHtst'].apply(lambda x: x.iloc[0] - x.iloc[-1]).reset_index()\n",
    "    well_out_geob_khtst['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_in_geob_khtst, well_out_geob_khtst])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x='KHtst', hue='geobody', log_scale=False, palette=custom_palette, ax=ax[0])\n",
    "    sns.boxplot(concat_df, x=\"geobody\", y=\"KHtst\", palette=custom_palette, ax=ax[1])\n",
    "    ax[0].set_title(title)\n",
    "    ax[1].set_title(title)\n",
    "    ax[0].grid(which='both')\n",
    "    return wells_points_df\n",
    "well_into_bal8_1510_3 = wells_in_out_polygon(df_bal_net2_kh, well_into_bal8_1510_3, ['Balakhany VIII 15', 'Balakhany VIII 10'],\n",
    "                                             'bal8_1510_3 polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strench one curve to another one with python\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define two curves (arrays)\n",
    "curve1 = np.array([1, 2, 3, 4, 5])\n",
    "curve2 = np.array([1.5, 2.8, 3.3, 4.2, 4.9])\n",
    "\n",
    "# Normalize curves\n",
    "curve1_norm = curve1 / curve1.max()\n",
    "curve2_norm = curve2 / curve2.max()\n",
    "\n",
    "# Define a function to minimize the difference between the curves\n",
    "def objective(params):\n",
    "    scale, shift = params\n",
    "    return np.sum((curve2_norm - scale * curve1_norm - shift) ** 2)\n",
    "\n",
    "# Minimize the objective function to find scaling and shifting parameters\n",
    "initial_guess = [1.0, 0.0]  # Initial guess for scale and shift\n",
    "result = minimize(objective, initial_guess)\n",
    "\n",
    "# Extract scaling and shifting parameters\n",
    "scale, shift = result.x\n",
    "\n",
    "# Stretch curve1 to match curve2\n",
    "stretched_curve1 = scale * curve1 + shift\n",
    "\n",
    "print(\"Scaling factor:\", scale)\n",
    "print(\"Shift factor:\", shift)\n",
    "print(\"Stretched curve 1:\", stretched_curve1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geobody in vs out comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wells_in_out_polygon_extended(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "    dataset = dataset[~dataset.well.str.contains('GCA')]\n",
    "    well_lst_in = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.well.isin(geobody_well_lst))\n",
    "                       & (dataset.NET_clp2 == 1)]\n",
    "    well_lst_out = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.NET_clp2 == 1)\n",
    "                       & (~dataset.well.isin(geobody_well_lst))]\n",
    "\n",
    "    well_lst_in['geobody'] = 'in'\n",
    "    well_lst_out['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_lst_in, well_lst_out])\n",
    "\n",
    "    def ntg_calculation_in(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_in_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_in_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_in_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_in_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'in'\n",
    "        return result\n",
    "    def ntg_calculation_out(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_in_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_in_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_in_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_in_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'out'\n",
    "        return result\n",
    "\n",
    "    ntg_in = ntg_calculation_in(dataset, wells_points_df, flow_units)\n",
    "    ntg_out = ntg_calculation_out(dataset, wells_points_df, flow_units)\n",
    "    ntg_total = pd.concat([ntg_in, ntg_out])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x=\"PHIT\", hue=\"geobody\", palette=custom_palette, ax=ax[0], alpha=0.66)\n",
    "    ax[0].set_title(title + ' phit')\n",
    "    sns.kdeplot(concat_df, x=\"VSH\", hue=\"geobody\", palette=custom_palette, ax=ax[1], alpha=0.66)\n",
    "    ax[1].set_title(title + ' vsh')\n",
    "    sns.kdeplot(concat_df, x=\"LPERM\", hue=\"geobody\", palette=custom_palette, ax=ax[2], alpha=0.66, log_scale=True)\n",
    "    ax[2].set_title(title + ' lperm')\n",
    "    sns.kdeplot(ntg_total, x=\"ntg\", hue=\"geobody\", palette=custom_palette, ax=ax[3], alpha=0.66)\n",
    "    ax[3].set_title(title + ' ntg')\n",
    "    return concat_df, ntg_total\n",
    "\n",
    "res_phit_etc, ntg = wells_in_out_polygon_extended(df_bal_net2_kh, well_into_bal8_1510_3, ['Balakhany VIII 15', 'Balakhany VIII 10'],'bal8_1510_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wells_in_out_polygon_extended(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "    dataset = dataset[~dataset.well.str.contains('GCA')]\n",
    "    well_lst_in = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.well.isin(geobody_well_lst))\n",
    "                       & (dataset.NET_clp2 == 1)]\n",
    "    well_lst_out = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.NET_clp2 == 1)\n",
    "                       & (~dataset.well.isin(geobody_well_lst))]\n",
    "\n",
    "    well_lst_in['geobody'] = 'in'\n",
    "    well_lst_out['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_lst_in, well_lst_out])\n",
    "\n",
    "    def ntg_calculation_in(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_in_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_in_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_in_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_in_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'in'\n",
    "        return result\n",
    "    def ntg_calculation_out(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_out_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_out_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_out_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_out_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'out'\n",
    "        return result\n",
    "\n",
    "    ntg_in = ntg_calculation_in(dataset, wells_points_df, flow_units)\n",
    "    ntg_out = ntg_calculation_out(dataset, wells_points_df, flow_units)\n",
    "    ntg_total = pd.concat([ntg_in, ntg_out])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x=\"PHIT\", hue=\"geobody\", palette=custom_palette, ax=ax[0], alpha=0.66)\n",
    "    ax[0].set_title(title + ' phit')\n",
    "    sns.kdeplot(concat_df, x=\"VSH\", hue=\"geobody\", palette=custom_palette, ax=ax[1], alpha=0.66)\n",
    "    ax[1].set_title(title + ' vsh')\n",
    "    sns.kdeplot(concat_df, x=\"LPERM\", hue=\"geobody\", palette=custom_palette, ax=ax[2], alpha=0.66, log_scale=True)\n",
    "    ax[2].set_title(title + ' lperm')\n",
    "    sns.kdeplot(ntg_total, x=\"ntg\", hue=\"geobody\", palette=custom_palette, ax=ax[3], alpha=0.66)\n",
    "    ax[3].set_title(title + ' ntg')\n",
    "    return concat_df, ntg_total\n",
    "\n",
    "res_phit_etc, ntg = wells_in_out_polygon_extended(df_bal_net2_kh, well_into_bal8_20_3, ['Balakhany VIII 20'],'bal8_20_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wells_in_out_polygon_extended(dataset, wells_points_df, flow_units, title):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "    dataset = dataset[~dataset.well.str.contains('GCA')]\n",
    "    well_lst_in = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.well.isin(geobody_well_lst))\n",
    "                       & (dataset.NET_clp2 == 1)]\n",
    "    well_lst_out = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.NET_clp2 == 1)\n",
    "                       & (~dataset.well.isin(geobody_well_lst))]\n",
    "\n",
    "    well_lst_in['geobody'] = 'in'\n",
    "    well_lst_out['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_lst_in, well_lst_out])\n",
    "\n",
    "    def ntg_calculation_in(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_in_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_in_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_in_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_in_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'in'\n",
    "        return result\n",
    "    def ntg_calculation_out(dataset, wells_points_df, flow_units):\n",
    "        wells_points_df = wells_points_df.well.values\n",
    "        well_lst_in_1 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 1)]\n",
    "        well_lst_in_0 = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                            & (dataset.FORMATION.isin(flow_units))\n",
    "                            & (~dataset.well.isin(geobody_well_lst))\n",
    "                            & (dataset.NET_clp2 == 0)]\n",
    "        well_net = well_lst_in_1.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_net = well_net.reset_index().rename(columns={'NET_clp2':'net'})\n",
    "        well_notnet = well_lst_in_0.groupby('well')['NET_clp2'].count()*0.1\n",
    "        well_notnet = well_notnet.reset_index().rename(columns={'NET_clp2':'not_net'})\n",
    "        result = well_net.set_index('well').join(well_notnet.set_index('well')).reset_index()\n",
    "        result['total'] = result.net + result.not_net\n",
    "        result['ntg'] = result.net/result.total\n",
    "        result['geobody'] = 'out'\n",
    "        return result\n",
    "\n",
    "    ntg_in = ntg_calculation_in(dataset, wells_points_df, flow_units)\n",
    "    ntg_out = ntg_calculation_out(dataset, wells_points_df, flow_units)\n",
    "    ntg_total = pd.concat([ntg_in, ntg_out])\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    custom_palette = {'in': 'red', 'out': 'lightgreen'}\n",
    "    sns.kdeplot(concat_df, x=\"PHIT\", hue=\"geobody\", palette=custom_palette, ax=ax[0], alpha=0.66)\n",
    "    ax[0].set_title(title + ' phit')\n",
    "    sns.kdeplot(concat_df, x=\"VSH\", hue=\"geobody\", palette=custom_palette, ax=ax[1], alpha=0.66)\n",
    "    ax[1].set_title(title + ' vsh')\n",
    "    sns.kdeplot(concat_df, x=\"LPERM\", hue=\"geobody\", palette=custom_palette, ax=ax[2], alpha=0.66, log_scale=False)\n",
    "    ax[2].set_title(title + ' lperm')\n",
    "    sns.kdeplot(ntg_total, x=\"ntg\", hue=\"geobody\", palette=custom_palette, ax=ax[3], alpha=0.66)\n",
    "    ax[3].set_title(title + ' ntg')\n",
    "    return concat_df, ntg_total\n",
    "\n",
    "res_phit_etc, ntg = wells_in_out_polygon_extended(df_bal_net2_kh, well_into_bal8_sand_3, ['Balakhany VIII 25'],'bal8_25_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geobody Bal8 Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bal8 1510"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wells_in_out_polygon_extended_v2(dataset, wells_points_df, flow_units):   \n",
    "    geobody_well_lst = wells_points_df.well.values\n",
    "    dataset = dataset[~dataset.well.isin(['GCA6Z'])]\n",
    "    well_lst_in = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (dataset.well.isin(geobody_well_lst))]\n",
    "    well_lst_out = dataset[(dataset.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])) \n",
    "                       & (dataset.FORMATION.isin(flow_units))\n",
    "                       & (~dataset.well.isin(geobody_well_lst))]\n",
    "    well_lst_in['geobody'] = 'in'\n",
    "    well_lst_out['geobody'] = 'out'\n",
    "    concat_df = pd.concat([well_lst_in, well_lst_out])\n",
    "    return concat_df\n",
    "concat_df = wells_in_out_polygon_extended_v2(df_bal_net2_kh, well_into_bal8_1510_3, ['Balakhany VIII 15', 'Balakhany VIII 10'])\n",
    "geobody_in = concat_df[concat_df.geobody == 'in']\n",
    "geobody_out = concat_df[concat_df.geobody == 'out']\n",
    "print('geobody_in', geobody_in.well.unique())\n",
    "print('geobody_out', geobody_out.well.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azeri_bal8_1510 = df_bal_net2_kh[df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                 & (df_bal_net2_kh.FORMATION.isin(['Balakhany VIII 15', 'Balakhany VIII 10']))]\n",
    "\n",
    "def display_well_traj_polygon_run(dataset, polygon, mult):\n",
    "    def well_traj_dataprep(dataset):\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-100:100]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[['X_mean', 'Y_mean', 'KHtst', 'TVD_SCS', 'Status']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep(dataset)\n",
    "    def display_well_traj_polygon(trajectory, map_data_middle, fmname, mult, path, comment, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle.KHtst, size=map_data_middle.KHtst*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure()  \n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', \n",
    "                                            fillcolor = 'orange', line=dict(color='orange'),\n",
    "                                            fill='toself', name='Polygon')\n",
    "                fig.add_trace(polygon_plotly)\n",
    "        polygon_drawing(polygon)  \n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' 1510 polygons. Size of bubbles is KHtst.'),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + comment, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'Balakhany8_KHtst', 'dont_print')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_run(azeri_bal8_1510, bal8_1510_3_polygon, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_geobody_in_out(dataset, wellname_in, wellname_out):\n",
    "    fig, ax = plt.subplots(1, 8, figsize=(10, 5))\n",
    "    def display_geobody(dataset, wellname, fm, col, geobody):\n",
    "        data = dataset[(dataset.well==wellname)]\n",
    "        data['PHIT_clipped'] = data['PHIT']\n",
    "        data.loc[data.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "        y = data.TST\n",
    "        gr = data.GR_N\n",
    "        phit_avg = data.PHIT\n",
    "        phit_cliped = data.PHIT_clipped\n",
    "        net = data.NET_clp2\n",
    "        perm = data.LPERM\n",
    "        kh = data.KHtst\n",
    "        # print(  'KH orig:', kh.iloc[0].round(0))\n",
    "        ax[col].plot(gr, y, color='lightgreen', label='gr_n')\n",
    "        ax[col].set_xlim(0, 100)\n",
    "        ax[col].invert_yaxis()\n",
    "        # ax[col].legend(fontsize=10)\n",
    "        ax[col+1].plot(phit_cliped, y, color='red', label='phit_cliped')\n",
    "        ax[col+1].plot(phit_avg, y, color='green',ls='--', label='phit')\n",
    "        ax[col+1].plot(net, y, color='orange', alpha=0.33)\n",
    "        ax[col+1].fill_betweenx(y,net, color='orange', alpha=0.33)\n",
    "        ax[col+1].set_xlim(0, 0.3)\n",
    "        ax[col+1].invert_yaxis()\n",
    "        # ax[col+1].legend(fontsize=10)\n",
    "        ax[col+2].plot(perm, y, color='purple', lw=2, label='perm')\n",
    "        ax[col+2].invert_yaxis()\n",
    "        ax[col+2].set_xscale('log')\n",
    "        # ax[col+2].legend(fontsize=8)\n",
    "        ax[col+3].plot(kh, y, color='black')\n",
    "        ax[col+3].invert_yaxis()\n",
    "        ax[col].set_title(wellname)\n",
    "        ax[col+1].set_title(fm)\n",
    "        ax[col+2].set_title(geobody)\n",
    "        ax[col+3].set_title(kh.iloc[0].round(0))\n",
    "        fig.show()\n",
    "    display_geobody(dataset, wellname_in, 'Balakhany VIII', 0, 'IN')\n",
    "    display_geobody(dataset, wellname_out, 'Balakhany VIII', 4, 'OUT')\n",
    "    plt.suptitle('Balakhany VIII 15, Balakhany VIII 10')\n",
    "display_geobody_in_out(concat_df, 'B06','B43')\n",
    "# display_geobody_in_out(concat_df, 'B12','B01ST1')\n",
    "# display_geobody_in_out(concat_df, 'B19','B26')\n",
    "# display_geobody_in_out(concat_df, 'B39','B38Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bal8 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azeri_bal8_20 = df_bal_net2_kh[df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                  & (df_bal_net2_kh.FORMATION.isin(['Balakhany VIII 20']))]\n",
    "\n",
    "def display_well_traj_polygon_run(dataset, polygon, mult):\n",
    "    def well_traj_dataprep(dataset):\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-100:100]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[['X_mean', 'Y_mean', 'KHtst', 'TVD_SCS', 'Status']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep(dataset)\n",
    "    def display_well_traj_polygon(trajectory, map_data_middle, fmname, mult, path, comment, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle.KHtst, size=map_data_middle.KHtst*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure()\n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', \n",
    "                                            fillcolor = 'orange', line=dict(color='orange'),\n",
    "                                            fill='toself', name='Polygon')\n",
    "                fig.add_trace(polygon_plotly)       \n",
    "        polygon_drawing(polygon)\n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' 20 polygons. Size of bubbles is KHtst.'),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + comment, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'Balakhany8_KHtst', 'dont_print')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_run(azeri_bal8_20, bal8_20_3_polygon, 0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bal8 sand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azeri_bal8_sand = df_bal_net2_kh[df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                  & (df_bal_net2_kh.FORMATION.isin(['Balakhany VIII sand']))]\n",
    "\n",
    "def display_well_traj_polygon_run(dataset, polygon, mult):\n",
    "    def well_traj_dataprep(dataset):\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-100:100]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[['X_mean', 'Y_mean', 'KHtst', 'TVD_SCS', 'Status']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep(dataset)\n",
    "    def display_well_traj_polygon(trajectory, map_data_middle, fmname, mult, path, comment, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle.KHtst, size=map_data_middle.KHtst*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure() \n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', \n",
    "                                            fillcolor = 'orange', line=dict(color='orange'),\n",
    "                                            fill='toself', name='Polygon')\n",
    "                fig.add_trace(polygon_plotly)\n",
    "        polygon_drawing(polygon)       \n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' Sand polygons. Size of bubbles is KHtst.'),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + comment, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'Balakhany8_KHtst', 'dont_print')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_run(azeri_bal8_sand, bal8_sand_3_polygon, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NET data analize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NET cleaning and joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetThicknessDistribution():\n",
    "\n",
    "    def metadata_parquet_loading():\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "        metadata_init = pd.read_csv(path + 'ACG_wells_metadata.csv', sep=',')\n",
    "        metadata = metadata_init.copy()\n",
    "        metadata = metadata.rename(columns={'X':'X_wellhead', 'Y':'Y_wellhead'})\n",
    "        metadata.Status = metadata.Status.str.strip()\n",
    "        metadata.Status = metadata.Status.str.lower()\n",
    "        metadata.loc[metadata.Status == 'oil', 'Status' ] = 'production oil'\n",
    "        metadata.loc[metadata.Status == 'oil producer', 'Status' ] = 'production oil'\n",
    "        metadata.loc[metadata.Status == 'production', 'Status' ] = 'production oil'\n",
    "        metadata.loc[metadata.Status == 'produiction oil', 'Status' ] = 'production oil'\n",
    "        metadata.loc[metadata.Status == 'production_oil', 'Status' ] = 'production oil'\n",
    "        metadata.loc[metadata.Status == 'abandoned production oil', 'Status' ] = 'abandoned oil'\n",
    "        metadata.loc[metadata.Status == 'abandoned  oil', 'Status' ] = 'abandoned oil'\n",
    "        metadata.loc[metadata.Status == 'abandoned oi', 'Status' ] = 'abandoned oil'\n",
    "        metadata.loc[metadata.Status == 'injector  - water', 'Status' ] = 'injector - water'\n",
    "        metadata.loc[metadata.Status == 'injector water', 'Status' ] = 'injector - water'\n",
    "        metadata.loc[metadata.Status == 'injetor  - water', 'Status' ] = 'injector - water'\n",
    "        metadata.loc[metadata.Status == 'abandoned injector - water per b', 'Status' ] = 'abandoned injector - water'\n",
    "        metadata.loc[metadata.Status == 'plugged and abandoned', 'Status' ] = 'p&a'\n",
    "        metadata.loc[metadata.X_wellhead==118.270, 'X_wellhead'] = 526258.84\n",
    "        metadata.loc[metadata.Y_wellhead==526261.510, 'Y_wellhead'] = 4435802.01\n",
    "        metadata.loc[metadata.well=='C39', 'X_wellhead'] = 526258.840\n",
    "        metadata.loc[metadata.well=='C39', 'Y_wellhead'] = 4435802.010\n",
    "        metadata.loc[metadata.field=='West Azeri', 'field'] = 'WEST AZERI'\n",
    "        metadata.loc[metadata.field=='COP', 'field'] = 'WEST CHIRAG'\n",
    "        metadata.loc[metadata.well=='AZERI2', 'field'] = 'WEST AZERI'\n",
    "        metadata.loc[metadata.well=='AZERI3', 'field'] = 'WEST AZERI'\n",
    "        metadata.loc[metadata.well=='B31', 'field'] = 'CENTRAL AZERI'\n",
    "        metadata.loc[metadata.well=='J28_bpQIP', 'field'] = 'WEST CHIRAG'\n",
    "\n",
    "        #Read data from parquet\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "        df_prq = pd.read_parquet(path + 'ACG_wells_JOINT_BEST_v10.parquet.gzip')\n",
    "        df_prq.rename(columns={'wellName':'well'}, inplace=True)\n",
    "        df_prq = df_prq.set_index('well').join(metadata.set_index('well')).reset_index()\n",
    "        # print('wells in df totally:', len(df_prq.well.unique()))\n",
    "        # Filter data with bad_well_list \n",
    "        bad_well_list = ['E10Z','Predrill_J01Z', 'Predrill_J08', 'J28_bpQIP', 'A01W_2']\n",
    "        df_prq = df_prq[~df_prq.well.isin(bad_well_list)]\n",
    "        #Assign any Fluidcode_mod number by variable gross_pay=1 and gross_pay=0 if Fluidcode_mod as NaN\n",
    "        df_prq.loc[df_prq.FLUIDS>0, 'FLUIDS_int'] = 1\n",
    "        df_prq.loc[df_prq.FLUIDS<=0, 'FLUIDS_int'] = 0\n",
    "        df_prq.FLUIDS_int = df_prq.FLUIDS_int.astype('int')\n",
    "        # Unite of FU for each formation\n",
    "\n",
    "        df_bal = df_prq[df_prq.FORMATION.str.contains('Balakhany')]\n",
    "        df_bal.loc[df_bal.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "        df_bal.loc[df_bal.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "        df_bal = df_bal[df_bal.FORMATION_up.notna()]\n",
    "        #Getting XY mean coords of Balakhany formation\n",
    "        xy_coord_mean = df_bal[['well', 'FORMATION_up', 'X', 'Y']]\n",
    "        xy_coord_mean = xy_coord_mean.groupby(['well', 'FORMATION_up']).agg({'X': 'mean', 'Y':'mean'}).reset_index()\n",
    "        xy_coord_mean = xy_coord_mean.rename(columns={'X':'X_mean', 'Y':'Y_mean'})\n",
    "        xy_coord_mean = xy_coord_mean[xy_coord_mean.FORMATION_up.str.contains('Balakhany') & (xy_coord_mean.X_mean>0) & (xy_coord_mean.Y_mean>0)]\n",
    "        df_bal.rename(columns={'X':'X_traj', 'Y':'Y_traj'}, inplace=True)\n",
    "        df_bal = df_bal.set_index(['well', 'FORMATION_up']).join(xy_coord_mean.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "        return df_bal\n",
    "    df_bal = metadata_parquet_loading()\n",
    "\n",
    "    def well_clean_v2():\n",
    "        #Counting of bad quality logs\n",
    "        bal8_list = [   'Balakhany VIII sand', 'Balakhany VIII 20',   'Balakhany VIII 10',   'Balakhany VIII 25',\n",
    "                        'Balakhany VIII 15',    'Balakhany VIII 5']\n",
    "        well_tot8 = []\n",
    "        well_zero8 = []\n",
    "        well_clean8 = []\n",
    "        for j in df_bal[(df_bal.FORMATION.isin(bal8_list) & (df_bal.PHIT>0))].well.unique():\n",
    "            phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII')]))\n",
    "            phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII') & (df_bal.PHIT > 0)]))\n",
    "            well_tot8.append(j)\n",
    "            if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "                well_zero8.append(j)\n",
    "            else:\n",
    "                well_clean8.append(j)\n",
    "                # well_display_ntd(df_bal, j, 'Balakhany VIII', 'NET', round((phit_nonzero/phit_zero),2), 1) #printing well plots with high quality logs\n",
    "\n",
    "        bal10_list = ['Balakhany X', 'Balakhany X sand', 'Balakhany X 40', 'Balakhany X 20', 'Balakhany X 50']\n",
    "        well_tot10 = []\n",
    "        well_zero10 = []\n",
    "        well_clean10 = []\n",
    "        for j in df_bal[(df_bal.FORMATION.isin(bal10_list) & (df_bal.PHIT>0))].well.unique():\n",
    "            phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X')]))\n",
    "            phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X') & (df_bal.PHIT > 0)]))\n",
    "            well_tot10.append(j)\n",
    "            if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "                # well_display_ntd(df_bal, j, 'Balakhany X', 'NET', round((phit_nonzero/phit_zero),2), 1)\n",
    "                well_zero10.append(j)\n",
    "            else:\n",
    "                well_clean10.append(j)\n",
    "        print('well_tot8', len(well_tot8))\n",
    "        print('well_zero8', len(well_zero8))\n",
    "        print('well_clean8', len(well_clean8))\n",
    "        print('----------------------')\n",
    "        print('well_tot10', len(well_tot10))\n",
    "        print('well_zero10', len(well_zero10))\n",
    "        print('well_clean10', len(well_clean10))\n",
    "\n",
    "        # broken wells Bal8\n",
    "        # A08, A19, H01Z, J05 \n",
    "        # broken wells Bal10\n",
    "        # C31, D25\n",
    "        # high tst_interv Bal8\n",
    "        # E30Z\n",
    "        # small tst_interv Bal8\n",
    "        # G01Z, E05, E01, E01Y, E11Z, E07, H01Y, H01Z, A14\n",
    "        # Add wells after review Bal8 \n",
    "        # D04Z, \n",
    "        # Remove wells from clean_list by any reasons\n",
    "        remove_tst8 = ['A08','A19','J05','E30Z','G01Z', 'E05', 'E01', 'E01Y', 'E11Z', 'E07', 'H01Y', 'H01Z', 'A14']\n",
    "        well_clean8_v2 = [i for i in well_clean8 if i not in remove_tst8]\n",
    "        remove_tst10 = ['C31','D25', 'E21A']\n",
    "        well_clean10_v2 = [i for i in well_clean10 if i not in remove_tst10]\n",
    "        print('----------------------')\n",
    "        print('well_clean8_v2: ', len(well_clean8_v2))\n",
    "        print('well_clean10_v2: ', len(well_clean10_v2))\n",
    "        return well_clean8_v2, well_clean10_v2\n",
    "    well_clean8_v2, well_clean10_v2 = well_clean_v2()\n",
    "\n",
    "    # Limitation dataframe to cleaned wells for Bal8 & Bal10\n",
    "    df_net_bal8 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal8 = df_net_bal8[df_net_bal8.well.isin(well_clean8_v2) & (df_net_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net_bal10 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal10 = df_net_bal10[df_net_bal10.well.isin(well_clean10_v2) & (df_net_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation dataframe with h_tst from MD to TST for Bal8\n",
    "\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var='NET'):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_list8 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal8')\n",
    "    for well in tqdm(df_net_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal8, well, 'Balakhany VIII', 'NET')\n",
    "        df_list8.append(df)\n",
    "    ntd_net_8 = pd.concat(df_list8)\n",
    "    df_list10 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal10')\n",
    "    for well in tqdm(df_net_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal10, well, 'Balakhany X', 'NET')\n",
    "        df_list10.append(df)\n",
    "    ntd_net_10 = pd.concat(df_list10)\n",
    "    ntd_net_final = pd.concat([ntd_net_8, ntd_net_10])\n",
    "\n",
    "    # Cleaning NET variable and making up NET_clp with clipped data, join NET_clp to main dataframe\n",
    "    def ntd_htst_cleaning(dataset, cutoff):\n",
    "        \"\"\"\n",
    "        dataset - any updated dataset like df_bal...\n",
    "        cutoff - value in TST to remove layers with thickness below cutoff\n",
    "        \"\"\"\n",
    "        df_list_ntd = []\n",
    "        for well in tqdm(dataset.well.unique()):\n",
    "            ntd_well = dataset[(dataset.well ==well)]\n",
    "            ntd_well_cutoff = ntd_well[ntd_well.h_tst >= cutoff]\n",
    "            well_short = df_bal[['well', 'FORMATION_up', 'MD', 'TST', 'GR_N', 'NET', 'FORMATION']]\n",
    "            net_well = well_short[(well_short.well==well)]\n",
    "            net_well['NET_clp'] = 0\n",
    "            for j in range(len(ntd_well_cutoff.well)):\n",
    "                ntd_top = ntd_well_cutoff.iloc[j, 2].round(3)\n",
    "                ntd_bot = ntd_well_cutoff.iloc[j, 3].round(3)\n",
    "                for i in range(len(net_well.TST)):\n",
    "                    well_tst = net_well['TST'].iloc[i].round(3)\n",
    "                    if well_tst >= ntd_top and well_tst <= ntd_bot:\n",
    "                        net_well['NET_clp'].iloc[i] = 1\n",
    "            df_list_ntd.append(net_well)\n",
    "        net_clp = pd.concat(df_list_ntd)\n",
    "        return net_clp\n",
    "    print('Cleaning NET variable and making up NET_clp with clipped data')\n",
    "    net_clp =  ntd_htst_cleaning(ntd_net_final, 1)\n",
    "    df_bal_net = df_bal.set_index(['well','MD']).join(net_clp.drop(\n",
    "        ['FORMATION_up','NET','TST', 'FORMATION', 'GR_N'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net = df_bal_net[df_bal_net.NET_clp.notna()]\n",
    "\n",
    "    # Cleaning NET_clp from 1-point zero\n",
    "    print('Cleaning NET_clp from 1-point zero')\n",
    "    for i in tqdm(range(len(df_bal_net.NET_clp))):\n",
    "        if (df_bal_net.NET_clp.iloc[i] == 0 and  \n",
    "            df_bal_net.NET_clp.iloc[i-1] == 1 and \n",
    "            df_bal_net.NET_clp.iloc[i+1] == 1):\n",
    "            df_bal_net.NET_clp.iloc[i] = 1        \n",
    "\n",
    "    df_zero_bal = df_bal_net[['well', 'MD', 'TST', 'NET_clp', 'FORMATION_up']]\n",
    "    df_zero_bal8 = df_zero_bal[df_zero_bal.well.isin(well_clean8_v2) & (df_zero_bal.FORMATION_up=='Balakhany VIII')]\n",
    "    df_zero_bal10 = df_zero_bal[df_zero_bal.well.isin(well_clean10_v2) & (df_zero_bal.FORMATION_up=='Balakhany X')]\n",
    "    \n",
    "    def ntd_calculation_zero(dataset,well,formation, net_var='NET'):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==formation)]\n",
    "        data.iloc[0, 3] = 1\n",
    "        data.iloc[-1, 3] = 1\n",
    "        tst_zero_top = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 0 and data.iloc[i-1][net_var] == 1)]\n",
    "        tst_zero_bot = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1) \n",
    "                    if (data.iloc[i][net_var] == 0 and data.iloc[i+1][net_var] == 1)]\n",
    "        tops_zero = zip(tst_zero_top, tst_zero_bot)\n",
    "        df_zero_htst = pd.DataFrame(tops_zero, columns=['tst_zero_top', 'tst_zero_bot'])\n",
    "        df_zero_htst['FORMATION_up'] = formation\n",
    "        df_zero_htst['well'] = well\n",
    "        df_zero_htst['h_tst_zero'] = df_zero_htst.tst_zero_bot - df_zero_htst.tst_zero_top\n",
    "        df_zero_htst = df_zero_htst[['well','FORMATION_up','tst_zero_top','tst_zero_bot','h_tst_zero']]\n",
    "        return df_zero_htst\n",
    "    print('NET-zero layers removing Bal8')\n",
    "    df_zero_list8 = []\n",
    "    for well in tqdm(df_zero_bal8.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal8, well, 'Balakhany VIII', 'NET_clp')\n",
    "        df_zero_list8.append(df)\n",
    "    ntd_zero_8 = pd.concat(df_zero_list8)\n",
    "    \n",
    "    print('NET-zero layers removing Bal10')\n",
    "    df_zero_list10 = []\n",
    "    for well in tqdm(df_zero_bal10.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal10, well, 'Balakhany X', 'NET_clp')\n",
    "        df_zero_list10.append(df)\n",
    "    ntd_zero_10 = pd.concat(df_zero_list10)\n",
    "    \n",
    "    ntd_zero = pd.concat([ntd_zero_8, ntd_zero_10])\n",
    "\n",
    "    # NET-zero layers removing\n",
    "    print('Run ntd_htst_zero_cleaning')\n",
    "    def ntd_htst_zero_cleaning(dataset_zero, dataset, cutoff, net_var1, net_var2):\n",
    "        df_list_ntd_zero = []\n",
    "        for well in tqdm(dataset_zero.well.unique()):\n",
    "            ntd_well_zero = dataset_zero[(dataset_zero.well ==well)]\n",
    "            ntd_well_zero_sel = ntd_well_zero[ntd_well_zero.h_tst_zero <= cutoff]\n",
    "            well_zero_short = dataset[['well','FORMATION_up','MD','TST', net_var1, 'GR_N', 'NET', 'FORMATION']]\n",
    "            well_zero_short[net_var2] = well_zero_short[net_var1]\n",
    "            well_zero_sel = well_zero_short[(well_zero_short.well==well)]\n",
    "            for j in range(len(ntd_well_zero_sel.well)):\n",
    "                ntd_zero_top = ntd_well_zero_sel.iloc[j, 2].round(3)\n",
    "                ntd_zero_bot = ntd_well_zero_sel.iloc[j, 3].round(3)\n",
    "                for i in range(len(well_zero_sel.TST)):\n",
    "                    well_zero_tst = well_zero_sel['TST'].iloc[i].round(3)\n",
    "                    if well_zero_tst >= ntd_zero_top and well_zero_tst <= ntd_zero_bot:\n",
    "                        well_zero_sel[net_var2].iloc[i] = 1\n",
    "            df_list_ntd_zero.append(well_zero_sel)\n",
    "        result = pd.concat(df_list_ntd_zero)\n",
    "        return result\n",
    "    net_clp2 = ntd_htst_zero_cleaning(ntd_zero, df_bal_net, 1, 'NET_clp', 'NET_clp2')\n",
    "\n",
    "    #Joining NET_clp2 to main dataframe df_bal_net\n",
    "    df_bal_net2 = df_bal_net.set_index(['well','MD']).join(net_clp2.drop(\n",
    "        ['FORMATION_up','GR_N', 'NET','NET_clp', 'FORMATION','TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net2 = df_bal_net2[df_bal_net2.NET_clp2.notna()]\n",
    "\n",
    "    # KHtst calculation and join to the main dataframe df_bal_net2\n",
    "    def calculation_khtst(dataset, net_var):\n",
    "        df_smpl_lst = []\n",
    "        print('TST sampling calculation')\n",
    "        for well_smpl in tqdm(dataset.well.unique()[:]):\n",
    "            tst_sampl = dataset[dataset.well==well_smpl]['TST'].diff()\n",
    "            df_new = dataset[dataset.well==well_smpl].join(tst_sampl, rsuffix='_smpl')    \n",
    "            df_smpl_lst.append(df_new)\n",
    "        df_bal_tst_smpl = pd.concat(df_smpl_lst)\n",
    "        df_kh_lst_fm = []\n",
    "        print('KHtst calculation')\n",
    "        for fm_kh in ['Balakhany VIII', 'Balakhany X']:\n",
    "            df_kh_lst = []\n",
    "            for well_kh in tqdm(dataset.well.unique()[:]):\n",
    "                well_tst_perm = df_bal_tst_smpl[(df_bal_tst_smpl.well==well_kh) & \n",
    "                                                (df_bal_tst_smpl.FORMATION_up==fm_kh)].sort_values(by='MD', ascending=False)\n",
    "                well_tst_perm.loc[well_tst_perm[net_var] == 0, 'LPERM'] = 0\n",
    "                well_tst_perm.loc[well_tst_perm[net_var] == 0, 'PHIT'] = 0\n",
    "                well_tst_perm.loc[well_tst_perm[net_var] == 0, 'VSH'] = 0\n",
    "                well_tst_perm['khtst'] = well_tst_perm.LPERM*well_tst_perm.TST_smpl\n",
    "                well_tst_perm['phithtst'] = well_tst_perm.PHIT*well_tst_perm.TST_smpl\n",
    "                well_tst_perm['vshhtst'] = well_tst_perm.VSH*well_tst_perm.TST_smpl\n",
    "                well_tst_perm['KHtst'] = well_tst_perm.khtst.cumsum()\n",
    "                well_tst_perm['PHITHtst'] = well_tst_perm.phithtst.cumsum()\n",
    "                well_tst_perm['VSHHtst'] = well_tst_perm.vshhtst.cumsum()\n",
    "                well_tst_perm = well_tst_perm.sort_values(by='MD')\n",
    "                df_kh_lst.append(well_tst_perm)\n",
    "            df_khlst = pd.concat(df_kh_lst)\n",
    "            df_kh_lst_fm.append(df_khlst)\n",
    "        df_khlst_fm = pd.concat(df_kh_lst_fm)\n",
    "        # df_khlst_fm = df_khlst_fm.dropna()\n",
    "        return df_khlst_fm[['well', 'FORMATION_up', 'MD', 'TST', 'TST_smpl','KHtst','PHITHtst','VSHHtst']]\n",
    "    df_kh = calculation_khtst(df_bal_net2, 'NET_clp2')\n",
    "    df_bal_net2_kh_init = df_bal_net2.set_index(['well','MD']).join(df_kh.drop(['FORMATION_up', 'TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net2_kh8 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean8_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany VIII')]\n",
    "    df_bal_net2_kh10 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean10_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany X')]\n",
    "    df_bal_net2_kh = pd.concat([df_bal_net2_kh8, df_bal_net2_kh10])\n",
    "\n",
    "    # Displaying of ramdom well for example\n",
    "    def well_display_net(dataset, well, formation, net1='NET_clp', net2_flag=0, net2='NET_clp_v2'):\n",
    "        well_sel = dataset[(dataset.well == well) & (dataset.FORMATION_up == formation)]\n",
    "        depth = well_sel['TST']\n",
    "        grn = well_sel['GR_N']\n",
    "        net = well_sel['NET']\n",
    "        net_clp = well_sel[net1]\n",
    "        if net2_flag == 0:\n",
    "            fig, ax = plt.subplots(1,3, figsize=(4.5,8), sharey=True)\n",
    "            ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "            ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "            well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "                ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "                ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "            ax[1].plot(net, depth, color='orange'), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "            ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "            ax[2].plot(net_clp, depth, color='orange'), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "            ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "            fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "            fig.tight_layout()\n",
    "        if net2_flag == 1:\n",
    "            net_clp2 = well_sel[net2]\n",
    "            fig, ax = plt.subplots(1,4, figsize=(6,8), sharey=True)\n",
    "            ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "            ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "            well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "                ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "                ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "            ax[1].plot(net, depth, color='orange', lw=0.25), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "            ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "            ax[2].plot(net_clp, depth, color='orange', lw=0.25), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "            ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "            ax[3].plot(net_clp2, depth, color='orange', lw=0.25), ax[3].set_xlim(0, 1), ax[3].grid(axis='y')\n",
    "            ax[3].fill_betweenx(depth,net_clp2, color='orange', alpha=0.33)\n",
    "            fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "            fig.tight_layout()\n",
    "        return fig.show()\n",
    "    well_display_net(df_bal_net2_kh, 'J28', 'Balakhany VIII', 'NET_clp', 1, 'NET_clp2')\n",
    "    return df_bal_net2_kh\n",
    "\n",
    "df_bal_net2_kh = NetThicknessDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geobodies vs NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal_net2_kh_v2 = df_bal_net2_kh[~df_bal_net2_kh.well.isin(['GCA2', 'GCA6', 'GCA6Y', 'GCA6Z', 'GCA7']) \n",
    "                                   & (df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']))]\n",
    "well_bal8 = df_bal_net2_kh_v2[(df_bal_net2_kh_v2.FORMATION_up == 'Balakhany VIII')]\n",
    "well_bal10 = df_bal_net2_kh_v2[(df_bal_net2_kh_v2.FORMATION_up == 'Balakhany X')]\n",
    "\n",
    "def interpolate_by_depth_fm_run(dataset_logs, formation_name, step):\n",
    "    def interpolate_by_depth_fm(one_well, formation_name, step):\n",
    "        one_well = one_well.sort_values(by='TST')\n",
    "        well_name = one_well[\"well\"].iloc[0]\n",
    "        data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "        starting_tst = one_well[\"TST\"].iloc[0]\n",
    "        new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "        interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET = interp1d(one_well['TST'], one_well['NET'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_VSH = interp1d(one_well['TST'], one_well['NET_VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp = interp1d(one_well['TST'], one_well['NET_clp'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_VSH = interp1d(one_well['TST'], one_well['VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_GR = interp1d(one_well['TST'], one_well['GR_N'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_MD = interp1d(one_well['TST'], one_well['MD'], kind='linear', fill_value=\"extrapolate\")\n",
    "        # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "        new_data = {\n",
    "            'well': [well_name for _ in range(len(new_TST_values))],\n",
    "            'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "            'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "            'TST': new_TST_values,\n",
    "            'X_traj': interp_X(new_TST_values),\n",
    "            'Y_traj': interp_Y(new_TST_values),\n",
    "            'PHIT': interp_PHIT(new_TST_values),\n",
    "            'TVD_SCS': interp_TVD(new_TST_values),\n",
    "            'NET': interp_NET(new_TST_values),\n",
    "            'NET_VSH': interp_NET_VSH(new_TST_values),\n",
    "            'NET_clp': interp_NET_clp(new_TST_values),\n",
    "            'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "            'LPERM': interp_LPERM(new_TST_values),\n",
    "            'KHtst': interp_KHtst(new_TST_values),\n",
    "            'VSH': interp_VSH(new_TST_values),\n",
    "            'GR_N': interp_GR(new_TST_values),\n",
    "            'MD': interp_MD(new_TST_values),\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        return new_df\n",
    "    df_lst = []\n",
    "    print(f'Start interpolation of {formation_name}')\n",
    "    for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "        well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "        well_interp = interpolate_by_depth_fm(well_sel, formation_name, step)\n",
    "        df_lst.append(well_interp)\n",
    "    result = pd.concat(df_lst)\n",
    "    result = result.round({'MD':1, 'TVD_SCS':1, 'TST':1})\n",
    "    return result\n",
    "well_bal8_interp = interpolate_by_depth_fm_run(well_bal8, 'Balakhany VIII', 0.1)\n",
    "\n",
    "def well_bal_interp_join(dataset):\n",
    "    data_fu = df_bal_net2_kh[['well','MD','FORMATION', 'field']]\n",
    "    well_bal8_interp_v2 = dataset.set_index(['well','MD']).join(data_fu.set_index(['well','MD'])).reset_index()\n",
    "    well_bal8_interp_v2.insert(3, 'FORMATION', well_bal8_interp_v2.pop('FORMATION'))\n",
    "    well_bal8_interp_v2.insert(14, 'tst_index', well_bal8_interp_v2.pop('tst_index'))\n",
    "    return well_bal8_interp_v2\n",
    "well_bal8_interp_v2 = well_bal_interp_join(well_bal8_interp)\n",
    "well_bal8_interp_v2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azeri_bal8_1510 = well_bal8_interp_v2[well_bal8_interp_v2.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                      & (well_bal8_interp_v2.FORMATION.isin(['Balakhany VIII 15', 'Balakhany VIII 10']))]\n",
    "\n",
    "def display_well_traj_polygon_fu_run(dataset, polygon, size_var, mult, comment):\n",
    "    def well_traj_dataprep_fu(dataset):\n",
    "        def takes_diff(series):\n",
    "            return series.iloc[0] - series.iloc[-1]\n",
    "        def takes_tops(series):\n",
    "            return series.iloc[0]\n",
    "        def sum_net(series):\n",
    "            return series.sum()*0.1\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-1:25]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[\n",
    "            ['X_traj', 'Y_traj', 'KHtst', 'TVD_SCS', 'NET', 'NET_VSH', 'NET_clp', 'NET_clp2']\n",
    "            ].agg({'X_traj': 'mean', 'Y_traj': 'mean', 'KHtst': takes_diff, 'TVD_SCS':takes_tops, \n",
    "                   'NET':sum_net, 'NET_VSH':sum_net, 'NET_clp':sum_net, 'NET_clp2':sum_net}).reset_index()\n",
    "        map_data_middle = map_data_middle.round({ 'NET':0, 'NET_VSH':0, 'NET_clp':0, 'NET_clp2':0})\n",
    "        map_data_middle = map_data_middle.rename(columns={'X_traj':'X_mean', 'Y_traj':'Y_mean'})\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep_fu(dataset)\n",
    "    def display_well_traj_polygon_fu(trajectory, map_data_middle, fmname, mult, path, print_name, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle[size_var], size=map_data_middle[size_var]*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', size_var]],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},var:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure()  \n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', fill='toself', name='Polygon',\n",
    "                                            fillcolor='rgba(245, 187, 39, 0.5)', line=dict(color='rgba(245, 187, 39, 1)', width=2))\n",
    "                fig.add_trace(polygon_plotly)\n",
    "        polygon_drawing(polygon)  \n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' ' + comment + size_var),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + print_name, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon_fu(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'bal8_polygon1510_net_vsh', 'notprint')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_fu_run(azeri_bal8_1510, bal8_1510_3_polygon, 'NET_VSH', 0.5, \n",
    "                                                                           '1510 polygons. Size of bubbles is ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bal8_1510_3_polygon # bal8_20_3_polygon # bal8_sand_3_polygon\n",
    "azeri_bal8_20 = well_bal8_interp_v2[well_bal8_interp_v2.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                  & (well_bal8_interp_v2.FORMATION.isin(['Balakhany VIII 20']))]\n",
    "\n",
    "def display_well_traj_polygon_fu_run(dataset, polygon, size_var, mult, comment):\n",
    "    def well_traj_dataprep_fu(dataset):\n",
    "        def takes_diff(series):\n",
    "            return series.iloc[0] - series.iloc[-1]\n",
    "        def takes_tops(series):\n",
    "            return series.iloc[0]\n",
    "        def sum_net(series):\n",
    "            return series.sum()*0.1\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-1:25]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[\n",
    "            ['X_traj', 'Y_traj', 'KHtst', 'TVD_SCS', 'NET', 'NET_VSH', 'NET_clp', 'NET_clp2']\n",
    "            ].agg({'X_traj': 'mean', 'Y_traj': 'mean', 'KHtst': takes_diff, 'TVD_SCS':takes_tops, \n",
    "                   'NET':sum_net, 'NET_VSH':sum_net, 'NET_clp':sum_net, 'NET_clp2':sum_net}).reset_index()\n",
    "        map_data_middle = map_data_middle.round({ 'NET':0, 'NET_VSH':0, 'NET_clp':0, 'NET_clp2':0})\n",
    "        map_data_middle = map_data_middle.rename(columns={'X_traj':'X_mean', 'Y_traj':'Y_mean'})\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep_fu(dataset)\n",
    "    def display_well_traj_polygon_fu(trajectory, map_data_middle, fmname, mult, path, print_name, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle[size_var], size=map_data_middle[size_var]*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', size_var]],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},var:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure()  \n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', fill='toself', name='Polygon',\n",
    "                                            fillcolor='rgba(245, 187, 39, 0.5)', line=dict(color='rgba(245, 187, 39, 1)', width=2))\n",
    "                fig.add_trace(polygon_plotly)\n",
    "        polygon_drawing(polygon)  \n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' ' + comment + size_var),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + print_name, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon_fu(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'bal8_polygon20_net_vsh', 'notprint')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_fu_run(azeri_bal8_20, bal8_20_3_polygon, 'NET_VSH', 1,\n",
    "                                                                           '20 polygons. Size of bubbles is ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azeri_bal8_sand = well_bal8_interp_v2[well_bal8_interp_v2.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']) \n",
    "                                  & (well_bal8_interp_v2.FORMATION.isin(['Balakhany VIII sand']))]\n",
    "\n",
    "def display_well_traj_polygon_fu_run(dataset, polygon, size_var, mult, comment):\n",
    "    def well_traj_dataprep_fu(dataset):\n",
    "        def takes_diff(series):\n",
    "            return series.iloc[0] - series.iloc[-1]\n",
    "        def takes_tops(series):\n",
    "            return series.iloc[0]\n",
    "        def sum_net(series):\n",
    "            return series.sum()*0.1\n",
    "        map_data = dataset.dropna()\n",
    "        map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-1:25]).reset_index()\n",
    "        map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "        map_data_middle = map_data.groupby(['well','FORMATION_up'])[\n",
    "            ['X_traj', 'Y_traj', 'KHtst', 'TVD_SCS', 'NET', 'NET_VSH', 'NET_clp', 'NET_clp2']\n",
    "            ].agg({'X_traj': 'mean', 'Y_traj': 'mean', 'KHtst': takes_diff, 'TVD_SCS':takes_tops, \n",
    "                   'NET':sum_net, 'NET_VSH':sum_net, 'NET_clp':sum_net, 'NET_clp2':sum_net}).reset_index()\n",
    "        map_data_middle = map_data_middle.round({ 'NET':0, 'NET_VSH':0, 'NET_clp':0, 'NET_clp2':0})\n",
    "        map_data_middle = map_data_middle.rename(columns={'X_traj':'X_mean', 'Y_traj':'Y_mean'})\n",
    "        map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "        return map_trajectory_display, map_data_middle\n",
    "    map_trajectory_display, map_data_middle = well_traj_dataprep_fu(dataset)\n",
    "    def display_well_traj_polygon_fu(trajectory, map_data_middle, fmname, mult, path, print_name, print_flag):\n",
    "        trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "        map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "        map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "        traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='black', size=1),\n",
    "                            customdata = trajectory[['well']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                            )\n",
    "        wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            marker=dict(color=map_data_middle[size_var], size=map_data_middle[size_var]*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                        line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = map_data_middle[['well', size_var]],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},var:%{customdata[1]}<extra></extra>\"]))\n",
    "        wells_centers = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                            mode='markers',\n",
    "                            marker=dict(symbol='diamond', color='black', size=3, opacity=1),\n",
    "                            customdata = map_data_middle[['well', 'KHtst']],\n",
    "                            hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "        fig = go.Figure()  \n",
    "        def polygon_drawing(polygon):\n",
    "            for i in range(len(polygon.geometry)):\n",
    "                lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "                polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', fill='toself', name='Polygon',\n",
    "                                            fillcolor='rgba(245, 187, 39, 0.5)', line=dict(color='rgba(245, 187, 39, 1)', width=2))\n",
    "                fig.add_trace(polygon_plotly)\n",
    "        polygon_drawing(polygon)  \n",
    "\n",
    "        fig.add_trace(traj)\n",
    "        fig.add_trace(wells)\n",
    "        fig.add_trace(wells_centers)\n",
    "        fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' ' + comment + size_var),\n",
    "                            autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "        if print_flag == 'print':\n",
    "            go_offline.plot(fig, filename=path + print_name, validate=True, auto_open=False)\n",
    "        else:\n",
    "            pass\n",
    "        return fig.show()\n",
    "    display_well_traj_polygon_fu(map_trajectory_display, map_data_middle, 'Balakhany VIII', mult, 'plots/', 'bal8_polygon_sand_net_vsh', 'notprint')\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = display_well_traj_polygon_fu_run(azeri_bal8_sand, bal8_sand_3_polygon, 'NET_VSH', 1.5,\n",
    "                                                                           'Sand polygons. Size of bubbles is ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking cluster 3 on Bal8 fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow_tracking_clusters():\n",
    "    def clustering_data_calculation(dataset):\n",
    "        df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                    'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "        df_net2_bal8 = df_net2_bal8[df_net2_bal8.FORMATION_up=='Balakhany VIII']\n",
    "        df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                    'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "        df_net2_bal10 = df_net2_bal10[df_net2_bal10.FORMATION_up=='Balakhany X']\n",
    "        # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "        print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "        def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "            data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "            data.iloc[0, 3] = 0\n",
    "            data.iloc[-1, 3] = 0\n",
    "            tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                        if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "            tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                        if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "            tops = zip(tst_top, tst_bot)\n",
    "            df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "            df_htst['FORMATION_up'] = desired_fm\n",
    "            df_htst['well'] = well\n",
    "            df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "            df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "            return df_htst\n",
    "        df_recalc_list8 = []\n",
    "        for well in tqdm(df_net2_bal8.well.unique()):\n",
    "            df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "            df_recalc_list8.append(df)\n",
    "        ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "        ntd_net2_8.drop_duplicates(inplace=True)\n",
    "        df_recalc_list10 = []\n",
    "        for well in tqdm(df_net2_bal10.well.unique()):\n",
    "            df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "            df_recalc_list10.append(df)\n",
    "        ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "        ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "        print('Calculation values for NTD Bal8 and Bal10')\n",
    "        def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "            well_data = []\n",
    "            well_formation = fmname\n",
    "            df_lst = []\n",
    "            for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "                ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "                well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "                fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "                fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "                well_phit = []\n",
    "                well_vsh = []\n",
    "                well_gperm = []\n",
    "                well_top = []\n",
    "                well_bot = []\n",
    "                well_h = []\n",
    "                well_fm_top = []\n",
    "                well_fm_bot = []\n",
    "                well_name = []\n",
    "                well_fm = []\n",
    "                well_khtst = []\n",
    "                for layers in range(len(ntd_well_avgprop.well)):\n",
    "                    ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                    ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                    ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                    phit_lst = []\n",
    "                    vsh_lst = []\n",
    "                    perm_lst = []\n",
    "                    khtst_lst = []\n",
    "                    for depth in range(len(well_avgprop_sel.TST)):\n",
    "                        well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                        if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                            phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                            vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                            perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                            khtst_lst.append(well_avgprop_sel['KHtst'].iloc[depth])\n",
    "                    well_name.append(well)\n",
    "                    well_fm.append(well_formation)\n",
    "                    well_phit.append(mean(phit_lst))\n",
    "                    well_vsh.append(mean(vsh_lst))\n",
    "                    well_gperm.append(gmean(perm_lst))\n",
    "                    well_khtst.append(khtst_lst[0] - khtst_lst[-1])\n",
    "                    well_h.append(ntd_h)\n",
    "                    well_top.append(ntd_top)\n",
    "                    well_bot.append(ntd_bot)\n",
    "                    well_fm_top.append(fm_top)\n",
    "                    well_fm_bot.append(fm_bot)\n",
    "                    well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_khtst, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                    well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                                'phit_avg',\n",
    "                                                                'vsh_avg', \n",
    "                                                                'perm_avg',\n",
    "                                                                'khtst',\n",
    "                                                                'htst',\n",
    "                                                                'top_tst',\n",
    "                                                                'bot_tst',\n",
    "                                                                'fm_top_tst',\n",
    "                                                                'fm_bot_tst'])\n",
    "                    well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                    well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'khtst','htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "                df_lst.append(well_df)\n",
    "            result = pd.concat(df_lst)\n",
    "            return result\n",
    "        ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "        ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "        ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "        return ntd_val_final\n",
    "    ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "    ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "    # ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "    def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "        def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "            row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "            row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "            return row_change\n",
    "        df_list = []\n",
    "        for wellname in dataset_ntd.well.unique():\n",
    "            df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "            df_list.append(df)\n",
    "        res_df_list = pd.concat(df_list)\n",
    "        result = pd.concat([dataset_ntd, res_df_list])\n",
    "        result = result.sort_values(by=['well','top_tst'])\n",
    "        result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "        return result_final\n",
    "    ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "    # ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')\n",
    "\n",
    "    def top_phit_bot_clustering(dataset):\n",
    "        print('Top & bot calculation')\n",
    "        def top_phit_bot_collection_run(dataset):\n",
    "            def top_phit_bot_collection(dataset, wellname):\n",
    "                data = dataset[dataset.well == wellname]\n",
    "                data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "                data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "                data['bot_htst'] = data['not_htst']\n",
    "                data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'khtst',\n",
    "                            'top_htst','htst','bot_htst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "                return data\n",
    "            df_lst = []\n",
    "            for wellname in tqdm(dataset.well.unique()):\n",
    "                res_df = top_phit_bot_collection(dataset, wellname)\n",
    "                df_lst.append(res_df)\n",
    "            top_phi_bot_cluster = pd.concat(df_lst).reset_index(drop=True)\n",
    "            return top_phi_bot_cluster\n",
    "        top_phi_bot_cluster = top_phit_bot_collection_run(dataset)\n",
    "\n",
    "        def top_phit_bot_ntg_run(dataset):\n",
    "            def top_phit_bot_ntg(dataset, wellname):\n",
    "                ntg = []\n",
    "                data = dataset[dataset.well == wellname].reset_index(drop=True)\n",
    "                for ind, row in data.iterrows():\n",
    "                    if ind == 0:\n",
    "                        ntg.append(row['htst']/(row['bot_htst'] + row['htst']))\n",
    "                    if ind != 0:\n",
    "                        ntg.append(row['htst']/(row['bot_htst'] + row['htst'] + row['top_htst']))\n",
    "                    if ind == len(data):\n",
    "                        ntg.append(row['htst']/(row['top_htst'] + row['htst']))\n",
    "                result = pd.concat([data, pd.DataFrame({'ntg':ntg})], axis=1)\n",
    "                return result\n",
    "            df_lst = []\n",
    "            for wellname in dataset.well.unique():\n",
    "                df = top_phit_bot_ntg(dataset, wellname)\n",
    "                df_lst.append(df)\n",
    "            top_phi_bot_cluster_ntg = pd.concat(df_lst).reset_index(drop=True)\n",
    "            return top_phi_bot_cluster_ntg\n",
    "        top_phi_bot_cluster_ntg = top_phit_bot_ntg_run(top_phi_bot_cluster)\n",
    "        \n",
    "        return top_phi_bot_cluster_ntg\n",
    "    top_phi_bot_cluster8 = top_phit_bot_clustering(ntd_val_final8_clean)\n",
    "    # top_phi_bot_cluster10 = top_phit_bot_clustering(ntd_val_final10_clean)\n",
    "\n",
    "    def data_clustering(dataset, feature_list, scaler, cluster_num):\n",
    "        \"\"\"\n",
    "        MinMaxScaler(), StandardScaler()\n",
    "        \"\"\"\n",
    "        data = dataset[feature_list]\n",
    "        normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "        kmeans_labels = kmeans.fit_predict(normalized_data)\n",
    "        kmeans_labels = pd.DataFrame(kmeans_labels, columns=['kmeans'])\n",
    "\n",
    "        gmm = GaussianMixture(n_components=cluster_num, random_state=42)\n",
    "        gmm.fit(normalized_data)\n",
    "        gmm_labels = gmm.predict(normalized_data)\n",
    "        gmm_labels = pd.DataFrame(gmm_labels, columns=['gmm'])\n",
    "\n",
    "        agglomerative = AgglomerativeClustering(n_clusters=cluster_num)\n",
    "        agglomerative_labels = agglomerative.fit_predict(normalized_data)\n",
    "        agglomerative_labels = pd.DataFrame(agglomerative_labels, columns=['agglomer'])\n",
    "        result = pd.concat([dataset, kmeans_labels, gmm_labels, agglomerative_labels], axis=1)\n",
    "        return result\n",
    "    data_clustered8 = data_clustering(top_phi_bot_cluster8, ['phit_avg', 'htst'], StandardScaler(), 3)\n",
    "    # data_clustered10 = data_clustering(top_phi_bot_cluster10, ['phit_avg', 'htst'], StandardScaler(), 3)\n",
    "    data_clustered8.head(3)\n",
    "\n",
    "    def histo_clustering(dataset, clustering, comment):\n",
    "        data = dataset[dataset.phit_avg !=0]\n",
    "        fig, ax = plt.subplots(1, 4, figsize=(18, 4))\n",
    "        custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "        sns.histplot(data=data, x='phit_avg', hue=clustering, ax=ax[0], kde=True,  palette=custom_palette)\n",
    "        ax[0].grid(True, axis='x'), ax[0].set_xticks(np.arange(0.12, 0.32, 0.02)), ax[0].tick_params(axis='both', which='major', labelsize=8)\n",
    "        sns.histplot(data=data[data.htst < 30], x='htst', hue=clustering, ax=ax[1], kde=True,  palette=custom_palette)\n",
    "        ax[1].grid(True, axis='x'), ax[1].set_xticks(np.arange(0, 30, 3)), ax[1].tick_params(axis='both', which='major', labelsize=8)\n",
    "        sns.histplot(data=data, x='ntg', hue=clustering, ax=ax[2], kde=True,  palette=custom_palette)\n",
    "        ax[2].grid(True, axis='x'), ax[2].set_xticks(np.arange(0, 1, 0.1)), ax[2].tick_params(axis='both', which='major', labelsize=8)\n",
    "        sns.histplot(data=data, x='vsh_avg', hue=clustering, ax=ax[3], kde=True,  palette=custom_palette)\n",
    "        ax[3].grid(True, axis='x'), ax[3].set_xticks(np.arange(0, 0.6, 0.1)), ax[3].tick_params(axis='both', which='major', labelsize=8)\n",
    "        fig.suptitle(comment)\n",
    "    histo_clustering(data_clustered8, 'kmeans', 'Kmeans Bal VIII')\n",
    "\n",
    "    def well_collecting_clusters_top_phi_bot_v2(dataset, clustering, fm):\n",
    "        df_lst = []\n",
    "        for wellname in dataset.well.unique()[:]:\n",
    "            data = dataset[dataset.well == wellname]\n",
    "            well_lst = []\n",
    "            phit_lst = []\n",
    "            htst_lst = []\n",
    "            bot_lst = []\n",
    "            ntg_lst = []\n",
    "            vsh_lst = []\n",
    "            cluster_lst = []\n",
    "            for ind, row in data.iterrows():\n",
    "                well_lst.append(wellname)\n",
    "                well_lst.append(wellname)\n",
    "\n",
    "                phit_lst.append(0)\n",
    "                phit_lst.append(row['phit_avg'])\n",
    "\n",
    "                cluster_lst.append(np.nan)\n",
    "                cluster_lst.append(row[clustering])\n",
    "\n",
    "                htst_lst.append(row['top_htst'])\n",
    "                htst_lst.append(row['htst'])\n",
    "                \n",
    "                bot_lst.append(row['bot_htst'])\n",
    "\n",
    "                ntg_lst.append(0)\n",
    "                ntg_lst.append(row['ntg'])\n",
    "\n",
    "                vsh_lst.append(0)\n",
    "                vsh_lst.append(row['vsh_avg'])\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            cluster_lst.append(np.nan)\n",
    "            htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "            well_lst.append(wellname)\n",
    "            well_collect_cluster_short = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, ntg_lst, vsh_lst, cluster_lst ), columns=[  'well','phit', 'htst', \n",
    "                                                                                                                                    'ntg', 'vsh', 'cluster'])\n",
    "            well_last_row = pd.DataFrame({'well':[well_lst[-1]], 'phit':[0], 'htst': [bot_lst[-1]], 'ntg':[0], 'vsh':[0], 'cluster':[cluster_lst[-1]]})\n",
    "            well_collect_cluster = pd.concat([well_collect_cluster_short, well_last_row]).reset_index(drop=True)\n",
    "            well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "            df_lst.append(well_collect_cluster)\n",
    "        result = pd.concat(df_lst)\n",
    "        result['FORMATION_up'] = fm\n",
    "        return result\n",
    "    tpb8_kmeans = well_collecting_clusters_top_phi_bot_v2(data_clustered8, 'kmeans', 'Balakhany VIII')\n",
    "    # tpb10_kmeans = well_collecting_clusters_top_phi_bot_v2(data_clustered10, 'agglomer', 'Balakhany VIII')\n",
    "    return tpb8_kmeans\n",
    "tpb8_kmeans = workflow_tracking_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    \"\"\"\n",
    "    ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']\n",
    "    \"\"\"\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.suptitle(clustering)\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "coloring_clusters_matrix_tpb3(tpb8_kmeans, ['C'], 4, 9, 'kmeans bal8', 'dontprint')\n",
    "coloring_clusters_matrix_tpb3(tpb8_kmeans, ['B'], 4, 9, 'kmeans bal8', 'dontprint')\n",
    "coloring_clusters_matrix_tpb3(tpb8_kmeans, ['D'], 4, 9, 'kmeans bal8', 'dontprint')\n",
    "# coloring_clusters_matrix_tpb3(tpb10_kmeans, ['A'], 4, 9, 'kmeans bal10', 'dontprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal8_xy = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up == 'Balakhany VIII'][['well','X_mean', 'Y_mean', 'field']]\n",
    "bal8_xy_v2 = bal8_xy.groupby('well')[['X_mean', 'Y_mean','field']].agg({'X_mean':'mean', 'Y_mean':'mean','field':(lambda x: x.iloc[0])}).reset_index()\n",
    "bal8_xy_v2 = bal8_xy_v2[bal8_xy_v2.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI'])]\n",
    "tpb8_kmeans_xy_all = tpb8_kmeans.set_index('well').join(bal8_xy_v2.set_index('well'), how='inner').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagram_files_path():\n",
    "    folder_path = \"C:\\\\jupyter\\\\SPP\\plots\\\\tooltip\\\\\"\n",
    "    file_names = os.listdir(folder_path)\n",
    "    df_lst = []\n",
    "    for file_name in file_names:\n",
    "        path = folder_path + file_name\n",
    "        well = file_name.split('.')[0]\n",
    "        df = pd.DataFrame({'well':[well], 'path':[path]})\n",
    "        df_lst.append(df)\n",
    "    path_diagram = pd.concat(df_lst).reset_index(drop=True)\n",
    "    return path_diagram\n",
    "path_diagram = diagram_files_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb8_kmeans_cluster0 = tpb8_kmeans_xy_all[(tpb8_kmeans_xy_all.cluster == 0)]\n",
    "tpb8_kmeans_xy = (tpb8_kmeans_xy_all[~tpb8_kmeans_xy_all.well.isin(tpb8_kmeans_cluster0.well.unique())].groupby(\n",
    "                 'well')[['X_mean','Y_mean','field']].apply(lambda x: x.iloc[0])).join(path_diagram.set_index('well')).reset_index()\n",
    "tpb8_kmeans_cluster0 = tpb8_kmeans_cluster0.groupby('well').agg(\n",
    "                        {'htst':'sum', 'phit':'mean','X_mean':'mean', 'Y_mean':'mean'}).round({'htst':0, 'phit':2}).reset_index()\n",
    "\n",
    "df_bal_net2_kh8 = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up == 'Balakhany VIII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_tracking(print_flag):\n",
    "    wells = go.Scatter( x=tpb8_kmeans_xy.X_mean, y=tpb8_kmeans_xy.Y_mean, \n",
    "                        mode='markers',\n",
    "                        marker=dict(color='gray', \n",
    "                                    # size=10, colorscale='viridis',  showscale=True,\n",
    "                                    line=dict(color='black', width=0.5)),\n",
    "                        customdata = tpb8_kmeans_xy[['well']],\n",
    "                        hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"]))\n",
    "    wells_cluster = go.Scatter( x=tpb8_kmeans_cluster0.X_mean, y=tpb8_kmeans_cluster0.Y_mean, \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=tpb8_kmeans_cluster0.phit, size=tpb8_kmeans_cluster0.htst, colorscale='viridis',  showscale=True,\n",
    "                                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = tpb8_kmeans_cluster0[['well', 'htst', 'phit']],\n",
    "                                hovertemplate=\"\".join([\"well:%{customdata[0]},h:%{customdata[1]}, ph:%{customdata[2]}<extra></extra>\"]))\n",
    "    wells_cluster_center = go.Scatter(  x=tpb8_kmeans_cluster0.X_mean, y=tpb8_kmeans_cluster0.Y_mean, \n",
    "                                        mode='markers',\n",
    "                                        marker=dict(color='black', size=3),\n",
    "                                        customdata = tpb8_kmeans_cluster0[['well']],\n",
    "                                        hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"]))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    polygon = bal8_1510_3_polygon\n",
    "    def polygon_drawing(polygon):\n",
    "        for i in range(len(polygon.geometry)):\n",
    "            lon, lat = polygon.geometry.iloc[i].exterior.coords.xy\n",
    "            polygon_plotly = go.Scatter( x=list(lon), y=list(lat), mode='lines', fill='toself', name='bal8_1510_3',\n",
    "                                        fillcolor='rgba(245, 187, 39, 0.5)', line=dict(color='rgba(245, 187, 39, 1)', width=2))\n",
    "            fig.add_trace(polygon_plotly)\n",
    "    polygon_drawing(polygon)  \n",
    "\n",
    "    fig.add_trace(wells)\n",
    "    fig.add_trace(wells_cluster)\n",
    "    fig.add_trace(wells_cluster_center)\n",
    "    fig.update_layout(  title_text= ('Tracking thick layers cluster=0 into Balakhany VIII'),\n",
    "                        autosize=True, \n",
    "                        width=1200, height=800, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    if print_flag == 'print':\n",
    "        fig.write_html('tracking_cluster0.html', config=dict(responsive=True))\n",
    "    else:\n",
    "        pass\n",
    "    # return fig.show()\n",
    "\n",
    "    app = Dash(__name__)\n",
    "    app.layout = html.Div([\n",
    "        dcc.Graph(id=\"graph-basic\", figure=fig, clear_on_unhover=True),\n",
    "        dcc.Tooltip(id=\"graph-tooltip\"),\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "        Output(\"graph-tooltip\", \"show\"),\n",
    "        Output(\"graph-tooltip\", \"bbox\"),\n",
    "        Output(\"graph-tooltip\", \"children\"),\n",
    "        Input(\"graph-basic\", \"hoverData\"),\n",
    "    )\n",
    "\n",
    "    def display_hover(hoverData):    \n",
    "        if hoverData is None:\n",
    "            return False, no_update, no_update\n",
    "        pt = hoverData[\"points\"][0]\n",
    "        bbox = pt[\"bbox\"]\n",
    "        num = pt[\"pointNumber\"]\n",
    "        df_row = tpb8_kmeans_xy.iloc[num]\n",
    "\n",
    "        # Load image with pillow\n",
    "        image_path = df_row['path']\n",
    "        im = Image.open(image_path)\n",
    "        # dump it to base64\n",
    "        buffer = io.BytesIO()\n",
    "        im.save(buffer, format=\"png\")\n",
    "        encoded_image = base64.b64encode(buffer.getvalue()).decode()\n",
    "        im_url = \"data:image/jpeg;base64, \" + encoded_image\n",
    "        print(im_url)\n",
    "        children = [\n",
    "            html.Div([\n",
    "                html.Img(src=im_url, style={\"width\": \"100%\"}),\n",
    "                # html.H2(f\"{name}\", style={\"color\": \"darkblue\", \"overflow-wrap\": \"break-word\"}),\n",
    "            ], style={'width': '300px', 'white-space': 'normal'})\n",
    "        ]\n",
    "        return True, bbox, children\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        app.run(debug=True, port=8051)\n",
    "\n",
    "display_cluster_tracking('dontprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import base64\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image with pillow\n",
    "image_path = 'dash_docs/assets/images/sample.jpg'\n",
    "im = Image.open(image_path)\n",
    "\n",
    "# dump it to base64\n",
    "buffer = io.BytesIO()\n",
    "im.save(buffer, format=\"jpeg\")\n",
    "encoded_image = base64.b64encode(buffer.getvalue()).decode()\n",
    "im_url = \"data:image/jpeg;base64, \" + encoded_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb8_kmeans_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_well_tooltip(dataset, wellname, fm, print_flag):\n",
    "#     col=0\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=(4, 5))\n",
    "#     data = dataset[(dataset.well==wellname)]\n",
    "#     data['PHIT_clipped'] = data['PHIT']\n",
    "#     data.loc[data.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "#     y = data.TST\n",
    "#     gr = data.GR_N\n",
    "#     phit_avg = data.PHIT\n",
    "#     phit_cliped = data.PHIT_clipped\n",
    "#     net = data.NET_clp2\n",
    "#     perm = data.LPERM\n",
    "#     ax[col].plot(gr, y, color='lightgreen', label='gr_n', lw=1.5)\n",
    "#     ax[col].plot([75 for i in range(len(y))], y, color='green', lw=1, ls='--', alpha=0.66)\n",
    "#     ax[col].set_xlim(20, 100)\n",
    "#     ax[col].xaxis.set_ticks(np.arange(25, 100, 25))\n",
    "#     ax[col].invert_yaxis()\n",
    "#     ax[col+1].plot(phit_cliped, y, color='red', label='phit_cliped')\n",
    "#     ax[col+1].plot(phit_avg, y, color='green',ls='--', label='phit')\n",
    "#     ax[col+1].plot(net, y, color='orange', alpha=0.33)\n",
    "#     ax[col+1].plot([0.13 for i in range(len(y))], y, color='black', lw=1, ls='--', alpha=1)\n",
    "#     ax[col+1].fill_betweenx(y,net, color='orange', alpha=0.33)\n",
    "#     ax[col+1].set_xlim(0, 0.3)\n",
    "#     ax[col+1].xaxis.set_ticks(np.arange(0, 0.3, 0.1))\n",
    "#     ax[col+1].invert_yaxis()\n",
    "#     ax[col+1].set_yticks([])\n",
    "#     ax[col+2].plot(perm, y, color='purple', lw=1, label='perm')\n",
    "#     ax[col+2].plot([10 for i in range(len(y))], y, color='purple', lw=1, ls='--', alpha=0.66)\n",
    "#     ax[col+2].plot([100 for i in range(len(y))], y, color='purple', lw=1, ls='--', alpha=0.66)\n",
    "#     ax[col+2].plot([500 for i in range(len(y))], y, color='purple', lw=1, ls='--', alpha=0.66)\n",
    "#     ax[col+2].invert_yaxis()\n",
    "#     ax[col+2].set_xscale('log')\n",
    "#     ax[col+2].set_xlim(1, 1500)\n",
    "#     ax[col+2].set_xticks([])\n",
    "#     ax[col+2].set_yticks([])\n",
    "\n",
    "#     ax[col].set_title(wellname)\n",
    "#     ax[col+1].set_title(fm)\n",
    "#     ax[col+2].set_title('10 - 100 - 500md', fontsize=8)\n",
    "#     fig.tight_layout()\n",
    "#     fig.show()\n",
    "#     if print_flag == 'print':\n",
    "#         folder_path = \"C:\\\\jupyter\\\\SPP\\plots\\\\tooltip\\\\\"\n",
    "#         plt.savefig(folder_path + wellname + \".png\")\n",
    "# for wellname in bal8_xy_v2.well.unique():\n",
    "#     display_well_tooltip(df_bal_net2_kh8, wellname, 'Balakhany VIII', 'print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_geobody_in_out(dataset, wellname_in, wellname_out):\n",
    "    fig, ax = plt.subplots(1, 6, figsize=(10, 5))\n",
    "    def display_geobody(dataset, wellname, fm, col):\n",
    "        data = dataset[(dataset.well==wellname)]\n",
    "        data['PHIT_clipped'] = data['PHIT']\n",
    "        data.loc[data.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "        y = data.TST\n",
    "        gr = data.GR_N\n",
    "        phit_avg = data.PHIT\n",
    "        phit_cliped = data.PHIT_clipped\n",
    "        net = data.NET_clp2\n",
    "        perm = data.LPERM\n",
    "        ax[col].plot(gr, y, color='lightgreen', label='gr_n', lw=1.5)\n",
    "        ax[col].set_xlim(20, 100)\n",
    "        # ax[r,c].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "        ax[col].xaxis.set_ticks(np.arange(25, 100, 25))\n",
    "        ax[col].invert_yaxis()\n",
    "        ax[col+1].plot(phit_cliped, y, color='red', label='phit_cliped')\n",
    "        ax[col+1].plot(phit_avg, y, color='green',ls='--', label='phit')\n",
    "        ax[col+1].plot(net, y, color='orange', alpha=0.33)\n",
    "        ax[col+1].plot([0.13 for i in range(len(y))], y, color='black', lw=1, ls='--', alpha=1)\n",
    "        ax[col+1].fill_betweenx(y,net, color='orange', alpha=0.33)\n",
    "        ax[col+1].set_xlim(0, 0.3)\n",
    "        ax[col+1].xaxis.set_ticks(np.arange(0, 0.3, 0.1))\n",
    "        ax[col+1].invert_yaxis()\n",
    "        ax[col+1].set_yticks([])\n",
    "        ax[col+2].plot(perm, y, color='purple', lw=1, label='perm')\n",
    "        ax[col+2].invert_yaxis()\n",
    "        ax[col+2].set_xscale('log')\n",
    "        ax[col+2].set_xlim(1, 1500)\n",
    "        ax[col+2].xaxis.set_ticks(np.arange(1, 1500, 100))\n",
    "        ax[col+2].set_xticks([])\n",
    "        ax[col+2].set_yticks([])\n",
    "        ax[col].set_title(wellname)\n",
    "        ax[col+1].set_title(fm)\n",
    "        fig.show()\n",
    "    display_geobody(dataset, wellname_in, 'Balakhany VIII', 0)\n",
    "    display_geobody(dataset, wellname_out, 'Balakhany VIII', 3)\n",
    "    plt.suptitle('Balakhany VIII 15, Balakhany VIII 10')\n",
    "display_geobody_in_out(df_bal_net2_kh8, 'C42','C17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal_net2_kh_v2 = df_bal_net2_kh[~df_bal_net2_kh.well.isin(['GCA2', 'GCA6', 'GCA6Y', 'GCA6Z', 'GCA7']) \n",
    "                                   & (df_bal_net2_kh.field.isin(['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']))]\n",
    "def interpolate_by_depth_fm_run(dataset_logs, formation_name, step):\n",
    "    def interpolate_by_depth_fm(one_well, formation_name, step):\n",
    "        one_well = one_well.sort_values(by='TST')\n",
    "        well_name = one_well[\"well\"].iloc[0]\n",
    "        data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "        starting_tst = one_well[\"TST\"].iloc[0]\n",
    "        new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "        interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET = interp1d(one_well['TST'], one_well['NET'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_VSH = interp1d(one_well['TST'], one_well['NET_VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp = interp1d(one_well['TST'], one_well['NET_clp'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_VSH = interp1d(one_well['TST'], one_well['VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_GR = interp1d(one_well['TST'], one_well['GR_N'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_MD = interp1d(one_well['TST'], one_well['MD'], kind='linear', fill_value=\"extrapolate\")\n",
    "        # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "        new_data = {\n",
    "            'well': [well_name for _ in range(len(new_TST_values))],\n",
    "            'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "            'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "            'TST': new_TST_values,\n",
    "            'X_traj': interp_X(new_TST_values),\n",
    "            'Y_traj': interp_Y(new_TST_values),\n",
    "            'PHIT': interp_PHIT(new_TST_values),\n",
    "            'TVD_SCS': interp_TVD(new_TST_values),\n",
    "            'NET': interp_NET(new_TST_values),\n",
    "            'NET_VSH': interp_NET_VSH(new_TST_values),\n",
    "            'NET_clp': interp_NET_clp(new_TST_values),\n",
    "            'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "            'LPERM': interp_LPERM(new_TST_values),\n",
    "            'KHtst': interp_KHtst(new_TST_values),\n",
    "            'VSH': interp_VSH(new_TST_values),\n",
    "            'GR_N': interp_GR(new_TST_values),\n",
    "            'MD': interp_MD(new_TST_values),\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        return new_df\n",
    "    df_lst = []\n",
    "    print(f'Start interpolation of {formation_name}')\n",
    "    for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "        well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "        well_interp = interpolate_by_depth_fm(well_sel, formation_name, step)\n",
    "        df_lst.append(well_interp)\n",
    "    result = pd.concat(df_lst)\n",
    "    result = result.round({'MD':1, 'TVD_SCS':1, 'TST':1})\n",
    "    return result\n",
    "well_bal8 = df_bal_net2_kh_v2[(df_bal_net2_kh_v2.FORMATION_up == 'Balakhany VIII')]\n",
    "well_bal10 = df_bal_net2_kh_v2[(df_bal_net2_kh_v2.FORMATION_up == 'Balakhany X')]\n",
    "well_bal8_interp = interpolate_by_depth_fm_run(well_bal8, 'Balakhany VIII', 0.1)\n",
    "well_bal10_interp = interpolate_by_depth_fm_run(well_bal10, 'Balakhany X', 0.1)\n",
    "\n",
    "data_fu = df_bal_net2_kh[['well','MD','FORMATION', 'field']]\n",
    "well_bal8_interp_v2 = well_bal8_interp.set_index(['well','MD']).join(data_fu.set_index(['well','MD'])).reset_index()\n",
    "well_bal8_interp_v2.insert(3, 'FORMATION', well_bal8_interp_v2.pop('FORMATION'))\n",
    "well_bal8_interp_v2.insert(14, 'tst_index', well_bal8_interp_v2.pop('tst_index'))\n",
    "well_bal8_interp_v2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_bal8_interp_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_charts_net(dataset, fm, field, print='dont print'):\n",
    "    df_bal_net2_kh_azeri8 = dataset[dataset.field.isin([field]) \n",
    "                                    & (dataset.KHtst.notna()) \n",
    "                                    & (dataset.FORMATION_up == fm)\n",
    "                                    & (~dataset.well.str.contains('GCA'))]\n",
    "    net_comparison = df_bal_net2_kh_azeri8.groupby('well')[['NET', 'NET_VSH', 'NET_clp', 'NET_clp2']].apply(lambda x: x.sum()*0.1).reset_index()\n",
    "    net_comparison = net_comparison.round({'NET':0, 'NET_VSH':0, 'NET_clp':0, 'NET_clp2':0})\n",
    "    trace1 = go.Bar(x=net_comparison['well'], y=net_comparison['NET'], name='NET')\n",
    "    trace2 = go.Bar(x=net_comparison['well'], y=net_comparison['NET_VSH'], name='NET_VSH')\n",
    "    trace3 = go.Bar(x=net_comparison['well'], y=net_comparison['NET_clp'], name='NET_clp')\n",
    "    trace4 = go.Bar(x=net_comparison['well'], y=net_comparison['NET_clp2'], name='NET_clp2')\n",
    "\n",
    "    fig = go.Figure(data=[trace1, trace2, trace3, trace4])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Bar Chart for ' + field,\n",
    "        xaxis=dict(title='Well'),\n",
    "        yaxis=dict(title='TST sum'),\n",
    "        width=1300, height=500,\n",
    "        barmode='group',\n",
    "        margin=dict(l=10,r=10,b=10,t=50))\n",
    "    # fig.to_image(format=\"jpg\", width=800, height=600, scale=2)\n",
    "\n",
    "    if print=='print':\n",
    "        fig.write_html(field + '.html', config=dict(responsive=True))\n",
    "    else:\n",
    "        pass\n",
    "    fig.show()\n",
    "bar_charts_net(well_bal8_interp_v2, 'Balakhany VIII', 'WEST AZERI', 'dont print')\n",
    "bar_charts_net(well_bal8_interp_v2, 'Balakhany VIII', 'CENTRAL AZERI', 'dont print')\n",
    "bar_charts_net(well_bal8_interp_v2, 'Balakhany VIII', 'EAST AZERI', 'dont print')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation NTD properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_ntd_properties(dataset):\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[df_net2_bal8.FORMATION_up=='Balakhany VIII']\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[df_net2_bal10.FORMATION_up=='Balakhany X']\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            well_fm_fu = []\n",
    "            well_khtst = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                khtst_lst = []\n",
    "                fu_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                        khtst_lst.append(well_avgprop_sel['KHtst'].iloc[depth])\n",
    "                        fu_lst.append(well_avgprop_sel['FORMATION'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_fm_fu.append(pd.Series(fu_lst).value_counts().reset_index().iloc[0]['index'])\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_khtst.append(khtst_lst[0] - khtst_lst[-1])\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm, well_fm_fu,well_phit, well_vsh, well_gperm, well_khtst, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up', 'FORMATION',       \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'khtst',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'FORMATION', \n",
    "                                   'phit_avg', 'vsh_avg', 'perm_avg', 'khtst','htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "ntd_val_final = calculation_ntd_properties(df_bal_net2_kh_v2)\n",
    "ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntd_val_final8.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy run of ML-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation_well_features_to_point(dataset_interp_fm):\n",
    "    def tst_diff(series):\n",
    "        return series.iloc[-1] - series.iloc[0]\n",
    "    def tvd_tops(series):\n",
    "        return series.iloc[0]\n",
    "\n",
    "    df_agg_per_well = dataset_interp_fm.groupby('well').agg({'X_traj': 'mean', 'Y_traj': 'mean', 'NET_clp2': 'sum', 'VSH':'mean','TST': tst_diff, 'TVD_SCS': tvd_tops})\n",
    "    df_agg_per_well.columns = ['x_mean', 'y_mean', 'net_sum', 'vsh_mean', 'tst_interv', 'tvd_top']\n",
    "    df_agg_per_well['net_sum'] = df_agg_per_well['net_sum']*0.1\n",
    "    df_agg_per_well = df_agg_per_well.reset_index()\n",
    "    df_agg_per_well = df_agg_per_well.round({'net_sum':0, 'vsh_mean':2})\n",
    "    return df_agg_per_well\n",
    "df_agg_per_well_8 = aggregation_well_features_to_point(well_bal8_interp)\n",
    "df_agg_per_well_8.insert(6, 'net_sum', df_agg_per_well_8.pop('net_sum'))\n",
    "df_agg_per_well_8.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "pairplot_special(df_agg_per_well_8, 6, 6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'net_sum'\n",
    "dataset = df_agg_per_well_8\n",
    "\n",
    "X = dataset.drop([target], axis=1)\n",
    "y = dataset[['well',target]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train_wnames = y_train[['well']].reset_index(drop=True)\n",
    "y_test_wnames = y_test[['well']].reset_index(drop=True)\n",
    "\n",
    "X_train = X_train.drop(['well'], axis=1)\n",
    "X_test = X_test.drop(['well'], axis=1)\n",
    "y_train = y_train.drop(['well'], axis=1)\n",
    "y_test = y_test.drop(['well'], axis=1)\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to be tuned\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "\n",
    "    # Instantiate and train the model with the suggested hyperparameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    res = mae(y_test, y_pred)\n",
    "\n",
    "    return res\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters and the best objective value\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best objective value (mae):', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerance = 0.1\n",
    "best_params = study.best_params\n",
    "\n",
    "best_model = RandomForestRegressor(\n",
    "                                    n_estimators=best_params['n_estimators'],\n",
    "                                    max_depth=best_params['max_depth'],\n",
    "                                    min_samples_split=best_params['min_samples_split'],\n",
    "                                    min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                    random_state=42\n",
    "                                )\n",
    "model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", best_model)])\n",
    "# model = best_model\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_train = np.array(y_train).flatten()\n",
    "y_test = np.array(y_test).flatten()\n",
    "train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "train = pd.concat([y_train_wnames, train], axis=1)\n",
    "test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "train['up'] = train['y_orig']*(1 + tolerance)\n",
    "train['down'] = train['y_orig']*(1 - tolerance)\n",
    "train['qc'] = 'out'\n",
    "train['dataset'] = 'train'\n",
    "train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "test['up'] = test['y_orig']*(1 + tolerance)\n",
    "test['down'] = test['y_orig']*(1 - tolerance)\n",
    "test['qc'] = 'out'\n",
    "test['dataset'] = 'test'\n",
    "test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "testqc = test.qc.value_counts(normalize=True)\n",
    "df = pd.concat([train, test])\n",
    "df['y_pred'] = df['y_pred'].astype('float')\n",
    "result = {'result':df, 'testqc':testqc['in'].round(2), 'trainqc':trainqc['in'].round(2), 'train_df':X_train.columns, 'model': model}\n",
    "\n",
    "def xplot_qc3(model, data, qc_train, qc_test, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "\n",
    "    feature_importances = model.named_steps['model'].feature_importances_\n",
    "    feature_names = model.named_steps['scaler'].get_feature_names_out()\n",
    "    feature_importances_df = pd.DataFrame(zip(feature_names, feature_importances), \n",
    "                                          columns = ['feature','importance']).sort_values(by='importance', ascending=True)\n",
    "    feature_importances_df = feature_importances_df.round({'importance':2})\n",
    "    data = data.round({y_orig: round, y_pred: round})\n",
    "    ds_train = data[data.dataset == 'train']\n",
    "    ds_test = data[data.dataset == 'test']\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors_train = [colors[qc] for qc in ds_train.qc]\n",
    "    qc_colors_test = [colors[qc] for qc in ds_test.qc]\n",
    "    scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_train, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_train[['well',y_orig, y_pred]],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}<extra></extra>\"])\n",
    "                                )\n",
    "    scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_test, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_test[['well', y_orig, y_pred]],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}<extra></extra>\"])\n",
    "                                )\n",
    "    \n",
    "    line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    \n",
    "    trace = go.Bar( x=feature_importances_df.importance,  \n",
    "                    y=feature_importances_df.feature,\n",
    "                    orientation='h', marker=dict(color='blue'))\n",
    "    \n",
    "    ds_test['diff'] = ds_test['y_orig'] - ds_test['y_pred']\n",
    "    ds_test = ds_test.round({'y_pred':2})\n",
    "    residuals = go.Scatter(   x=ds_test['y_pred'], y=ds_test['diff'],\n",
    "                            mode='markers',\n",
    "                            marker=dict(color='red', size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = ds_test[['well', 'y_orig', 'y_pred']],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}<extra></extra>\"]))\n",
    "    residuals_line = go.Scatter(x=[30, 70], y=[0,0], mode='lines+markers', line=dict(color='black')) \n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=1, cols=4, subplot_titles=(f'train ds {qc_train}', f'test ds {qc_test}', 'feature imp', 'residuals'))\n",
    "    fig.add_trace(scatter_train,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "    fig.add_trace(scatter_test,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "    fig.add_trace(trace,  row=1, col=3)\n",
    "    fig.add_trace(residuals, row=1, col=4)\n",
    "    fig.add_trace(residuals_line, row=1, col=4)\n",
    "    fig.update_layout(  title_text= (comment), width=1300, height=350, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "xplot_qc3(model, result['result'], result['trainqc'], result['testqc'], 'y_orig', 'y_pred', 80, tolerance, 0, 3, f'RandomForestRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = result['result'][result['result'].dataset == 'test']\n",
    "train = result['result'][result['result'].dataset == 'train']\n",
    "test_xy = test.set_index('well').join(df_agg_per_well_8[['well','x_mean','y_mean']].set_index('well')).reset_index()\n",
    "train_xy = train.set_index('well').join(df_agg_per_well_8[['well','x_mean','y_mean']].set_index('well')).reset_index()\n",
    "\n",
    "def display_net_prediction(train_xy, test_xy):\n",
    "    train_well = go.Scatter(    x=train_xy.x_mean, y=train_xy.y_mean, \n",
    "                                mode='markers',\n",
    "                                marker=dict(color='blue', size=10),\n",
    "                                customdata = train_xy[['well']],\n",
    "                                hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"]))\n",
    "    test_xy_in = test_xy[test_xy.qc == 'in']\n",
    "    test_xy_out = test_xy[test_xy.qc == 'out']\n",
    "    test_well_in = go.Scatter(      x=test_xy_in.x_mean, y=test_xy_in.y_mean, \n",
    "                                    mode='markers',\n",
    "                                    marker=dict(color='green', size=10),\n",
    "                                    customdata = test_xy[['well']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"]))\n",
    "    test_well_out = go.Scatter(      x=test_xy_out.x_mean, y=test_xy_out.y_mean, \n",
    "                                    mode='markers',\n",
    "                                    marker=dict(color='red', size=10),\n",
    "                                    customdata = test_xy[['well']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"]))\n",
    "    fig = go.Figure() \n",
    "    fig.add_trace(train_well)\n",
    "    fig.add_trace(test_well_in)\n",
    "    fig.add_trace(test_well_out)\n",
    "    fig.update_layout(  title_text= ('Map of well points'),\n",
    "                        autosize=True, width=1000, height=600, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    fig.show()\n",
    "display_net_prediction(train_xy, test_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NET maps RDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_bal810_upload():\n",
    "    net_maps_bal8_sand_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_VIII_Sand.csv')\n",
    "    net_maps_bal8_20_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_VIII_20.csv')\n",
    "    net_maps_bal8_10_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_VIII_10.csv')\n",
    "    net_maps_bal8_05_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_VIII_5.csv')\n",
    "    net_maps_bal8_sand = net_maps_bal8_sand_init[['x','y','NET','Formation']]\n",
    "    net_maps_bal8_20 = net_maps_bal8_20_init[['x','y','NET','Formation']]\n",
    "    net_maps_bal8_10 = net_maps_bal8_10_init[['x','y','NET','Formation']]\n",
    "    net_maps_bal8_05 = net_maps_bal8_05_init[['x','y','NET','Formation']]\n",
    "\n",
    "    net_maps_bal10_20_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_X_20.csv')\n",
    "    net_maps_bal10_40_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_X_40.csv')\n",
    "    net_maps_bal10_sand_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\net\\net_maps_Balakhany_X_Sand.csv')\n",
    "    net_maps_bal10_20 = net_maps_bal10_20_init[['x','y','NET','Formation']]\n",
    "    net_maps_bal10_40 = net_maps_bal10_40_init[['x','y','NET','Formation']]\n",
    "    net_maps_bal10_sand = net_maps_bal10_sand_init[['x','y','NET','Formation']]\n",
    "\n",
    "    net_maps_bal8 = pd.concat([net_maps_bal8_sand, net_maps_bal8_20, net_maps_bal8_10, net_maps_bal8_05])\n",
    "    net_maps_bal10 = pd.concat([net_maps_bal10_20, net_maps_bal10_40, net_maps_bal10_sand])\n",
    "    net_maps_bal8.rename(columns={'NET':'NTG'}, inplace=True)\n",
    "    net_maps_bal10.rename(columns={'NET':'NTG'}, inplace=True)\n",
    "    net_maps_bal10['NTG'] =abs(net_maps_bal10.NTG)\n",
    "    return net_maps_bal8, net_maps_bal10\n",
    "def phit_bal810_upload():\n",
    "    phit_maps_bal8_sand_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\\\phit_maps_Balakhany_VIII_Sand.csv')\n",
    "    phit_maps_bal8_20_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_VIII_20.csv')\n",
    "    phit_maps_bal8_10_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_VIII_10.csv')\n",
    "    phit_maps_bal8_05_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_VIII_5.csv')\n",
    "    phit_maps_bal8_sand = phit_maps_bal8_sand_init[['x','y','PHIT','Formation']]\n",
    "    phit_maps_bal8_20 = phit_maps_bal8_20_init[['x','y','PHIT','Formation']]\n",
    "    phit_maps_bal8_10 = phit_maps_bal8_10_init[['x','y','PHIT','Formation']]\n",
    "    phit_maps_bal8_05 = phit_maps_bal8_05_init[['x','y','PHIT','Formation']]\n",
    "\n",
    "    phit_maps_bal10_20_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_X_20.csv')\n",
    "    phit_maps_bal10_40_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_X_40.csv')\n",
    "    phit_maps_bal10_sand_init = pd.read_csv(r'C:\\jupyter\\spp_farid_data\\phit_ntg_maps\\phit\\phit_maps_Balakhany_X_Sand.csv')\n",
    "\n",
    "    phit_maps_bal10_20 = phit_maps_bal10_20_init[['x','y','PHIT','Formation']]\n",
    "    phit_maps_bal10_40 = phit_maps_bal10_40_init[['x','y','PHIT','Formation']]\n",
    "    phit_maps_bal10_sand = phit_maps_bal10_sand_init[['x','y','PHIT','Formation']]\n",
    "\n",
    "    phit_maps_bal8 = pd.concat([phit_maps_bal8_sand, phit_maps_bal8_20, phit_maps_bal8_10, phit_maps_bal8_05])\n",
    "    phit_maps_bal10 = pd.concat([phit_maps_bal10_20, phit_maps_bal10_40, phit_maps_bal10_sand])                               \n",
    "    phit_maps_bal8.rename(columns={'PHIT':'phit_map'}, inplace=True)\n",
    "    phit_maps_bal10.rename(columns={'PHIT':'phit_map'}, inplace=True)\n",
    "    phit_maps_bal10['phit_map'] =abs(phit_maps_bal10.phit_map)\n",
    "    return phit_maps_bal8, phit_maps_bal10\n",
    "def map_prop_accum(logging_df, map_df, well_var_x, well_var_y, map_var, zone_size):\n",
    "    \"\"\"\n",
    "    logging_df - dataframe with well data\n",
    "    map_df - datafram with map data\n",
    "    well_var_x, well_var_y - well coordinates from wells dataframe\n",
    "    map_var - name of map's variable\n",
    "    margin - distance +/- to accumulate map's data\n",
    "    \"\"\"\n",
    "    df_lst = []\n",
    "    for i in range(len(logging_df.well.unique())):\n",
    "        well_name = logging_df.iloc[i]['well']\n",
    "        well_x = logging_df.iloc[i][well_var_x]\n",
    "        well_y =logging_df.iloc[i][well_var_y]\n",
    "        var_map_avg = map_df[   (map_df.x < well_x + zone_size) &\n",
    "                                (map_df.x > well_x - zone_size) &\n",
    "                                (map_df.y < well_y + zone_size) &\n",
    "                                (map_df.y > well_y - zone_size)].reset_index()\n",
    "        var_map_avg=var_map_avg[[map_var,'Formation']]\n",
    "        var_map_avg['well'] = well_name\n",
    "        df_lst.append(var_map_avg)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "def map_phit_ntg_process(phit_map_towell, net_map_towell, phit_log, wavg_ntg_flag=1):\n",
    "    if wavg_ntg_flag == 1:\n",
    "        phit_net_memcell = phit_map_towell.set_index(['well','Formation']).join(net_map_towell.set_index(['well','Formation'])).reset_index()\n",
    "        phit_maps_bal_towell_v2 = phit_net_memcell.groupby(['well','Formation']).mean().reset_index()\n",
    "        phit_maps_bal_towell_v2['pnit_ntg'] = phit_maps_bal_towell_v2.phit_map*phit_maps_bal_towell_v2.NTG\n",
    "        sum_ntg = phit_maps_bal_towell_v2.groupby('well')['NTG'].sum().reset_index().rename(columns = {'NTG':'sum_ntg'})\n",
    "        phit_maps_bal_towell_v3 = phit_maps_bal_towell_v2.set_index('well').join(sum_ntg.set_index('well')).reset_index()\n",
    "        sum_phit = phit_maps_bal_towell_v3.groupby('well')['pnit_ntg'].sum().reset_index().rename(columns = {'pnit_ntg':'sum_phit'})\n",
    "        phit_maps_bal_towell_v4 = phit_maps_bal_towell_v3.set_index('well').join(sum_phit.set_index('well')).reset_index()\n",
    "        phit_maps_bal_towell_v4['phit_wavg_map'] = phit_maps_bal_towell_v4.sum_phit / phit_maps_bal_towell_v4.sum_ntg\n",
    "        phit_maps_bal_towell_v4 = phit_maps_bal_towell_v4[['well','Formation','phit_wavg_map']].groupby('well')[['well', 'phit_wavg_map']].apply(\n",
    "                                                                                                lambda x: x.iloc[0]).drop('well',axis=1).reset_index()\n",
    "        phit_log_v2 = phit_log.set_index('well').join(phit_maps_bal_towell_v4.set_index('well')).reset_index()\n",
    "        result = phit_log_v2.round({'phit_wavg':3, 'phit_wavg_map':3})\n",
    "    if wavg_ntg_flag == 0:\n",
    "        phit_maps_bal_towell_v2 = phit_map_towell.groupby(['well','Formation']).mean().reset_index()\n",
    "        phit_log_v2 = phit_log.set_index('well').join(phit_maps_bal_towell_v2.set_index('well')).reset_index()\n",
    "        phit_log_v3 = phit_log_v2.groupby(['well','FORMATION_up'])['phit_map'].mean().reset_index()\n",
    "        phit_log_v3 = phit_log_v3.rename(columns={'phit_map':'phit_avg_map'})\n",
    "        phit_log_v4 = phit_log_v2.drop( ['Formation','phit_map'], axis=1).groupby(\n",
    "                                        ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1)\n",
    "        result = phit_log_v3.set_index(['well','FORMATION_up']).join(phit_log_v4).reset_index()\n",
    "    return result\n",
    "def phit_map_metrics(phit_map, target_value, rng, margin):\n",
    "    phit_map['l_range'] = phit_map.phit_wavg*(1-rng) - margin \n",
    "    phit_map['h_range'] = phit_map.phit_wavg*(1+rng) + margin\n",
    "    phit_map['qc'] = 'out'\n",
    "    phit_map.loc[(phit_map[target_value] >= phit_map.l_range) & (phit_map[target_value] <= phit_map.h_range), 'qc'] = 'in'\n",
    "    phit_map['diff'] = (phit_map.phit_wavg - phit_map[target_value]).round(3)\n",
    "    metrics_dict = {    'wells_total':          phit_map.shape[0], \n",
    "                        'wells_unpred':         phit_map['qc'].value_counts()['out'],\n",
    "                        'wells_unpred_v/v':     (phit_map['qc'].value_counts()['out']/phit_map.shape[0]).round(2),\n",
    "                        'wells_pred':           phit_map['qc'].value_counts()['in'],\n",
    "                        'wells_pred_v/v':       (phit_map['qc'].value_counts()['in']/phit_map.shape[0]).round(2)\n",
    "                    }\n",
    "    return metrics_dict\n",
    "def xplot_qc_map(data, target_value, rng, max_val, margin):\n",
    "    data = data.round({'phit_wavg':3, target_value:3})\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors = [colors[qc] for qc in data.qc]\n",
    "    scatter = go.Scatter( x=data.phit_wavg, y=data[target_value],\n",
    "                            mode='markers',\n",
    "                            marker=dict(color=qc_colors, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = data[['well','phit_wavg', target_value, 'diff', 'FORMATION_up']],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"well:%{customdata[0]}, phit_wavg: %{customdata[1]}, target_val:%{customdata[2]}, diff:%{customdata[3]}, fm:%{customdata[4]}<extra></extra>\"])\n",
    "                            )\n",
    "    fig = go.Figure()\n",
    "    line_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line_up)\n",
    "    fig.add_trace(line_dw)\n",
    "    fig.update_xaxes(title_text='phit_wavg')\n",
    "    fig.update_yaxes(title_text=target_value)\n",
    "    fig.update_layout(  title_text= ('phit_map'), width=450, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading data from CSV\n",
    "net_maps_bal8, net_maps_bal10 = net_bal810_upload()\n",
    "phit_maps_bal8, phit_maps_bal10 = phit_bal810_upload()\n",
    "# Getting middle x/y-coordinates from wells for each formation\n",
    "xy_middle = df_bal_net2_kh[['well','FORMATION_up', 'X_mean','Y_mean']]\n",
    "xy_middle8 = xy_middle[xy_middle.FORMATION_up == 'Balakhany VIII'].groupby('well').apply(lambda x: x.iloc[0]).drop('well', axis=1).reset_index()\n",
    "xy_middle10 = xy_middle[xy_middle.FORMATION_up == 'Balakhany X'].groupby('well').apply(lambda x: x.iloc[0]).drop('well', axis=1).reset_index()\n",
    "\n",
    "\n",
    "# phit_wavg8 = ntd_val_final8[ntd_val_final8.FORMATION_up == 'Balakhany VIII'][['well','phit_avg']]\n",
    "# phit_wavg10 = ntd_val_final10[ntd_val_final10.FORMATION_up == 'Balakhany X'][['well','phit_avg']]\n",
    "# phit_wavg8_xy = pd.merge(phit_wavg8, xy_middle8, on='well')\n",
    "# phit_wavg10_xy = pd.merge(phit_wavg10, xy_middle10, on='well')\n",
    "# # Calculation net- & phit average values based on margin 50m (+/- 25m)\n",
    "# zone_size = 50\n",
    "# net_maps_bal8_towell = map_prop_accum(phit_wavg8_xy, net_maps_bal8, 'X','Y', 'NTG', zone_size)\n",
    "# phit_maps_bal8_towell = map_prop_accum(phit_wavg8_xy, phit_maps_bal8, 'X','Y', 'phit_map', zone_size)\n",
    "# net_maps_bal10_towell = map_prop_accum(phit_wavg10_xy, net_maps_bal10, 'X','Y', 'NTG', zone_size)\n",
    "# phit_maps_bal10_towell = map_prop_accum(phit_wavg10_xy, phit_maps_bal10, 'X','Y', 'phit_map', zone_size)\n",
    "# # Calculation weithed on NTG phit average values\n",
    "# phit_map_bal8 = map_phit_ntg_process(phit_maps_bal8_towell, net_maps_bal8_towell, phit_wavg8_xy, 1)\n",
    "# phit_map_bal10 = map_phit_ntg_process(phit_maps_bal10_towell, net_maps_bal10_towell, phit_wavg10_xy, 1)\n",
    "# phit_map = pd.concat([phit_map_bal8, phit_map_bal10]).drop(['X','Y'], axis=1)\n",
    "# phit_map_metrcis = phit_map_metrics(phit_map,'phit_wavg_map', 0.05, 0)\n",
    "# phit_map_final = phit_map[['well', 'FORMATION_up', 'phit_wavg_map', 'qc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot_qc_map(phit_map,'phit_wavg_map', 0.05, 0.3, 0)\n",
    "phit_map_metrcis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bal_net2_kh[df_bal_net2_kh.well == 'A01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_techlog = pd.read_csv(r'C:\\jupyter\\SPP\\input\\newTechlogData_allWells.csv')\n",
    "new_techlog = new_techlog[new_techlog.FORMATION_JOIN_BEST.notna()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_techlog.loc[new_techlog.FORMATION_JOIN_BEST.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "new_techlog.loc[new_techlog.FORMATION_JOIN_BEST.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "len(new_techlog.wellName.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_techlog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a01 = new_techlog[new_techlog.wellName == 'A01']\n",
    "a01 = a01[a01.FORMATION_up == 'Balakhany VIII']\n",
    "fig, ax = plt.subplots(1,4, figsize=(8,10))\n",
    "md = a01.MD\n",
    "gr = a01.GR_N\n",
    "rhob = a01.RHOB\n",
    "npss = a01.NPSS\n",
    "phit = a01.PHIT\n",
    "net = a01.NET_VSH\n",
    "vsh = a01.VSH\n",
    "ax[0].plot(gr, md, color='green', lw=2)\n",
    "ax[0].invert_yaxis()\n",
    "ax[1].plot(vsh, md, color='grey', lw=2)\n",
    "ax[1].invert_yaxis()\n",
    "ax[3].plot(net, md, color='orange', lw=1)\n",
    "ax[3].fill_betweenx(md,net, color='orange', alpha=0.33)\n",
    "ax[3].set_xlim(0,0.5)\n",
    "ax[3].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a01.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with Voronoi diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_points = df_agg_per_well[['x_mean', 'y_mean']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vor_wells = Voronoi(well_points)\n",
    "voronoi_plot_2d(vor_wells, show_vertices=False, furthest_site=True, incremental=True);\n",
    "# plt.plot(well_points[:, 0], well_points[:, 1], 'ko', color='gray', marker=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import Delaunay, delaunay_plot_2d\n",
    "tri = Delaunay(well_points)\n",
    "delaunay_plot_2d(tri);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import Voronoi, ConvexHull\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random spatial points\n",
    "np.random.seed(123)\n",
    "points = np.random.rand(20, 2)\n",
    "\n",
    "# Compute Voronoi diagram\n",
    "vor = Voronoi(points)\n",
    "\n",
    "# Compute convex hull of the points\n",
    "hull = ConvexHull(points)\n",
    "\n",
    "# Clip Voronoi diagram to convex hull\n",
    "new_vertices = []\n",
    "for simplex in hull.simplices:\n",
    "    for vert in vor.vertices:\n",
    "        if (vert[0] >= hull.points[simplex[0]][0] and \n",
    "        vert[0] <= hull.points[simplex[1]][0] and \n",
    "        vert[1] >= hull.points[simplex[0]][1] and \n",
    "        vert[1] <= hull.points[simplex[1]][1]):\n",
    "            new_vertices.append(vert)\n",
    "\n",
    "print(new_vertices)\n",
    "# new_vor = Voronoi(new_vertices)\n",
    "\n",
    "# # Plot Voronoi diagram within convex hull\n",
    "# plt.plot(points[:, 0], points[:, 1], 'ko')  # Plot points\n",
    "voronoi_plot_2d(vor, show_vertices=False)\n",
    "plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], 'r--', lw=2)  # Plot convex hull\n",
    "# plt.plot(new_vertices[:, 0], new_vertices[:, 1], 'ro')  # Plot new vertices\n",
    "# voronoi_plot_2d(new_vor, show_vertices=False)  # Plot clipped Voronoi diagram\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Voronoi Diagram Clipped to Convex Hull')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Generate random spatial points\n",
    "np.random.seed(123)\n",
    "points = np.random.rand(20, 2)\n",
    "\n",
    "# Compute Voronoi diagram\n",
    "vor = Voronoi(points)\n",
    "\n",
    "# Convert Voronoi diagram to polygons\n",
    "polygons = []\n",
    "for region_index in vor.regions:\n",
    "    if region_index and -1 not in region_index:\n",
    "        polygon = [vor.vertices[i] for i in region_index]\n",
    "        polygons.append(Polygon(polygon))\n",
    "\n",
    "# Plot Voronoi diagram with polygons\n",
    "# plt.plot(points[:, 0], points[:, 1], 'ko')  # Plot points\n",
    "# for polygon in polygons:\n",
    "#     plt.fill(*zip(*polygon.exterior.coords), alpha=0.5)  # Plot polygons\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# plt.title('Voronoi Diagram with Polygons')\n",
    "# plt.show()\n",
    "\n",
    "voronoi_plot_2d(vor)\n",
    "polygons[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
