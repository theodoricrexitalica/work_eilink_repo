{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libs \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gmean\n",
    "from scipy import stats\n",
    "import math\n",
    "from catboost import CatBoostRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as go_offline\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score as r2, mean_absolute_error as mae, mean_squared_error as mse, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime\n",
    "import random\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, Conv1DTranspose, Conv2DTranspose\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the ACG_wells_JOINT_BEST_v10.csv file\n",
    "# path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "# data_init = pd.read_csv(path + 'ACG_wells_JOINT_BEST_v10.csv', sep=',')\n",
    "# df = data_init.copy()\n",
    "# df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Select only neccessary data\n",
    "# df_cln = df[[   'wellName', 'MD', 'BADPORLOG', 'Casings', 'FORMATION', 'DEVI', 'HAZI',\n",
    "#                 'FLANK', 'FLANK1', 'FLANK2', 'NET', 'NET_VSH','FLUIDS',\n",
    "#                 'LPERM', 'PHIT', \n",
    "#                 'GR_N', 'GRMATRIX', 'GRSHALE','VSH', 'NPSS', 'RHOB', 'RHOF', 'RHOMA', \n",
    "#                 'RDEEP',  'SON', 'SONSH', \n",
    "#                 'TVD_SCS','TST', 'X', 'Y']]\n",
    "# #Fill up nan and -9999 values with 0\n",
    "# df_cln = df_cln.fillna(0)\n",
    "# df_cln = df_cln.replace(-9999, 0)\n",
    "# df_cln = df_cln.replace('-9999', '0')\n",
    "# #Assing proper datatypes for df\n",
    "# dicttypes = {   'wellName':'string', 'MD':'float', 'BADPORLOG':'int', 'Casings':'float', 'FORMATION':'string','DEVI':'float', 'HAZI':'float',\n",
    "#                 'FLANK':'int', 'FLANK1':'int', 'FLANK2':'int', 'NET':'int', 'NET_VSH':'int', 'FLUIDS':'int',\n",
    "#                 'LPERM':'float','PHIT':'float',\n",
    "#                 'GR_N':'float', 'GRMATRIX':'float', 'GRSHALE':'float', 'VSH':'float', 'NPSS':'float','RHOB':'float','RHOF':'float', 'RHOMA':'float',\n",
    "#                 'RDEEP':'float', 'SON':'float', 'SONSH':'float',\n",
    "#                 'TVD_SCS':'float', 'TST':'float', 'X':'float', 'Y':'float'}\n",
    "# df_cln = df_cln.astype(dicttypes, errors='ignore')\n",
    "# df_cln.loc[df_cln.FORMATION=='0', 'FORMATION']='None'\n",
    "# # Save data to parquet\n",
    "# df_cln.to_parquet('ACG_wells_JOINT_BEST_v10.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata, distribution wells per Platforms and all the that.\n",
    "def metadata_parquet_loading():\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    metadata_init = pd.read_csv(path + 'ACG_wells_metadata.csv', sep=',')\n",
    "    metadata = metadata_init.copy()\n",
    "    metadata = metadata.rename(columns={'X':'X_wellhead', 'Y':'Y_wellhead'})\n",
    "    metadata.Status = metadata.Status.str.strip()\n",
    "    metadata.Status = metadata.Status.str.lower()\n",
    "    metadata.loc[metadata.Status == 'oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'oil producer', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'produiction oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production_oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned production oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned  oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned oi', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'injector  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injector water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injetor  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'abandoned injector - water per b', 'Status' ] = 'abandoned injector - water'\n",
    "    metadata.loc[metadata.Status == 'plugged and abandoned', 'Status' ] = 'p&a'\n",
    "    metadata.loc[metadata.X_wellhead==118.270, 'X_wellhead'] = 526258.84\n",
    "    metadata.loc[metadata.Y_wellhead==526261.510, 'Y_wellhead'] = 4435802.01\n",
    "    metadata.loc[metadata.well=='C39', 'X_wellhead'] = 526258.840\n",
    "    metadata.loc[metadata.well=='C39', 'Y_wellhead'] = 4435802.010\n",
    "    metadata.loc[metadata.field=='West Azeri', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.field=='COP', 'field'] = 'WEST CHIRAG'\n",
    "    metadata.loc[metadata.well=='AZERI2', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='AZERI3', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='B31', 'field'] = 'CENTRAL AZERI'\n",
    "    metadata.loc[metadata.well=='J28_bpQIP', 'field'] = 'WEST CHIRAG'\n",
    "\n",
    "    #Read data from parquet\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    df_prq = pd.read_parquet(path + 'ACG_wells_JOINT_BEST_v10.parquet.gzip')\n",
    "    df_prq.rename(columns={'wellName':'well'}, inplace=True)\n",
    "    df_prq = df_prq.set_index('well').join(metadata.set_index('well')).reset_index()\n",
    "    # print('wells in df totally:', len(df_prq.well.unique()))\n",
    "    # Filter data with bad_well_list \n",
    "    bad_well_list = ['E10Z','Predrill_J01Z', 'Predrill_J08', 'J28_bpQIP', 'A01W_2']\n",
    "    df_prq = df_prq[~df_prq.well.isin(bad_well_list)]\n",
    "    #Assign any Fluidcode_mod number by variable gross_pay=1 and gross_pay=0 if Fluidcode_mod as NaN\n",
    "    df_prq.loc[df_prq.FLUIDS>0, 'FLUIDS_int'] = 1\n",
    "    df_prq.loc[df_prq.FLUIDS<=0, 'FLUIDS_int'] = 0\n",
    "    df_prq.FLUIDS_int = df_prq.FLUIDS_int.astype('int')\n",
    "    # Unite of FU for each formation\n",
    "\n",
    "    df_bal = df_prq[df_prq.FORMATION.str.contains('Balakhany')]\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "    df_bal = df_bal[df_bal.FORMATION_up.notna()]\n",
    "    #Getting XY mean coords of Balakhany formation\n",
    "    xy_coord_mean = df_bal[['well', 'FORMATION_up', 'X', 'Y']]\n",
    "    xy_coord_mean = xy_coord_mean.groupby(['well', 'FORMATION_up']).agg({'X': 'mean', 'Y':'mean'}).reset_index()\n",
    "    xy_coord_mean = xy_coord_mean.rename(columns={'X':'X_mean', 'Y':'Y_mean'})\n",
    "    xy_coord_mean = xy_coord_mean[xy_coord_mean.FORMATION_up.str.contains('Balakhany') & (xy_coord_mean.X_mean>0) & (xy_coord_mean.Y_mean>0)]\n",
    "    df_bal.rename(columns={'X':'X_traj', 'Y':'Y_traj'}, inplace=True)\n",
    "    df_bal = df_bal.set_index(['well', 'FORMATION_up']).join(xy_coord_mean.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "    return df_bal\n",
    "df_bal = metadata_parquet_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display in TST well plots with logging curves\n",
    "def well_display_khtst( dataset, wellname, fmname, net_var, comments, \n",
    "                        ref_depth, fm_flag, depth_step, kh_include, print):\n",
    "    \"\"\"\n",
    "    dataset = df_bal or something else\n",
    "    net_var = NET or FLUIDS_int\n",
    "    comments = put what you want\n",
    "    ref_depth = MD or TST\n",
    "    fm_flag = 1 if you need a FORMATION_up, 0 if just a simple FORMATION\n",
    "    depth_step = step for ticks on the diagramm\n",
    "    kh_include = 1 if we have KHtst in dataset, 0 if there is not KHtst\n",
    "    print = 1 if we want to print the plot\n",
    "    \"\"\"\n",
    "    if fm_flag == 0:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION == fmname)]\n",
    "    if fm_flag == 1:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    depth = data[ref_depth]\n",
    "    grn = data['GR_N']\n",
    "    vsh = data['VSH']\n",
    "    rhob = data['RHOB'] \n",
    "    npss = data['NPSS']\n",
    "    rdeep = data['RDEEP']\n",
    "    phit = data['PHIT'] \n",
    "    net = data[net_var]\n",
    "    perm = data['LPERM']\n",
    "    if kh_include == 1:\n",
    "        kh = data['KHtst']\n",
    "    else:\n",
    "        data['KHtst'] = 0\n",
    "        kh = data['KHtst']\n",
    "    fig, ax = plt.subplots(1,4, figsize=(7,7), sharey=True)\n",
    "    well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "    ax[0].plot(grn, depth, color='lightgreen', lw=3, zorder=10)\n",
    "    ax[0].invert_yaxis() \n",
    "    ax[0].set_xlim(0, 150) \n",
    "    ax[0].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "    twin0 = ax[0].twiny()\n",
    "    twin0.plot(vsh, depth, color='black', alpha=0.5, zorder=5)\n",
    "    twin0.set_xlim(0, 1.5)\n",
    "    ax[1].plot(rhob, depth, color='red') \n",
    "    ax[1].invert_yaxis() \n",
    "    ax[1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "    ax[1].set_xlim(1.65, 2.65)\n",
    "    ax[1].grid(axis='y'), ax[1].grid(axis='x')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[1].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "        xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "        ax[1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "    twin1 = ax[1].twiny()\n",
    "    twin1.plot(npss, depth, color='blue')\n",
    "    twin1.set_xlim(0.6, 0)\n",
    "    # ax[2].plot(rdeep, depth, color='black'), ax[2].set_xscale('log'), ax[2].set_xlim(0.1, 50), ax[2].invert_yaxis(), ax[2].grid(axis='x', which='both')\n",
    "    ax[2].plot(phit, depth, color='green', linestyle='dashed'), ax[2].set_xlim(0.3, 0), ax[2].grid(axis='x') \n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].grid(axis='y')\n",
    "    ax[2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "    twin2 = ax[2].twiny()\n",
    "    twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "    twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "    twin2.set_xlim(0, 1)\n",
    "    twin2.set_ylim(min(depth), max(depth))\n",
    "    ax[3].plot(perm, depth, color='purple', alpha=0.66), ax[3].set_xscale('log'), ax[3].set_xlim(0.1, 1000)\n",
    "    ax[3].invert_yaxis()\n",
    "    ax[3].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.66)\n",
    "    twin4 = ax[3].twiny()\n",
    "    twin4.plot(kh, depth, color='black', alpha=1)\n",
    "    fig.suptitle(wellname + ' ' + fmname + ' ' + ref_depth + ' ' + str(round(max(kh.dropna()),0)) + ' ' + str(comments), fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    if print == 1:\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\wellplots\\\\'\n",
    "        fig.savefig(path + fmname.replace(' ','') + '_' + wellname + '.png')\n",
    "    else:\n",
    "        pass\n",
    "# Draw a map\n",
    "def map_value_2plots(metadata, dataset, formation, value, color, multi_chr = 0.001, multi_azr = 0.001):\n",
    "    \"\"\"\n",
    "    metadata, \n",
    "    dataset = dataset with X & Y, \n",
    "    formation = 'Balakhany VIII',  \n",
    "    value = for example 'KHtst' or 'tst_interv'\n",
    "    multi_chr = 0.001, multi_azr = 0.001\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=('crg: ' + str(multi_chr), 'azr: ' + str(multi_azr)), \n",
    "                        vertical_spacing = 0.025)\n",
    "    azr_lst = ['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']\n",
    "    chg_lst = ['CHIRAG', 'DWG', 'DDGG', 'WEST CHIRAG']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    field_avg_coord_chg = field_avg_coord[field_avg_coord.field.isin(chg_lst)]\n",
    "    field_avg_coord_azr = field_avg_coord[field_avg_coord.field.isin(azr_lst)] \n",
    "    df_chg = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(chg_lst))]\n",
    "    df_azr = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(azr_lst))]\n",
    "    fig.add_trace(go.Scatter(x=df_chg.X, y=df_chg.Y, customdata = df_chg[['well', value, color]],\n",
    "                            marker=dict(color=df_chg[color], size=df_chg[value]*multi_chr, colorscale='Viridis_r',  showscale=True,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_chg.X_wellhead, y=field_avg_coord_chg.Y_wellhead, customdata = field_avg_coord_chg[['field']],\n",
    "                            text=field_avg_coord_chg['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_azr.X, y=df_azr.Y, customdata = df_azr[['well', value, color]],\n",
    "                            marker=dict(color=df_azr[color], size=df_azr[value]*multi_azr, colorscale='Viridis_r',  showscale=False,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_azr.X_wellhead, y=field_avg_coord_azr.Y_wellhead, customdata = field_avg_coord_azr[['field']],\n",
    "                            text=field_avg_coord_azr['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.update_layout(  title_text= ('formation: ' + str(formation) + ' value: ' + str(value) + ' color: ' + str(color)),\n",
    "                        autosize=True, width=1300, height=1400, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Calculation NTD\n",
    "def ntd_calculation_big(dataset, desired_fm, net_var='NET'):\n",
    "    df_lst = []\n",
    "    for well_in_loop in tqdm(dataset.well.unique()[:]):\n",
    "        well_lst = []\n",
    "        data = dataset[(dataset.well==well_in_loop)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        for k in range(len(tst_top)):\n",
    "            if (round(tst_top[k],1) == round(tst_bot[k],1)):\n",
    "                h_tst = 0 \n",
    "            elif (round(tst_bot[k],1) == round(tst_top[k]+0.1,1)):\n",
    "                h_tst = 0\n",
    "            else:\n",
    "                h_tst = (round((tst_bot[k] - tst_top[k]),1))\n",
    "                md_perm = []\n",
    "                md_phit = []\n",
    "                md_vsh = []\n",
    "                for i in range(len(data)):\n",
    "                    if round(data.iloc[i]['TST'],1) >= round(tst_top[k],1) and round(data.iloc[i]['TST'],1) <= round(tst_bot[k],1):\n",
    "                        md_perm.append(data.iloc[i]['LPERM'])\n",
    "                        md_phit.append(data.iloc[i]['PHIT'])\n",
    "                        md_vsh.append(data.iloc[i]['VSH'])\n",
    "                if len(md_perm) == 0:\n",
    "                    md_perm.append(0)\n",
    "                if len(md_phit) == 0:\n",
    "                    md_phit.append(0)\n",
    "                if len(md_vsh) == 0:\n",
    "                    md_vsh.append(0)\n",
    "                well_lst.append([data.iloc[0]['well'], h_tst, tst_top[k], tst_bot[k], round(mean(md_perm),0), round(mean(md_phit),2), round(mean(md_vsh),2)])\n",
    "            df_tst = pd.DataFrame(well_lst, columns = ['well', 'h_tst', 'top_tst', 'bot_tst', 'md_perm_avg', 'md_phit_avg', 'md_vsh_avg'])\n",
    "        df_lst.append(df_tst)\n",
    "    ntd_bal = pd.concat(df_lst)\n",
    "    ntd_bal['FORMATION_up'] = desired_fm\n",
    "    return ntd_bal\n",
    "def ntd_calculation_brief(dataset,well,desired_fm, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "    data.iloc[0, 3] = 0\n",
    "    data.iloc[-1, 3] = 0\n",
    "    tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "    tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "    tops = zip(tst_top, tst_bot)\n",
    "    df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "    df_htst['FORMATION_up'] = desired_fm\n",
    "    df_htst['well'] = well\n",
    "    df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "    df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "    return df_htst\n",
    "# Calculation NTD zero\n",
    "def ntd_calculation_zero(dataset,well,formation, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==formation)]\n",
    "    data.iloc[0, 3] = 1\n",
    "    data.iloc[-1, 3] = 1\n",
    "    tst_zero_top = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i-1][net_var] == 1)]\n",
    "    tst_zero_bot = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1) \n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i+1][net_var] == 1)]\n",
    "    tops_zero = zip(tst_zero_top, tst_zero_bot)\n",
    "    df_zero_htst = pd.DataFrame(tops_zero, columns=['tst_zero_top', 'tst_zero_bot'])\n",
    "    df_zero_htst['FORMATION_up'] = formation\n",
    "    df_zero_htst['well'] = well\n",
    "    df_zero_htst['h_tst_zero'] = df_zero_htst.tst_zero_bot - df_zero_htst.tst_zero_top\n",
    "    df_zero_htst = df_zero_htst[['well','FORMATION_up','tst_zero_top','tst_zero_bot','h_tst_zero']]\n",
    "    return df_zero_htst\n",
    "# Print numerical table with layers\n",
    "def ntd_numerical(dataset, wellname, fmname):\n",
    "    \"\"\"\n",
    "    dataset = ntd_final\n",
    "    \"\"\"\n",
    "    df = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname) ][['well','h_tst','top_tst', 'bot_tst','FORMATION_up']]\n",
    "    q50 = df['h_tst'].quantile(q=0.5, interpolation='nearest')\n",
    "    df['q50'] = q50\n",
    "    return df\n",
    "#Cleaning NET variable and making up NET_clp with clipped data\n",
    "def ntd_htst_cleaning(dataset, cutoff):\n",
    "    \"\"\"\n",
    "    dataset - any updated dataset like df_bal...\n",
    "    cutoff - value in TST to remove layers with thickness below cutoff\n",
    "    \"\"\"\n",
    "    df_list_ntd = []\n",
    "    for well in tqdm(dataset.well.unique()):\n",
    "        ntd_well = dataset[(dataset.well ==well)]\n",
    "        ntd_well_cutoff = ntd_well[ntd_well.h_tst >= cutoff]\n",
    "        well_short = df_bal[['well', 'FORMATION_up', 'MD', 'TST', 'GR_N', 'NET', 'FORMATION']]\n",
    "        net_well = well_short[(well_short.well==well)]\n",
    "        net_well['NET_clp'] = 0\n",
    "        for j in range(len(ntd_well_cutoff.well)):\n",
    "            ntd_top = ntd_well_cutoff.iloc[j, 2].round(3)\n",
    "            ntd_bot = ntd_well_cutoff.iloc[j, 3].round(3)\n",
    "            for i in range(len(net_well.TST)):\n",
    "                well_tst = net_well['TST'].iloc[i].round(3)\n",
    "                if well_tst >= ntd_top and well_tst <= ntd_bot:\n",
    "                    net_well['NET_clp'].iloc[i] = 1\n",
    "        df_list_ntd.append(net_well)\n",
    "    net_clp = pd.concat(df_list_ntd)\n",
    "    return net_clp\n",
    "# Cleaning NET_clp variable from zero values with zero_samples <=cutoff\n",
    "def ntd_htst_zero_cleaning(dataset_zero, dataset, cutoff, net_var1, net_var2):\n",
    "    df_list_ntd_zero = []\n",
    "    for well in tqdm(dataset_zero.well.unique()):\n",
    "        ntd_well_zero = dataset_zero[(dataset_zero.well ==well)]\n",
    "        ntd_well_zero_sel = ntd_well_zero[ntd_well_zero.h_tst_zero <= cutoff]\n",
    "        well_zero_short = dataset[['well','FORMATION_up','MD','TST', net_var1, 'GR_N', 'NET', 'FORMATION']]\n",
    "        well_zero_short[net_var2] = well_zero_short[net_var1]\n",
    "        well_zero_sel = well_zero_short[(well_zero_short.well==well)]\n",
    "        for j in range(len(ntd_well_zero_sel.well)):\n",
    "            ntd_zero_top = ntd_well_zero_sel.iloc[j, 2].round(3)\n",
    "            ntd_zero_bot = ntd_well_zero_sel.iloc[j, 3].round(3)\n",
    "            for i in range(len(well_zero_sel.TST)):\n",
    "                well_zero_tst = well_zero_sel['TST'].iloc[i].round(3)\n",
    "                if well_zero_tst >= ntd_zero_top and well_zero_tst <= ntd_zero_bot:\n",
    "                    well_zero_sel[net_var2].iloc[i] = 1\n",
    "        df_list_ntd_zero.append(well_zero_sel)\n",
    "    result = pd.concat(df_list_ntd_zero)\n",
    "    return result\n",
    "# View desired TST-interval\n",
    "def net_view1(dataset, well, top, bot):\n",
    "    dataset = dataset[dataset.well==well][['well','TST','GR_N', 'RHOB', 'NET','NET_clp']]\n",
    "    return dataset[(dataset.TST >= top) & (dataset.TST <= bot)].head(50)\n",
    "#TST sampling & TST KH curve calculation per formation/well\n",
    "def proph_calculation(dataset, net_var):\n",
    "    df_smpl_lst = []\n",
    "    print('TST sampling calculation')\n",
    "    for well_smpl in tqdm(dataset.well.unique()[:]):\n",
    "        tst_sampl = dataset[dataset.well==well_smpl]['TST'].diff()\n",
    "        df_new = dataset[dataset.well==well_smpl].join(tst_sampl, rsuffix='_smpl')    \n",
    "        df_smpl_lst.append(df_new)\n",
    "    df_bal_tst_smpl = pd.concat(df_smpl_lst)\n",
    "    df_kh_lst_fm = []\n",
    "    print('KHtst calculation')\n",
    "    for fm_kh in ['Balakhany VIII', 'Balakhany X']:\n",
    "        df_kh_lst = []\n",
    "        for well_kh in tqdm(dataset.well.unique()[:]):\n",
    "            well_tst_perm = df_bal_tst_smpl[(df_bal_tst_smpl.well==well_kh) & \n",
    "                                            (df_bal_tst_smpl.FORMATION_up==fm_kh)].sort_values(by='MD', ascending=False)\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'LPERM'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'PHIT'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'VSH'] = 0\n",
    "            well_tst_perm['khtst'] = well_tst_perm.LPERM*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['phithtst'] = well_tst_perm.PHIT*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['vshhtst'] = well_tst_perm.VSH*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['KHtst'] = well_tst_perm.khtst.cumsum()\n",
    "            well_tst_perm['PHITHtst'] = well_tst_perm.phithtst.cumsum()\n",
    "            well_tst_perm['VSHHtst'] = well_tst_perm.vshhtst.cumsum()\n",
    "            well_tst_perm = well_tst_perm.sort_values(by='MD')\n",
    "            df_kh_lst.append(well_tst_perm)\n",
    "        df_khlst = pd.concat(df_kh_lst)\n",
    "        df_kh_lst_fm.append(df_khlst)\n",
    "    df_khlst_fm = pd.concat(df_kh_lst_fm)\n",
    "    # df_khlst_fm = df_khlst_fm.dropna()\n",
    "    return df_khlst_fm[['well', 'FORMATION_up', 'MD', 'TST', 'TST_smpl','KHtst','PHITHtst','VSHHtst']]\n",
    "# Comparison NET_clp and NET_clp2\n",
    "def well_display_net(dataset, well, formation, net1='NET_clp', net2_flag=0, net2='NET_clp_v2'):\n",
    "    well_sel = dataset[(dataset.well == well) & (dataset.FORMATION_up == formation)]\n",
    "    depth = well_sel['TST']\n",
    "    grn = well_sel['GR_N']\n",
    "    net = well_sel['NET']\n",
    "    net_clp = well_sel[net1]\n",
    "    if net2_flag == 0:\n",
    "        fig, ax = plt.subplots(1,3, figsize=(4.5,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange'), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange'), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    if net2_flag == 1:\n",
    "        net_clp2 = well_sel[net2]\n",
    "        fig, ax = plt.subplots(1,4, figsize=(6,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange', lw=0.25), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange', lw=0.25), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        ax[3].plot(net_clp2, depth, color='orange', lw=0.25), ax[3].set_xlim(0, 1), ax[3].grid(axis='y')\n",
    "        ax[3].fill_betweenx(depth,net_clp2, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    return fig.show()\n",
    "# Run RFR model with train/test split\n",
    "def rfr_train_test_split(train_dataset, gs_set, scorer, target='KHtst', rng=0.25, margin=0.005):\n",
    "    \"\"\"\n",
    "    'train_ds', \n",
    "    'metrics: r2_train, r2_test, mae_train, mae_test, test_in', \n",
    "    'grid_search', \n",
    "    'result_df', \n",
    "    'train_df', \n",
    "    'test_df'\n",
    "    --------\n",
    "    scorer = make_scorer(mse, greater_is_better=False) <- format scorer like this\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','FORMATION_up',target]])\n",
    "    x = np.array(train_dataset.drop(target, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    # num=42\n",
    "    train_dataset_list.append(train_dataset.drop(['FORMATION_up', target], axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    # x_train_wells = x_train_init[:,2]\n",
    "    # x_test_wells = x_test_init[:,2]\n",
    "    y_train_wells = y_train_init[:,0:2]\n",
    "    y_test_wells = y_test_init[:,0:2]\n",
    "    x_train = x_train_init[:,2:]\n",
    "    x_test = x_test_init[:,2:]\n",
    "    y_train = y_train_init[:,2]\n",
    "    y_test = y_test_init[:,2]\n",
    "    # GridSearch for ML-model\n",
    "    grid_rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "    grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = gs_set, scoring=scorer, cv = 5)\n",
    "    grid_calc_rfr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_rfr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    print('Grid_search: ', grid_rfr)\n",
    "    # Applying Pipeline for ML-model\n",
    "    rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**gd_sr_setting, n_jobs=-1, random_state=42))])\n",
    "    rfr.fit(x_train, y_train)\n",
    "    y_pred_train = rfr.predict(x_train)\n",
    "    y_pred_test = rfr.predict(x_test)\n",
    "    # Reporting\n",
    "    print('Pipeline: ', rfr.steps[1][1])\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    well_fm_train = pd.DataFrame(y_train_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_train = pd.DataFrame(zip(y_train, y_pred_train), columns=['actual','predict'])\n",
    "    df_rfr_train = well_fm_train.join(rfr_train)\n",
    "    df_rfr_train['l_limit'] = df_rfr_train.actual*dwn_range - margin\n",
    "    df_rfr_train['h_limit'] = df_rfr_train.actual*up_range + margin\n",
    "    df_rfr_train['qc'] = 'out'\n",
    "    df_rfr_train['dataset'] = 'train'\n",
    "    df_rfr_train.loc[(df_rfr_train.predict >= df_rfr_train.l_limit) & (df_rfr_train.predict <= df_rfr_train.h_limit), 'qc'] = 'in'\n",
    "    well_fm_test = pd.DataFrame(y_test_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_test = pd.DataFrame(zip(y_test, y_pred_test), columns=['actual','predict'])\n",
    "    df_rfr_test = well_fm_test.join(rfr_test)\n",
    "    df_rfr_test['l_limit'] = df_rfr_test.actual*dwn_range - margin\n",
    "    df_rfr_test['h_limit'] = df_rfr_test.actual*up_range + margin\n",
    "    df_rfr_test['qc'] = 'out'\n",
    "    df_rfr_test['dataset'] = 'test'\n",
    "    df_rfr_test.loc[(df_rfr_test.predict >= df_rfr_test.l_limit) & (df_rfr_test.predict <= df_rfr_test.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_result = pd.concat([df_rfr_train,df_rfr_test])\n",
    "    df_rfr_result['diff'] = (df_rfr_result.actual - df_rfr_result.predict).round(3)\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'train_in':     df_rfr_train['qc'].value_counts(normalize=True)['in'].round(2),\n",
    "                        'test_in':      df_rfr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=train_dataset_list[0]).sort_values(ascending=True)\n",
    "    return {'train_ds':train_dataset_list[0], \n",
    "            'metrics':metrics_dict, \n",
    "            'grid_search' : grids_setting_list, \n",
    "            'result_df' : df_rfr_result,\n",
    "            'train_df' : df_rfr_train,\n",
    "            'test_df' : df_rfr_test,\n",
    "            'feature_imp' : feature_imp}\n",
    "# Run RFR model with loop\n",
    "def rfr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        # Statement of ML-model\n",
    "        rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**hyperdict, n_jobs=-1, random_state=42))])                                                                                  \n",
    "        # Fitting the ML-model\n",
    "        rfr.fit(x_train, y_train)\n",
    "        y_pred_train = rfr.predict(x_train)\n",
    "        y_pred_test = rfr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "    # Building up of dataframe\n",
    "    print(rfr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin\n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff', 'l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 3, 'predict': 3, 'diff': 3})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Run XGBR model with loop \n",
    "def xgbr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        xgbr = Pipeline([(\"scaler\",StandardScaler()),(\"xgbr\",XGBRegressor(**hyperdict, n_jobs=-1, random_state=42))])\n",
    "        # Fitting the ML-model\n",
    "        xgbr.fit(x_train, y_train)\n",
    "        y_pred_train = xgbr.predict(x_train)\n",
    "        y_pred_test = xgbr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(xgbr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "\n",
    "    # Building up of dataframe\n",
    "    print(xgbr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin \n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff','l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 0, 'predict': 0, 'diff': 0})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Display results of ML-modeling\n",
    "def xplot_qc(dataset, dataframe, max_val, rng=0.25):\n",
    "    fig1_ml = px.scatter(dataset[dataframe], x='actual', y='predict', \n",
    "                        color='qc', \n",
    "                        hover_data=['well'], \n",
    "                        width=400, height=400,\n",
    "                        #  color_discrete_sequence=[\"red\", \"green\"]\n",
    "                        )\n",
    "    up_range = rng+1\n",
    "    dwn_range = 1- rng\n",
    "    fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "    fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "    fig2_1_ml=px.line(x=[0,max_val], y=[0,max_val*up_range])\n",
    "    fig2_2_ml=px.line(x=[0,max_val], y=[0,max_val*dwn_range])\n",
    "    fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "    fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "    fig3_ml.update_layout(  title = 'Comparison Actual vs Pred' + \n",
    "                                    ' QC_train: ' + str(dataset['metrics']['train_in']) +\n",
    "                                    ' QC_test: ' + str(dataset['metrics']['test_in']),\n",
    "                            width=600,height=400, xaxis_title='actual', yaxis_title='predict',\n",
    "                            margin=dict(l=10,r=10,b=10,t=40))\n",
    "    return fig3_ml.show()\n",
    "# Calculate weighted avg properties \n",
    "def avg_prop_calculation(dataset_ntd, dataset, formation):\n",
    "    well_data = []\n",
    "    well_formation = formation\n",
    "    for well in tqdm(dataset_ntd.well.unique()):\n",
    "        # print(well)\n",
    "        ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "        well_avgprop_sel = dataset[(dataset.well==well)]\n",
    "        well_phit = []\n",
    "        well_phit10 = []\n",
    "        well_phit50 = []\n",
    "        well_phit90 = []\n",
    "        well_vsh = []\n",
    "        well_vsh10 = []\n",
    "        well_vsh50 = []\n",
    "        well_vsh90 = []\n",
    "        well_gperm = []\n",
    "        well_h = []\n",
    "        for layers in range(len(ntd_well_avgprop.well)):\n",
    "            ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "            ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "            ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "            phit_lst = []\n",
    "            vsh_lst = []\n",
    "            perm_lst = []\n",
    "            for depth in range(len(well_avgprop_sel.TST)):\n",
    "                well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                    phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                    vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                    perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "            well_phit.append(mean(phit_lst)*ntd_h)\n",
    "            well_phit10.append(np.quantile(phit_lst, 0.1)*ntd_h)\n",
    "            well_phit50.append(np.quantile(phit_lst, 0.5)*ntd_h)\n",
    "            well_phit90.append(np.quantile(phit_lst, 0.9)*ntd_h)\n",
    "            well_vsh.append(mean(vsh_lst)*ntd_h)\n",
    "            well_vsh10.append(np.quantile(vsh_lst, 0.1)*ntd_h)\n",
    "            well_vsh50.append(np.quantile(vsh_lst, 0.5)*ntd_h)\n",
    "            well_vsh90.append(np.quantile(vsh_lst, 0.9)*ntd_h)\n",
    "            well_gperm.append(gmean(perm_lst)*ntd_h)\n",
    "            well_h.append(ntd_h)\n",
    "        well_phit_wavg = sum(well_phit)/sum(well_h)\n",
    "        well_phit10_wavg = sum(well_phit10)/sum(well_h)\n",
    "        well_phit50_wavg = sum(well_phit50)/sum(well_h)\n",
    "        well_phit90_wavg = sum(well_phit90)/sum(well_h)\n",
    "        well_vsh_wavg = sum(well_vsh)/sum(well_h)\n",
    "        well_vsh10_wavg = sum(well_vsh10)/sum(well_h)\n",
    "        well_vsh50_wavg = sum(well_vsh50)/sum(well_h)\n",
    "        well_vsh90_wavg = sum(well_vsh90)/sum(well_h)\n",
    "        well_perm_wavg = sum(well_gperm)/sum(well_h)\n",
    "        well_hmax = max(well_h)\n",
    "        well_h_p50 = np.quantile(well_h, 0.5)\n",
    "        well_layers_count =len(well_h)\n",
    "        well_hsum = sum(well_h)\n",
    "        well_data.append([  well, well_formation, \n",
    "                            well_hmax, well_h_p50, well_layers_count, well_hsum,\n",
    "                            well_phit_wavg, well_phit10_wavg, well_phit50_wavg, well_phit90_wavg,\n",
    "                            well_vsh_wavg, well_vsh10_wavg, well_vsh50_wavg, well_vsh90_wavg,\n",
    "                            well_perm_wavg])\n",
    "    result = pd.DataFrame(well_data, columns=[  'well','FORMATION_up',\n",
    "                                                'htst_max', 'htst_p50','htst_count', 'htst_sum',            \n",
    "                                                'phit_wavg', 'phit10_wavg','phit50_wavg','phit90_wavg',\n",
    "                                                'vsh_wavg', 'vsh10_wavg', 'vsh50_wavg', 'vsh90_wavg',\n",
    "                                                'perm_wavg'])\n",
    "    return result\n",
    "# Euclidian dist calculation with prop\n",
    "def dist_prop_calc(dataset, dist_formation, dist_cutoff, value):\n",
    "    \"\"\"\n",
    "    dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "    \"\"\"\n",
    "    data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "    row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "    distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean', 'TVD_SCS']]), columns=list(data.well))\n",
    "    distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "    distance_fm_well.reset_index(inplace=True)\n",
    "    def well_kh_accum(wells, dataset, kh_formation):\n",
    "        well_kh_accum = []\n",
    "        well_x_accum = []\n",
    "        well_y_accum = []\n",
    "        for i in wells:\n",
    "            well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "            well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "            well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "        well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "        well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "        well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "        well_x3.columns = ['x1','x2','x3']\n",
    "        well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "        well_y3.columns = ['y1','y2','y3']\n",
    "        final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                            well_x3.reset_index().drop('index',axis=1), \n",
    "                            well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "        return final\n",
    "    df_collect = []\n",
    "    for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "        well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "        well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "        well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "        well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "        well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "        well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "        well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "        well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "        concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "        result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "        df_collect.append(result)     \n",
    "    df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "    df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "    return df_well_kh_dist\n",
    "# Feature importance bar chart for 1-to-all algorithm\n",
    "def feature_imp_loop(dataset, wellname, fmname, xsize, ysize):\n",
    "    # dataset = test['result_df']\n",
    "    data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    ftr_imp = data['features_imp'].values[0]\n",
    "    f, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ftr_imp.plot.barh()\n",
    "    ax.set_title('RFR feature imp  ' + wellname + ' ' + fmname)\n",
    "    ax.tick_params(axis='y', labelsize=8, rotation=0)\n",
    "    return f.show()\n",
    "# Save datafram to csv\n",
    "def save_tocsv(dataframe, filename, flag):\n",
    "    if flag == 1:\n",
    "        # Saving avg_prop dataframe to .csv\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\'\n",
    "        dataframe.to_csv(path + filename)\n",
    "    else:\n",
    "        pass\n",
    "# Feature importance bar chart for split dataframe\n",
    "def feature_imp_split(dataset, xsize, ysize):\n",
    "    fig, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ax = dataset.plot.barh()\n",
    "    ax.set_title(\"RFR Feature Importances\")\n",
    "    ax.tick_params(axis='y', labelsize=9, rotation=0)\n",
    "    ax.figure.tight_layout()\n",
    "    return fig.show()\n",
    "# Logging results of ml\n",
    "def write_res_file(finename, comments, target, trainds, metrics, gridsearch):\n",
    "    with open(finename, 'a') as file:\n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.now()\n",
    "        # Write the result to the file\n",
    "        file.write(f'\\n{current_datetime} \\n {comments} target: {target}')\n",
    "        file.write(f'\\n training_ds_{trainds} \\n metrics_{[metrics]} \\n grid_search_{gridsearch}')\n",
    "    file.close()\n",
    "# Remover categorical values from datasets\n",
    "def cat_finder(dataset):\n",
    "    \"\"\"\n",
    "    cat_list: categorical columns to drop out\n",
    "    get_dum_list: categorical columns to run via pd.get_dummies\n",
    "    \"\"\"\n",
    "    cat_list = []\n",
    "    gm_list = []\n",
    "    for col in dataset.columns:\n",
    "        # print(i)\n",
    "        if dataset[col].dtype == 'string':\n",
    "            cat_list.append(col)\n",
    "            if col != 'well':\n",
    "                gm_list.append(col)\n",
    "    # return {'cat_list':cat_list,\n",
    "    #         'get_dum_list': gm_list}\n",
    "    return cat_list, gm_list\n",
    "# Display results of ML-modeling ver2\n",
    "def xplot_qc2(data, max_val, rng, margin, round):\n",
    "    data = data.round({'actual': round, 'predict': round, 'diff': round})\n",
    "    ds_train = data[data.dataset == 'train']\n",
    "    ds_test = data[data.dataset == 'test']\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "    qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "    scatter_train = go.Scatter( x=ds_train.actual, y=ds_train.predict,\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_tr, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_train[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    scatter_test = go.Scatter(  x=ds_test.actual, y=ds_test.predict, \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_ts, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_test[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('train ds', 'test ds'))\n",
    "    fig.add_trace(scatter_train,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "    fig.add_trace(scatter_test,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "    fig.update_layout(  title_text= ('rfr_train_test_split'), width=900, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling ver2 via loop    \n",
    "def xplot_qc2_loop(data, max_val, rng, margin=0.005):\n",
    "    data = data.round({'actual': 3, 'predict': 3, 'diff ': 3})\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors = [colors[qc] for qc in data.qc]\n",
    "    scatter = go.Scatter( x=data.actual, y=data.predict,\n",
    "                            mode='markers',\n",
    "                            marker=dict(color=qc_colors, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = data[['well','actual','predict', 'diff', 'FORMATION_up']],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                            )\n",
    "    fig = go.Figure()\n",
    "    line_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line_up)\n",
    "    fig.add_trace(line_dw)\n",
    "    fig.update_xaxes(title_text='actual')\n",
    "    fig.update_yaxes(title_text='predict')\n",
    "    fig.update_layout(  title_text= ('rfr_loop'), width=450, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling on map\n",
    "def map_qc(metadata, data, fmname, scale):\n",
    "    data['diff'] = abs(data['diff'])\n",
    "    data = data[data.FORMATION_up == fmname]\n",
    "    data_in = data[data.qc=='in']\n",
    "    data_out = data[data.qc=='out']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    platform  = go.Scatter(         x=field_avg_coord.X_wellhead, y=field_avg_coord.Y_wellhead, customdata = field_avg_coord[['field']],\n",
    "                                    text=field_avg_coord['field'], textposition=\"middle right\",\n",
    "                                    marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                                    mode='markers+text', \n",
    "                                    marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_in = go.Scatter(   x=data_in.X, y=data_in.Y,\n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='circle', color='green', size=data_in['actual']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)\n",
    "                                    ),\n",
    "                                    customdata = data_in[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_out = go.Scatter(  x=data_out.X, y=data_out.Y, \n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='diamond', color='red', size=data_out['diff']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                    customdata = data_out[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(platform)\n",
    "    fig.add_trace(scatter_data_in)\n",
    "    fig.add_trace(scatter_data_out)\n",
    "    fig.update_layout(title_text= ('rfr_train_test_split'),autosize=True, width=1000, height=600, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Pairplot new version\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "# Columns reorder for better display of variables\n",
    "def columns_reorder(dataset, selected_column):\n",
    "    new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "    dataset = dataset[new_order]\n",
    "    return dataset\n",
    "# Just simple x-plot for 1 dataframe\n",
    "def log_map_plot(dataframe, x_var, y_var, min_val, max_val):\n",
    "    fig = go.Figure()\n",
    "    scatter = go.Scatter(   x=dataframe[x_var], y=dataframe[y_var], \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='orange', size=10, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = dataframe[['well',x_var,y_var]],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},x:%{customdata[1]}, y:%{customdata[2]}<extra></extra>\"])\n",
    "                            )\n",
    "    line = go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', line=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line)\n",
    "    fig.update_layout(  title_text= ('scatter plot'), width=600, height=600, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Joining main and additional dataframes for predictions\n",
    "def join_add_df_prediction(base_dataframe, add_dataframe, target_var):\n",
    "    \"\"\"\n",
    "    Both dataframes have contain 'well' & 'FORMATION_up' for joining\n",
    "    \"\"\"\n",
    "    join_dataframe = base_dataframe.set_index(['well','FORMATION_up']).join(add_dataframe.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    col_names, gm_list = cat_finder(join_dataframe)\n",
    "    df_corr = join_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(join_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    join_dataframe_gm = pd.concat([join_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, join_dataframe_gm\n",
    "# Preparation dataframes for pairplot and for predictions\n",
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            # print(i)\n",
    "            if dataset[col].dtype == 'string':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        # return {'cat_list':cat_list,\n",
    "        #         'get_dum_list': gm_list}\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "# Function to calculate grid_search via train_split\n",
    "def run_rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin, logtxt_name, comment, xplot_flag, ftr_imp_flag):\n",
    "    model_res = rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, \n",
    "                    model_res['train_ds'], model_res['metrics'], model_res['grid_search'])\n",
    "    print('train_ds: ', model_res['train_ds'])\n",
    "    print('metrics: ', model_res['metrics'])\n",
    "    print('grid_search: ', model_res['grid_search'])\n",
    "    model_res_hyper_par = model_res['grid_search'][0]\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2(dataset['result_df'], 0.3, 0.05, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(dataset['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return model_res_hyper_par\n",
    "# Function to calculate target via 1-to-all\n",
    "def run_rfr_1_to_all(dataset, hyperdict, target, rng, margin, logtxt_name, comment, xplot_flag, max_val, ftr_imp_flag):\n",
    "    loop_res = rfr_loop(dataset, 'FORMATION_up', target, hyperdict, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, loop_res['train_ds'], loop_res['metrics'], loop_res['grid_search'])\n",
    "    loop_res_pred = loop_res['result_df']\n",
    "    print('train_ftrs: ',loop_res['train_ftrs'])\n",
    "    print('metrics: ',loop_res['metrics'])\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2_loop(loop_res['result_df'], max_val, rng, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(loop_res['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return loop_res_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well quality calculation & printing of diagrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_clean_v2():\n",
    "    #Counting of bad quality logs\n",
    "    bal8_list = [   'Balakhany VIII sand', 'Balakhany VIII 20',   'Balakhany VIII 10',   'Balakhany VIII 25',\n",
    "                    'Balakhany VIII 15',    'Balakhany VIII 5']\n",
    "    well_tot8 = []\n",
    "    well_zero8 = []\n",
    "    well_clean8 = []\n",
    "    for j in df_bal[(df_bal.FORMATION.isin(bal8_list) & (df_bal.PHIT>0))].well.unique():\n",
    "        phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII')]))\n",
    "        phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII') & (df_bal.PHIT > 0)]))\n",
    "        well_tot8.append(j)\n",
    "        if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "            well_zero8.append(j)\n",
    "        else:\n",
    "            well_clean8.append(j)\n",
    "            # well_display_ntd(df_bal, j, 'Balakhany VIII', 'NET', round((phit_nonzero/phit_zero),2), 1) #printing well plots with high quality logs\n",
    "\n",
    "    bal10_list = ['Balakhany X', 'Balakhany X sand', 'Balakhany X 40', 'Balakhany X 20', 'Balakhany X 50']\n",
    "    well_tot10 = []\n",
    "    well_zero10 = []\n",
    "    well_clean10 = []\n",
    "    for j in df_bal[(df_bal.FORMATION.isin(bal10_list) & (df_bal.PHIT>0))].well.unique():\n",
    "        phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X')]))\n",
    "        phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X') & (df_bal.PHIT > 0)]))\n",
    "        well_tot10.append(j)\n",
    "        if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "            # well_display_ntd(df_bal, j, 'Balakhany X', 'NET', round((phit_nonzero/phit_zero),2), 1)\n",
    "            well_zero10.append(j)\n",
    "        else:\n",
    "            well_clean10.append(j)\n",
    "    print('well_tot8', len(well_tot8))\n",
    "    print('well_zero8', len(well_zero8))\n",
    "    print('well_clean8', len(well_clean8))\n",
    "    print('----------------------')\n",
    "    print('well_tot10', len(well_tot10))\n",
    "    print('well_zero10', len(well_zero10))\n",
    "    print('well_clean10', len(well_clean10))\n",
    "\n",
    "    # broken wells Bal8\n",
    "    # A08, A19, H01Z, J05 \n",
    "    # broken wells Bal10\n",
    "    # C31, D25\n",
    "    # high tst_interv Bal8\n",
    "    # E30Z\n",
    "    # small tst_interv Bal8\n",
    "    # G01Z, E05, E01, E01Y, E11Z, E07, H01Y, H01Z, A14\n",
    "    # Add wells after review Bal8 \n",
    "    # D04Z, \n",
    "    # Remove wells from clean_list by any reasons\n",
    "    remove_tst8 = ['A08','A19','J05','E30Z','G01Z', 'E05', 'E01', 'E01Y', 'E11Z', 'E07', 'H01Y', 'H01Z', 'A14']\n",
    "    well_clean8_v2 = [i for i in well_clean8 if i not in remove_tst8]\n",
    "    remove_tst10 = ['C31','D25', 'E21A']\n",
    "    well_clean10_v2 = [i for i in well_clean10 if i not in remove_tst10]\n",
    "    print('----------------------')\n",
    "    print('well_clean8_v2: ', len(well_clean8_v2))\n",
    "    print('well_clean10_v2: ', len(well_clean10_v2))\n",
    "    return well_clean8_v2, well_clean10_v2\n",
    "well_clean8_v2, well_clean10_v2 = well_clean_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetThicknessDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetThicknessDistribution():\n",
    "    # Limitation dataframe to cleaned wells for Bal8 & Bal10\n",
    "    df_net_bal8 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal8 = df_net_bal8[df_net_bal8.well.isin(well_clean8_v2) & (df_net_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net_bal10 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal10 = df_net_bal10[df_net_bal10.well.isin(well_clean10_v2) & (df_net_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation dataframe with h_tst from MD to TST for Bal8\n",
    "    df_list8 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal8')\n",
    "    for well in tqdm(df_net_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal8, well, 'Balakhany VIII', 'NET')\n",
    "        df_list8.append(df)\n",
    "    ntd_net_8 = pd.concat(df_list8)\n",
    "    df_list10 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal10')\n",
    "    for well in tqdm(df_net_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal10, well, 'Balakhany X', 'NET')\n",
    "        df_list10.append(df)\n",
    "    ntd_net_10 = pd.concat(df_list10)\n",
    "    ntd_net_final = pd.concat([ntd_net_8, ntd_net_10])\n",
    "    # Cleaning NET variable and making up NET_clp with clipped data, join NET_clp to main dataframe\n",
    "    print('Cleaning NET variable and making up NET_clp with clipped data')\n",
    "    net_clp =  ntd_htst_cleaning(ntd_net_final, 1)\n",
    "    df_bal_net = df_bal.set_index(['well','MD']).join(net_clp.drop(\n",
    "        ['FORMATION_up','NET','TST', 'FORMATION', 'GR_N'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net = df_bal_net[df_bal_net.NET_clp.notna()]\n",
    "    # Cleaning NET_clp from 1-point zero\n",
    "    print('Cleaning NET_clp from 1-point zero')\n",
    "    for i in tqdm(range(len(df_bal_net.NET_clp))):\n",
    "        if (df_bal_net.NET_clp.iloc[i] == 0 and  \n",
    "            df_bal_net.NET_clp.iloc[i-1] == 1 and \n",
    "            df_bal_net.NET_clp.iloc[i+1] == 1):\n",
    "            df_bal_net.NET_clp.iloc[i] = 1\n",
    "    # NET-zero layers removing\n",
    "    df_zero_bal = df_bal_net[['well', 'MD', 'TST', 'NET_clp', 'FORMATION_up']]\n",
    "    df_zero_bal8 = df_zero_bal[df_zero_bal.well.isin(well_clean8_v2) & (df_zero_bal.FORMATION_up=='Balakhany VIII')]\n",
    "    df_zero_list8 = []\n",
    "    print('NET-zero layers removing Bal8')\n",
    "    for well in tqdm(df_zero_bal8.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal8, well, 'Balakhany VIII', 'NET_clp')\n",
    "        df_zero_list8.append(df)\n",
    "    ntd_zero_8 = pd.concat(df_zero_list8)\n",
    "    df_zero_bal10 = df_zero_bal[df_zero_bal.well.isin(well_clean10_v2) & (df_zero_bal.FORMATION_up=='Balakhany X')]\n",
    "    df_zero_list10 = []\n",
    "    print('NET-zero layers removing Bal10')\n",
    "    for well in tqdm(df_zero_bal10.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal10, well, 'Balakhany X', 'NET_clp')\n",
    "        df_zero_list10.append(df)\n",
    "    ntd_zero_10 = pd.concat(df_zero_list10)\n",
    "    ntd_zero = pd.concat([ntd_zero_8, ntd_zero_10])\n",
    "    print('Run ntd_htst_zero_cleaning')\n",
    "    net_clp2 = ntd_htst_zero_cleaning(ntd_zero, df_bal_net, 1, 'NET_clp', 'NET_clp2')\n",
    "    #Joining NET_clp2 to main dataframe df_bal_net\n",
    "    df_bal_net2 = df_bal_net.set_index(['well','MD']).join(net_clp2.drop(\n",
    "        ['FORMATION_up','GR_N', 'NET','NET_clp', 'FORMATION','TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net2 = df_bal_net2[df_bal_net2.NET_clp2.notna()]\n",
    "    # Displaying of ramdom well for example\n",
    "    well_display_net(df_bal_net2, 'J28', 'Balakhany VIII', 'NET_clp', 1, 'NET_clp2')\n",
    "    return df_bal_net2\n",
    "df_bal_net2 = NetThicknessDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full set of FU for Bal VIII / X FORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_fullset_bal_fu_calc_prop(dataset):\n",
    "    \"\"\"\n",
    "    Calculation XY coordinates of Bal VIII/X \n",
    "    Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2\n",
    "    Calculation average properties for Bal8 and Bal10 based on NET_clp2\n",
    "    Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    \"\"\"\n",
    "    # Calculation XY coordinates of Bal VIII/X\n",
    "    df_bal_net2_kh_xy8 = dataset[   (dataset.well.isin(well_clean8_v2)) & \n",
    "                                    (dataset.FORMATION_up=='Balakhany VIII')][['well','FORMATION_up','X_mean','Y_mean','TVD_SCS','field']].groupby(\n",
    "                                    ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index() \n",
    "    df_bal_net2_kh_xy10 = dataset[  (dataset.well.isin(well_clean10_v2)) & \n",
    "                                    (dataset.FORMATION_up=='Balakhany X')][['well','FORMATION_up','X_mean','Y_mean','TVD_SCS','field']].groupby(\n",
    "                                    ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "    df_bal_net2_kh_xy = pd.concat([df_bal_net2_kh_xy8, df_bal_net2_kh_xy10]).sort_values(by='well')  \n",
    "\n",
    "    df_net2_bal8 = dataset[[ 'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[['well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "    ntd_net2_final = pd.concat([ntd_net2_8, ntd_net2_10])\n",
    "    # Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and available FU Bal VIII/X\n",
    "    df_kh8 = df_net2_bal8.groupby('well')[[ 'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                            'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[1]).reset_index()                              \n",
    "    df_kh10 = df_net2_bal10.groupby('well')[[   'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[1]).reset_index()\n",
    "    df_tst8 = df_net2_bal8.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[1]).reset_index()\n",
    "    df_tst8['FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_tst10 = df_net2_bal10.groupby('well')[['TST','TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[1]).reset_index()\n",
    "    df_tst10['FORMATION_up'] = 'Balakhany X'\n",
    "    df_kh_fin = pd.concat([df_kh8, df_kh10])\n",
    "    df_tst_fin = pd.concat([df_tst8, df_tst10])\n",
    "    df_tst_fin.rename(columns={'TST':'tst_interv', 'TVD_SCS':'tvd_interv'}, inplace=True)\n",
    "    df_tst_kh = df_tst_fin.set_index(['well','FORMATION_up']).join(df_kh_fin.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    # Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation average properties for Bal8 and Bal10 based on NET_clp2')\n",
    "    avg_prop_bal8 = avg_prop_calculation(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    avg_prop_bal10 = avg_prop_calculation(ntd_net2_10, df_net2_bal10, 'Balakhany X')   \n",
    "    avg_prop_final = pd.concat([avg_prop_bal8, avg_prop_bal10])\n",
    "    avg_prop_final.sort_values(by='well', inplace=True)  \n",
    "    # Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    avg_prop_tst_kh = avg_prop_final.set_index(['well','FORMATION_up']).join(df_tst_kh.set_index(['well','FORMATION_up'])).reset_index().sort_values(by='well')\n",
    "    avg_prop_tst_kh = avg_prop_tst_kh.round({   'htst_max': 0, 'htst_sum': 0, 'tst_interv': 0, 'tvd_interv': 0, 'TVD_SCS': 0, \n",
    "                                                'KHtst': 0, 'PHITHtst': 0, 'VSHHtst': 0, 'perm_wavg':1, 'phit_wavg':2})\n",
    "    return avg_prop_tst_kh\n",
    "def prep_dist_calc_brief(dataset, offset):\n",
    "    avg_prop_targKH = dataset[['well', 'FORMATION_up', 'KHtst']]\n",
    "    df_dist_kh_bal8 = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'KHtst')\n",
    "    df_dist_kh_bal10 = dist_prop_calc(dataset, 'Balakhany X', offset, 'KHtst')\n",
    "    df_dist_kh_bal_concat = pd.concat([df_dist_kh_bal8, df_dist_kh_bal10])\n",
    "    df_dist_kh_bal_fin = df_dist_kh_bal_concat.set_index(['well', 'FORMATION_up']).join(avg_prop_targKH.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "    df_dist_kh_bal_fin['dist1_op'] = 1/df_dist_kh_bal_fin.dist1\n",
    "    df_dist_kh_bal_fin['dist2_op'] = 1/df_dist_kh_bal_fin.dist2\n",
    "    df_dist_kh_bal_fin['dist3_op'] = 1/df_dist_kh_bal_fin.dist3\n",
    "    return df_dist_kh_bal_fin\n",
    "def prep_remove_balfu_calc_prop(dataset):\n",
    "    \"\"\"\n",
    "    Calculation XY coordinates of Bal VIII sand/X sand\n",
    "    Removing Bal VIII and Bal X from FORMATION\n",
    "    Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    Calculation average properties for Bal8 and Bal10 based on NET_clp2.\n",
    "    Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    \"\"\"\n",
    "    # Calculation XY coordinates of Bal VIII/X\n",
    "    df_bal_net2_kh_cut_xy8 = dataset[   (dataset.well.isin(well_clean8_v2)) & \n",
    "                                        (dataset.FORMATION_up=='Balakhany VIII') &\n",
    "                                        (dataset.FORMATION != 'Balakhany VIII')][['well','FORMATION_up','X_mean','Y_mean','field']].groupby(\n",
    "                                        ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index() \n",
    "    df_bal_net2_kh_cut_xy10 = dataset[  (dataset.well.isin(well_clean10_v2)) & \n",
    "                                        (dataset.FORMATION_up=='Balakhany X') &\n",
    "                                        (dataset.FORMATION != 'Balakhany X')][['well','FORMATION_up','X_mean','Y_mean','field']].groupby(\n",
    "                                        ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "    df_bal_net2_kh_cut_xy = pd.concat([df_bal_net2_kh_cut_xy8, df_bal_net2_kh_cut_xy10]).sort_values(by='well')  \n",
    "    # Removing Bal VIII and Bal X from FORMATION to calc avg prop\n",
    "    df_net2_bal8_cut = dataset[[ 'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8_cut = df_net2_bal8_cut[    df_net2_bal8_cut.well.isin(well_clean8_v2) & \n",
    "                                            (df_net2_bal8_cut.FORMATION_up=='Balakhany VIII') &\n",
    "                                            (df_net2_bal8_cut.FORMATION != 'Balakhany VIII')]\n",
    "    df_net2_bal10_cut = dataset[['well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10_cut = df_net2_bal10_cut[      df_net2_bal10_cut.well.isin(well_clean10_v2) & \n",
    "                                                (df_net2_bal10_cut.FORMATION_up=='Balakhany X') &\n",
    "                                                (df_net2_bal10_cut.FORMATION != 'Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2 and renoved FU Bal VIII/X')\n",
    "    df_recalc_list8_cut = []\n",
    "    for well in tqdm(df_net2_bal8_cut.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8_cut, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8_cut.append(df)\n",
    "    ntd_net2_8_cut = pd.concat(df_recalc_list8_cut)\n",
    "    ntd_net2_8_cut.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10_cut = []\n",
    "    for well in tqdm(df_net2_bal10_cut.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10_cut, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10_cut.append(df)\n",
    "    ntd_net2_10_cut = pd.concat(df_recalc_list10_cut)\n",
    "    ntd_net2_10_cut.drop_duplicates(inplace=True)\n",
    "    ntd_net2_cut_final = pd.concat([ntd_net2_8_cut, ntd_net2_10_cut])\n",
    "    # Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    df_kh8_cut = df_net2_bal8_cut.groupby('well')[[ 'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                    'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[0]).reset_index()                              \n",
    "    df_kh10_cut = df_net2_bal10_cut.groupby('well')[[   'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                        'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    df_tst8_cut = df_net2_bal8_cut.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index()\n",
    "    df_tst8_cut['FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_tst10_cut = df_net2_bal10_cut.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index()\n",
    "    df_tst10_cut['FORMATION_up'] = 'Balakhany X'\n",
    "    df_kh_cut_fin = pd.concat([df_kh8_cut, df_kh10_cut])\n",
    "    df_tst_cut_fin = pd.concat([df_tst8_cut, df_tst10_cut])\n",
    "    df_tst_cut_fin.rename(columns={'TST':'tst_interv', 'TVD_SCS':'tvd_interv'}, inplace=True)\n",
    "    df_tst_kh_cut = df_tst_cut_fin.set_index(['well','FORMATION_up']).join(df_kh_cut_fin.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    # Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X')\n",
    "    avg_prop_bal8_cut = avg_prop_calculation(ntd_net2_8_cut, df_net2_bal8_cut, 'Balakhany VIII')\n",
    "    avg_prop_bal10_cut = avg_prop_calculation(ntd_net2_10_cut, df_net2_bal10_cut, 'Balakhany X')   \n",
    "    avg_prop_final_cut = pd.concat([avg_prop_bal8_cut, avg_prop_bal10_cut])\n",
    "    avg_prop_final_cut.sort_values(by='well', inplace=True)  \n",
    "    # Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    avg_prop_tst_kh_cut = avg_prop_final_cut.set_index(['well','FORMATION_up']).join(df_tst_kh_cut.set_index(['well','FORMATION_up'])).reset_index().sort_values(by='well')\n",
    "    avg_prop_tst_kh_cut = avg_prop_tst_kh_cut.round({   'htst_max': 0, 'htst_sum': 0, 'tst_interv': 0, 'tvd_interv': 0, 'TVD_SCS': 0, \n",
    "                                                        'KHtst': 0, 'PHITHtst': 3, 'VSHHtst': 3, 'perm_wavg':5, 'phit_wavg':5})\n",
    "    return avg_prop_tst_kh_cut                  \n",
    "def prep_dist_calc(dataset, offset, blacklist):\n",
    "    \"\"\"\n",
    "    Calculation properties based for offset wells: KHtst and phit_wavg\n",
    "    Dataset - dataset with log data and any other variables \n",
    "    Offset - I prefer 0 or 300m, but it could be various \n",
    "    Blacklist - indices of rows which need to be removed\n",
    "    \"\"\"\n",
    "    df_dist_kh_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'KHtst')\n",
    "    df_dist_kh_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'KHtst')\n",
    "    df_dist_kh_bal_cut_fin = pd.concat([df_dist_kh_bal8_cut, df_dist_kh_bal10_cut])\n",
    "    df_dist_ph_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'phit_wavg')\n",
    "    df_dist_ph_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'phit_wavg')\n",
    "    df_dist_ph_bal_cut_fin = pd.concat([df_dist_ph_bal8_cut, df_dist_ph_bal10_cut])\n",
    "    df_dist_htst_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'htst_sum')\n",
    "    df_dist_htst_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'htst_sum')\n",
    "    df_dist_htst_bal10_cut_fin = pd.concat([df_dist_htst_bal8_cut, df_dist_htst_bal10_cut])\n",
    "    htst_dist = df_dist_htst_bal10_cut_fin[['well','FORMATION_up', 'htst_sum_1', 'htst_sum_2', 'htst_sum_3']]\n",
    "    \n",
    "    phit_dist = df_dist_ph_bal_cut_fin[['well','FORMATION_up', 'phit_wavg_1', 'phit_wavg_2', 'phit_wavg_3']]\n",
    "    phit_dist_htst = phit_dist.set_index(['well','FORMATION_up']).join(htst_dist.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    df_dist_ph_kh_cut = df_dist_kh_bal_cut_fin.set_index(['well','FORMATION_up']).join(phit_dist_htst.set_index(['well','FORMATION_up']))\n",
    "    avg_prop_tst_kh_dist_cut = dataset.set_index(['well','FORMATION_up']).join(df_dist_ph_kh_cut).reset_index()\n",
    "\n",
    "    avg_prop_tst_kh_dist_cut['dist1_op'] = 1/avg_prop_tst_kh_dist_cut.dist1\n",
    "    avg_prop_tst_kh_dist_cut['dist2_op'] = 1/avg_prop_tst_kh_dist_cut.dist2\n",
    "    avg_prop_tst_kh_dist_cut['dist3_op'] = 1/avg_prop_tst_kh_dist_cut.dist3\n",
    "\n",
    "    index_to_drop = blacklist\n",
    "    avg_prop_tst_kh_cut_test = avg_prop_tst_kh_dist_cut.drop(index_to_drop).reset_index().drop('index', axis=1)\n",
    "    return avg_prop_tst_kh_cut_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullset_bal8_10():\n",
    "    # KHtst calculation and join to the main dataframe df_bal_net2\n",
    "    df_kh = proph_calculation(df_bal_net2, 'NET_clp2')\n",
    "    df_bal_net2_kh_init = df_bal_net2.set_index(['well','MD']).join(df_kh.drop(['FORMATION_up', 'TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    # Cleaning KHtst dataset according to mentioned well lists\n",
    "    df_bal_net2_kh8 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean8_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany VIII')]\n",
    "    df_bal_net2_kh10 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean10_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany X')]\n",
    "    df_bal_net2_kh = pd.concat([df_bal_net2_kh8, df_bal_net2_kh10])\n",
    "    # Save df_bal_net2_kh to csv-file\n",
    "    save_tocsv(df_bal_net2_kh,'ACG_wells_JOINT_BEST_v10_output.csv', 0)\n",
    "    # Preparation full set of Bal8/10 and save it to csv\n",
    "    avg_prop_tst_kh = prep_fullset_bal_fu_calc_prop(df_bal_net2_kh)\n",
    "    save_tocsv(avg_prop_tst_kh,'avg_prop_tst_kh.csv', 0)\n",
    "    # Dist calculation for full set of Bal8/10\n",
    "    df_dist_kh_bal_fin = prep_dist_calc_brief(avg_prop_tst_kh, 0)\n",
    "    return avg_prop_tst_kh, df_dist_kh_bal_fin, df_bal_net2_kh\n",
    "avg_prop_tst_kh, df_dist_kh_bal_fin, df_bal_net2_kh = fullset_bal8_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 offsets wells\n",
    "def display_3offset_wells(well, formation, dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh):\n",
    "    \"\"\"\n",
    "    Pay attention dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh\n",
    "    well:       just well name\n",
    "    formation:  just formation\n",
    "    \"\"\"\n",
    "    def well_offset_selection(dataset_dist, fmname, well_target):\n",
    "        try:\n",
    "            well_df = dataset_dist[(dataset_dist.well == well_target) & (dataset_dist.FORMATION_up == fmname)][['well', 'well1', 'well2', 'well3',\n",
    "                                                                                                                        'dist1', 'dist2', 'dist3',\n",
    "                                                                                                                'KHtst','KHtst_1', 'KHtst_2', 'KHtst_3']]\n",
    "            well1 = well_df['well1'].iloc[0]\n",
    "            well2 = well_df['well2'].iloc[0]\n",
    "            well3 = well_df['well3'].iloc[0]\n",
    "            dist1 = well_df['dist1'].astype('int').iloc[0]\n",
    "            dist2 = well_df['dist2'].astype('int').iloc[0]\n",
    "            dist3 = well_df['dist3'].astype('int').iloc[0]\n",
    "            kh = well_df['KHtst'].astype('int').iloc[0]\n",
    "            kh1 = well_df['KHtst_1'].astype('int').iloc[0]\n",
    "            kh2 = well_df['KHtst_2'].astype('int').iloc[0]\n",
    "            kh3 = well_df['KHtst_3'].astype('int').iloc[0]\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return {'target': well_target, 'w1':well1, 'w2':well2, 'w3':well3, \n",
    "                'dist': 0,'d1':dist1, 'd2':dist2,'d3':dist3,\n",
    "                'kh':kh,'kh1':kh1, 'kh2':kh2, 'kh3':kh3}\n",
    "    def display_tracks(dataset, wellname, fmname, ref_depth, depth_step, r, c, kh_value, dist):\n",
    "        try:\n",
    "            data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "            depth = data[ref_depth]\n",
    "            grn = data['GR_N']\n",
    "            vsh = data['VSH']\n",
    "            rhob = data['RHOB'] \n",
    "            npss = data['NPSS']\n",
    "            rdeep = data['RDEEP']\n",
    "            phit = data['PHIT'] \n",
    "            net = data['NET_clp2']\n",
    "            perm = data['LPERM']\n",
    "            kh = data['KHtst']\n",
    "            well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            ax[r,c].plot(grn, depth, color='lightgreen', lw=2, zorder=10)\n",
    "            ax[r,c].set_xlim(0, 150) \n",
    "            ax[r,c].grid(axis='y')\n",
    "            ax[r,c].invert_yaxis()\n",
    "            ax[r,c].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].tick_params(axis='y', labelsize=8)\n",
    "            ax[r,c].set_title(wellname + ' ' + fmname + ' kh:' + str(kh_value) + ' dist:' + str(dist), fontsize=12) \n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                # ax[r,c].text(10, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            ax[r,c+1].plot(rhob, depth, color='red')\n",
    "            ax[r,c+1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "            ax[r,c+1].set_xlim(1.65, 2.65)\n",
    "            ax[r,c+1].grid(axis='y')\n",
    "            ax[r,c+1].grid(axis='x')\n",
    "            ax[r,c+1].invert_yaxis()\n",
    "            ax[r,c+1].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+1].set_xticks([])\n",
    "            ax[r,c+1].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+1].hlines( well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                ax[r,c+1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            twin1 = ax[r,c+1].twiny()\n",
    "            twin1.plot(npss, depth, color='blue')\n",
    "            twin1.set_xlim(0.6, 0)\n",
    "            twin1.set_xticks([])\n",
    "            ax[r,c+2].plot(phit, depth, color='green', linestyle='dashed')\n",
    "            ax[r,c+2].set_xlim(0.3, 0)\n",
    "            ax[r,c+2].grid(axis='x')\n",
    "            ax[r,c+2].grid(axis='y')\n",
    "            ax[r,c+2].invert_yaxis()\n",
    "            ax[r,c+2].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+2].set_xticks([])\n",
    "            ax[r,c+2].set_yticks([])\n",
    "            ax[r,c+2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+2].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "            twin2 = ax[r,c+2].twiny()\n",
    "            twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "            twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "            twin2.set_xlim(0, 1)\n",
    "            twin2.set_xticks([])\n",
    "            ax[r,c+3].plot(perm, depth, color='purple', alpha=0.66)\n",
    "            ax[r,c+3].set_xscale('log')\n",
    "            ax[r,c+3].set_xlim(0.1, 1000)\n",
    "            ax[r,c+3].grid(axis='y')\n",
    "            ax[r,c+3].grid(axis='x')\n",
    "            ax[r,c+3].invert_yaxis()\n",
    "            ax[r,c+3].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+3].set_xticks([])\n",
    "            ax[r,c+3].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.5)\n",
    "            twin4 = ax[r,c+3].twiny()\n",
    "            twin4.plot(kh, depth, color='black', alpha=1)\n",
    "            twin4.set_xticks([])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return fig.show()\n",
    "    def display_subplots():\n",
    "        try:\n",
    "            well_dist_dict = well_offset_selection(dataset_dist, fmname, well_target)\n",
    "            display_tracks(dataset_logs, well_dist_dict['target'], fmname,'TST', 10, 0,0,well_dist_dict['kh'], well_dist_dict['dist'])\n",
    "            display_tracks(dataset_logs, well_dist_dict['w1'], fmname,'TST', 10 ,0,4, well_dist_dict['kh1'], well_dist_dict['d1'])  \n",
    "            display_tracks(dataset_logs, well_dist_dict['w2'], fmname,'TST', 10,1,0, well_dist_dict['kh2'], well_dist_dict['d2'])      \n",
    "            display_tracks(dataset_logs, well_dist_dict['w3'], fmname,'TST', 10,1,4, well_dist_dict['kh3'], well_dist_dict['d3'])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "    well_target = well\n",
    "    fmname = formation\n",
    "    fig, ax = plt.subplots(2,8, figsize=(9,8), constrained_layout=True)\n",
    "    return display_subplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments wiht NTD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps & 3D view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing maps of well trajectories\n",
    "def well_traj_dataprep(dataset):\n",
    "    map_data = dataset.dropna()\n",
    "    map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-100:100]).reset_index()\n",
    "    map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    map_data_middle = map_data.groupby(['well','FORMATION_up'])[['X_mean', 'Y_mean', 'KHtst', 'TVD_SCS', 'Status']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = well_traj_dataprep(df_bal_net2_kh)\n",
    "\n",
    "bal8_1510 = pd.read_csv(r'C:\\jupyter\\SPP\\input\\surfaces\\petrel\\bal8_1510_base.csv', sep=' ', names=['X','Y','geobody'])\n",
    "def display_well_traj(trajectory, map_data_middle, petrel, fmname, mult, path, comment, print_flag):\n",
    "    trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "    map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "    map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "    traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                        mode='markers',\n",
    "                        marker=dict(color='black', size=1),\n",
    "                        customdata = trajectory[['well']],\n",
    "                        hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                        )\n",
    "    wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                        mode='markers',\n",
    "                        # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                        marker=dict(color=map_data_middle.KHtst, size=map_data_middle.KHtst*mult, colorscale='RdYlGn',  showscale=True,\n",
    "                                    line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                        customdata = map_data_middle[['well', 'KHtst']],\n",
    "                        hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "    geobody_map = go.Scatter(   x=petrel['X'], y=petrel['Y'],\n",
    "                                mode='markers',\n",
    "                                marker=dict(size=5, color=petrel['geobody'],colorscale='Viridis', opacity=0.5))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(geobody_map)\n",
    "    fig.add_trace(traj)\n",
    "    fig.add_trace(wells)\n",
    "    fig.update_layout(  title_text= ('Map of traj and well mean points with'+ ' ' + fmname + ' 1510 polygons. Size of bubbles is KHtst.'),\n",
    "                        autosize=True, width=1000, height=700, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    if print_flag == 'print':\n",
    "        go_offline.plot(fig, filename=path + comment, validate=True, auto_open=False)\n",
    "    else:\n",
    "        pass\n",
    "    return fig.show()\n",
    "display_well_traj(map_trajectory_display, map_data_middle, bal8_1510, 'Balakhany VIII', 0.00125, 'plots/', 'Balakhany8_KHtst', 'dont_print')\n",
    "# display_well_traj(map_trajectory_display, map_data_middle, 'Balakhany X', 0.003, 'plots/', 'Balakhany10_KHtst', 'dont_print')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Bal VIII / X from FORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_fu_bal8_10():\n",
    "    # KHtst calculation and join to the main dataframe df_bal_net2\n",
    "    df_kh = proph_calculation(df_bal_net2, 'NET_clp2')\n",
    "    df_bal_net2_kh_init = df_bal_net2.set_index(['well','MD']).join(df_kh.drop(['FORMATION_up', 'TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    # Cleaning KHtst dataset according to mentioned well lists\n",
    "    df_bal_net2_kh8 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean8_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany VIII')]\n",
    "    df_bal_net2_kh10 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean10_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany X')]\n",
    "    df_bal_net2_kh = pd.concat([df_bal_net2_kh8, df_bal_net2_kh10])\n",
    "    # Removing upper part of Bal8/10 and property calc\n",
    "    avg_prop_tst_kh_cut = prep_remove_balfu_calc_prop(df_bal_net2_kh)\n",
    "    # Calculation of well properties and applying blacklist with indecies of outliers\n",
    "    blacklist = [244, 240, 320, 324]\n",
    "    avg_prop_tst_kh_cut0 = prep_dist_calc(avg_prop_tst_kh_cut, 0, blacklist)\n",
    "    avg_prop_tst_kh_cut0 = avg_prop_tst_kh_cut0.convert_dtypes()\n",
    "    well_offsets_df = avg_prop_tst_kh_cut0.copy()\n",
    "    #Saving well propetries dataset to csv\n",
    "    save_tocsv(avg_prop_tst_kh_cut0, 'avg_prop_tst_kh_dist0_cut.csv', 0)\n",
    "    # Displaying features list of dataframe\n",
    "    avg_prop_tst_kh_cut0.columns\n",
    "    return avg_prop_tst_kh_cut0\n",
    "avg_prop_tst_kh_cut0 = removing_fu_bal8_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prop_tst_kh_cut0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_by_depth_fm(dataset_logs, formation_name, step):\n",
    "    def interpolate_by_depth(one_well, formation_name, step):\n",
    "        one_well = one_well.sort_values(by='TST')\n",
    "        well_name = one_well[\"well\"].iloc[0]\n",
    "        data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "        starting_tst = one_well[\"TST\"].iloc[0]\n",
    "        new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "        interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "        # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "        new_data = {\n",
    "            'well': [well_name for _ in range(len(new_TST_values))],\n",
    "            'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "            'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "            'TST': new_TST_values,\n",
    "            'X_traj': interp_X(new_TST_values),\n",
    "            'Y_traj': interp_Y(new_TST_values),\n",
    "            'PHIT': interp_PHIT(new_TST_values),\n",
    "            'TVD_SCS': interp_TVD(new_TST_values),\n",
    "            'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "            'LPERM': interp_LPERM(new_TST_values),\n",
    "            'KHtst': interp_KHtst(new_TST_values),\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        return new_df\n",
    "    df_lst = []\n",
    "    print(f'Start interpolation of {formation_name}')\n",
    "    for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "        well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "        well_interp = interpolate_by_depth(well_sel, formation_name, step)\n",
    "        df_lst.append(well_interp)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "well_bal8 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany VIII')]\n",
    "well_bal10 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany X')]\n",
    "well_bal8_interp = interpolate_by_depth_fm(well_bal8, 'Balakhany VIII', 0.1)\n",
    "well_bal10_interp = interpolate_by_depth_fm(well_bal10, 'Balakhany X', 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHIT_clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_bal8_interp_rn = well_bal8_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "well_bal10_interp_rn = well_bal10_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "\n",
    "def phit_rolling_averaging(input_dataset, samples_per_window):\n",
    "    df_lst = []\n",
    "    avg_report = []\n",
    "    fmname = input_dataset['FORMATION_up'].iloc[0] \n",
    "    print(f'Start rolling averaging of {fmname}')\n",
    "    for wellname in tqdm(input_dataset.well.unique()):\n",
    "        dataset = input_dataset[input_dataset.well == wellname]\n",
    "        window_size = int(len(dataset) / samples_per_window)\n",
    "        dataset['PHIT'] = dataset['PHIT_orig'].rolling(window=window_size, center=True).mean()\n",
    "        dataset =  dataset.dropna(subset=['PHIT'])\n",
    "        df_lst.append(dataset)\n",
    "        avg_report.append((wellname, len(dataset), window_size, samples_per_window))\n",
    "    result = pd.concat(df_lst)\n",
    "    avg_report_df = pd.DataFrame(avg_report, columns=['well','lenght_ds','window_size','samples_per_window'])\n",
    "    return result, avg_report_df\n",
    "samples_per_window = 100\n",
    "well_bal8_interp_phavg, avg_report_df8 = phit_rolling_averaging(well_bal8_interp_rn, samples_per_window)\n",
    "well_bal10_interp_phavg, avg_report_df10 = phit_rolling_averaging(well_bal10_interp_rn, samples_per_window)\n",
    "well_bal8_interp_phavg['PHIT_clp'] = well_bal8_interp_phavg['PHIT']\n",
    "well_bal10_interp_phavg['PHIT_clp'] = well_bal10_interp_phavg['PHIT']\n",
    "well_bal8_interp_phavg['LPERM_clp'] = well_bal8_interp_phavg['LPERM']\n",
    "well_bal10_interp_phavg['LPERM_clp'] = well_bal10_interp_phavg['LPERM']\n",
    "well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_block_lenght(dataset, block_lenght):\n",
    "    df_lst = []\n",
    "    fmname = dataset['FORMATION_up'].iloc[0]\n",
    "    print(f'Start processing of dataset for {fmname} with block lenght {block_lenght}')\n",
    "    for wellname in tqdm(dataset.well.unique()):\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        tst_index_repaired = [i for i in range(0, len(data))]\n",
    "        data['tst_index'] = tst_index_repaired\n",
    "        new_index = [i for i in range(0, len(data), block_lenght)]\n",
    "        data_cut = data[(data.tst_index < new_index[-1])]\n",
    "        df_lst.append(data_cut)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "block_lenght = 100\n",
    "well_bal8_interp_phavg_cut = cutting_block_lenght(well_bal8_interp_phavg, block_lenght)\n",
    "well_bal10_interp_phavg_cut = cutting_block_lenght(well_bal10_interp_phavg, block_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exercise_phit_avg_kh(dataset, wellname):\n",
    "    well_a01w = dataset[(dataset.well==wellname) & (dataset.FORMATION_up=='Balakhany VIII')]\n",
    "\n",
    "    well_a01w['PHIT_clipped'] = well_a01w['PHIT']\n",
    "    well_a01w.loc[well_a01w.NET_clp2 == 0, 'PHIT_clipped'] = 0\n",
    "    well_a01w['LPERM_avg'] = 0.00000002*(np.exp(well_a01w.PHIT*105.56))\n",
    "    well_a01w.loc[well_a01w['PHIT'] >= 0.2, 'LPERM_avg'] = (7.7925*((well_a01w.PHIT*100)**2))-(29881.0*well_a01w.PHIT)+2891.8\n",
    "    well_a01w.loc[well_a01w['PHIT'] < 0.16, 'LPERM_avg'] = 0.0159*(np.exp(well_a01w.PHIT*21.27))\n",
    "    well_a01w['khtst'] = well_a01w.LPERM_avg*0.1\n",
    "    well_a01w['KHtst_avg'] = well_a01w.loc[::-1, 'khtst'].cumsum()[::-1]\n",
    "\n",
    "    y = well_a01w.TST\n",
    "    phit_orig = well_a01w.PHIT_orig\n",
    "    phit_avg = well_a01w.PHIT\n",
    "    phit_cliped = well_a01w.PHIT_clipped\n",
    "    net = well_a01w.NET_clp2\n",
    "    perm = well_a01w.LPERM\n",
    "    perm_avg = well_a01w.LPERM_avg\n",
    "    kh = well_a01w.KHtst\n",
    "    kh_avg = well_a01w.KHtst_avg\n",
    "    print(  'KH orig:', kh.iloc[0].round(0), \n",
    "            'KH avg:',kh_avg.iloc[0].round(0), \n",
    "            'KHavg/KHorig:',((kh.iloc[0].round(0)-kh_avg.iloc[0].round(0))/kh.iloc[0].round(0)).round(2))\n",
    "\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(6, 7))\n",
    "    ax[0].plot(phit_orig, y, color='green')\n",
    "    ax[0].plot(phit_avg, y, color='red')\n",
    "    ax[0].set_xlim(0, 0.3)\n",
    "    ax[0].invert_yaxis()\n",
    "    ax[0].set_title(wellname)\n",
    "    ax[1].plot(phit_cliped, y, color='red', zorder=1)\n",
    "    ax[1].plot(net, y, color='orange', zorder=0)\n",
    "    ax[1].set_xlim(0, 0.3)\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[2].plot(perm, y, color='purple', lw=3)\n",
    "    ax[2].plot(perm_avg, y, color='yellow')\n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].set_xscale('log')\n",
    "    ax[3].plot(kh, y, color='black')\n",
    "    ax[3].plot(kh_avg, y, color='gray')\n",
    "    ax[3].invert_yaxis()\n",
    "    fig.show()\n",
    "exercise_phit_avg_kh(well_bal8_interp_phavg_cut, 'A01W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PHIT & GRcube - martix plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_letter_def(dataset):\n",
    "    wells_letter = [wellname[0] for wellname in dataset.well.unique()]\n",
    "    return set(wells_letter)\n",
    "well_letter_def(well_bal8_interp_phavg_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gr_cube_upload():\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    vsh_cube_log = pd.read_parquet(path + 'ACG_GRcube_VSH_v3.parquet.gzip')\n",
    "    vsh_cube_log = vsh_cube_log.replace(-9999.000, np.nan)\n",
    "    vsh_cube_log = vsh_cube_log.dropna()\n",
    "    vsh_cube_log.loc[vsh_cube_log.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "    vsh_cube_log.loc[vsh_cube_log.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "    vsh_cube_log = vsh_cube_log[vsh_cube_log.FORMATION_up.isin(['Balakhany VIII', 'Balakhany X'])]\n",
    "    vsh_grcube = vsh_cube_log[['wellName', 'DEPT','VSH_GRcube', 'FORMATION_up']]\n",
    "    vsh_grcube = vsh_grcube.rename(columns={'wellName':'well', 'DEPT':'MD'})\n",
    "    return vsh_grcube\n",
    "vsh_grcube = gr_cube_upload()\n",
    "df_bal_net2_kh['MD'] = df_bal_net2_kh.MD.round(1)\n",
    "df_bal_net2_kh_cube = df_bal_net2_kh.set_index(['well','MD', 'FORMATION_up']).join(vsh_grcube.set_index(['well','MD', 'FORMATION_up'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsh_gr_cube_recalc(dataset):\n",
    "    def interpolate_by_depth_fm(dataset_logs, formation_name, step):\n",
    "        def interpolate_by_depth(one_well, formation_name, step):\n",
    "            one_well = one_well.sort_values(by='TST')\n",
    "            well_name = one_well[\"well\"].iloc[0]\n",
    "            data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "            starting_tst = one_well[\"TST\"].iloc[0]\n",
    "            new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "            interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "            interp_VSH_GRcube = interp1d(one_well['TST'], one_well['VSH_GRcube'], kind='linear', fill_value=\"extrapolate\")\n",
    "            # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "            new_data = {\n",
    "                'well': [well_name for _ in range(len(new_TST_values))],\n",
    "                'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "                'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "                'TST': new_TST_values,\n",
    "                'X_traj': interp_X(new_TST_values),\n",
    "                'Y_traj': interp_Y(new_TST_values),\n",
    "                'PHIT': interp_PHIT(new_TST_values),\n",
    "                'TVD_SCS': interp_TVD(new_TST_values),\n",
    "                'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "                'LPERM': interp_LPERM(new_TST_values),\n",
    "                'KHtst': interp_KHtst(new_TST_values),\n",
    "                'VSH_GRcube':interp_VSH_GRcube(new_TST_values)\n",
    "            }\n",
    "            new_df = pd.DataFrame(new_data)\n",
    "            return new_df\n",
    "        df_lst = []\n",
    "        print(f'Start interpolation of {formation_name}')\n",
    "        for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "            well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "            well_interp = interpolate_by_depth(well_sel, formation_name, step)\n",
    "            df_lst.append(well_interp)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    well_bal8 = dataset[(dataset.FORMATION_up == 'Balakhany VIII')]\n",
    "    well_bal10 = dataset[(dataset.FORMATION_up == 'Balakhany X')]\n",
    "    well_bal8_interp = interpolate_by_depth_fm(well_bal8, 'Balakhany VIII', 0.1)\n",
    "    well_bal10_interp = interpolate_by_depth_fm(well_bal10, 'Balakhany X', 0.1)\n",
    "    well_bal8_interp_rn = well_bal8_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "    well_bal10_interp_rn = well_bal10_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "\n",
    "    def phit_rolling_averaging(input_dataset, samples_per_window):\n",
    "        df_lst = []\n",
    "        avg_report = []\n",
    "        fmname = input_dataset['FORMATION_up'].iloc[0] \n",
    "        print(f'Start rolling averaging of {fmname}')\n",
    "        for wellname in tqdm(input_dataset.well.unique()):\n",
    "            dataset = input_dataset[input_dataset.well == wellname]\n",
    "            window_size = int(len(dataset) / samples_per_window)\n",
    "            dataset['PHIT'] = dataset['PHIT_orig'].rolling(window=window_size, center=True).mean()\n",
    "            dataset =  dataset.dropna(subset=['PHIT'])\n",
    "            df_lst.append(dataset)\n",
    "            avg_report.append((wellname, len(dataset), window_size, samples_per_window))\n",
    "        result = pd.concat(df_lst)\n",
    "        avg_report_df = pd.DataFrame(avg_report, columns=['well','lenght_ds','window_size','samples_per_window'])\n",
    "        return result, avg_report_df\n",
    "    samples_per_window = 100\n",
    "    well_bal8_interp_phavg, avg_report_df8 = phit_rolling_averaging(well_bal8_interp_rn, samples_per_window)\n",
    "    well_bal10_interp_phavg, avg_report_df10 = phit_rolling_averaging(well_bal10_interp_rn, samples_per_window)\n",
    "    well_bal8_interp_phavg['PHIT_clp'] = well_bal8_interp_phavg['PHIT']\n",
    "    well_bal10_interp_phavg['PHIT_clp'] = well_bal10_interp_phavg['PHIT']\n",
    "    well_bal8_interp_phavg['LPERM_clp'] = well_bal8_interp_phavg['LPERM']\n",
    "    well_bal10_interp_phavg['LPERM_clp'] = well_bal10_interp_phavg['LPERM']\n",
    "    well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "    well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "    well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "    well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "    return well_bal8_interp_phavg, well_bal10_interp_phavg\n",
    "well_bal8_interp_phavg, well_bal10_interp_phavg = vsh_gr_cube_recalc(df_bal_net2_kh_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_plots_phit_vsh_matrix(dataset, platform, variable, flag, max_var, comment):\n",
    "    \"\"\"\n",
    "    flag = 'phit' or 'perm'\n",
    "    \"\"\"\n",
    "    rows = 4\n",
    "    columns = 9\n",
    "    wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(platform)]\n",
    "    fig, ax = plt.subplots(rows,columns, figsize=(16,rows*3))\n",
    "    counter = 0\n",
    "    y_real_list = []\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                data = dataset[dataset.well==wells_letter[counter]]\n",
    "                y_real_list.append(len(data))\n",
    "                counter +=1\n",
    "    max_ind = max(y_real_list)\n",
    "    counter = 0\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                well_data = dataset[dataset.well==wells_letter[counter]]\n",
    "                ind = well_data[variable]\n",
    "                y_real = [k for k in range(len(ind))]\n",
    "                y_desired = [k for k in range(max_ind)]\n",
    "                y_diff = len(y_desired) - len(y_real)\n",
    "                values_to_add = [0.12 for k in range(y_diff)]\n",
    "                x = well_data[variable]\n",
    "                x_gr = well_data['VSH_GRcube']\n",
    "                x_new = pd.concat([x, pd.Series(values_to_add)])\n",
    "                x_gr_new = pd.concat([x_gr, pd.Series(values_to_add)])          \n",
    "                if flag == 'phit':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='green', lw=1.5, alpha=1, zorder=1)\n",
    "                    ax[j,i].set_xlim(0.1, 0.35)\n",
    "                    # twin = ax[j,i].twiny()\n",
    "                    # twin.plot(x_gr_new, y_desired, color='green', lw=2, alpha=0.5, zorder=0)\n",
    "                    # twin.set_xlim(0, 1)\n",
    "                if flag == 'perm':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='purple', lw=2, alpha=0.75)\n",
    "                    ax[j,i].set_xscale('log')\n",
    "                    ax[j,i].set_xlim(0.1, max_var)\n",
    "                ax[j,i].set_title(wells_letter[counter] + comment)\n",
    "                ax[j,i].invert_yaxis()\n",
    "                ax[j,i].grid()\n",
    "                counter +=1\n",
    "\n",
    "    return plt.tight_layout()\n",
    "# for letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']:\n",
    "for letter in ['A']:\n",
    "    well_plots_phit_vsh_matrix(well_bal8_interp_phavg, letter, 'PHIT_clp', 'phit', 0.35, ' bal8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_data_calculation(dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "    def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "        row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "        row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "        return row_change\n",
    "    df_list = []\n",
    "    for wellname in dataset_ntd.well.unique():\n",
    "        df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "        df_list.append(df)\n",
    "    res_df_list = pd.concat(df_list)\n",
    "    result = pd.concat([dataset_ntd, res_df_list])\n",
    "    result = result.sort_values(by=['well','top_tst'])\n",
    "    result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "    return result_final\n",
    "ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with block3\n",
    "def blocks_for_clustering(df, block_size):\n",
    "    def rows_selection(dataframe, block_size):\n",
    "        selected_rows = dataframe.iloc[:block_size]   \n",
    "        phit = []\n",
    "        htst = []\n",
    "        not_htst = []\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            phit.append(rows['phit_avg'])\n",
    "            htst.append(rows['htst'])\n",
    "            not_htst.append(rows['not_htst'])\n",
    "        def add_zero_values(var, block_size):\n",
    "            if len(var) < block_size:\n",
    "                zero_add = [np.nan for i in range(0, (block_size-len(var)))]\n",
    "                res_var = np.concatenate([var,np.array(zero_add)], axis=0)\n",
    "            else:\n",
    "                res_var = var\n",
    "            return res_var\n",
    "        phit = add_zero_values(phit, block_size)\n",
    "        htst = add_zero_values(htst, block_size)\n",
    "        not_htst = add_zero_values(not_htst, block_size)\n",
    "        res = np.concatenate([phit, htst, not_htst], axis=0) \n",
    "        res = res.reshape(1, block_size*3)\n",
    "        return res\n",
    "    def columns_names(name):\n",
    "        columns_names = []\n",
    "        for i in range(block_size):\n",
    "            columns_names.append(name + '_' + str(i+1))\n",
    "        return columns_names\n",
    "    wellname = df['well'][0]\n",
    "    fmname = df['FORMATION_up'][0]\n",
    "\n",
    "    blocks = [df.iloc[i:i + block_size] for i in range(0, len(df), block_size)]\n",
    "    phit_names = columns_names('phit')\n",
    "    htst_names = columns_names('htst')\n",
    "    not_htst_names = columns_names('not_htst')\n",
    "    columns_names_list = phit_names + htst_names + not_htst_names\n",
    "\n",
    "    res_lists = []\n",
    "    for i in range(len(blocks)):\n",
    "        res = rows_selection(blocks[i], block_size)\n",
    "        res_lists.append(res)\n",
    "    res = np.concatenate(res_lists)\n",
    "    df_res = pd.DataFrame(res, columns = columns_names_list)\n",
    "    df_res['well'] = wellname\n",
    "    df_res['FORMATION_up'] = fmname\n",
    "    return df_res\n",
    "ntd_val_final8 = ntd_val_final8_clean\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    df8 = ntd_val_final8[(ntd_val_final8.well==wellname)]\n",
    "    res_df8 = blocks_for_clustering(df8, 3)\n",
    "    df_lst.append(res_df8)\n",
    "ntd_val_cluster = pd.concat(df_lst)\n",
    "# ntd_val_cluster.to_csv('.\\output\\\\ntd_val_cluster2.csv', index=False)\n",
    "ntd_val_cluster = ntd_val_cluster.reset_index().drop('index', axis=1)\n",
    "ntd_val_cluster.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with stride\n",
    "def stride_for_clustering(df, block_size):\n",
    "    def rows_selection(dataframe, block_size):\n",
    "        selected_rows = dataframe.iloc[:block_size]   \n",
    "        phit = []\n",
    "        htst = []\n",
    "        not_htst = []\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            phit.append(rows['phit_avg'])\n",
    "            htst.append(rows['htst'])\n",
    "            not_htst.append(rows['not_htst'])\n",
    "        def add_zero_values(var, block_size):\n",
    "            if len(var) < block_size:\n",
    "                zero_add = [np.nan for i in range(0, (block_size-len(var)))]\n",
    "                res_var = np.concatenate([var,np.array(zero_add)], axis=0)\n",
    "            else:\n",
    "                res_var = var\n",
    "            return res_var\n",
    "        phit = add_zero_values(phit, block_size)\n",
    "        htst = add_zero_values(htst, block_size)\n",
    "        not_htst = add_zero_values(not_htst, block_size)\n",
    "        res = np.concatenate([phit, htst, not_htst], axis=0) \n",
    "        res = res.reshape(1, block_size*3)\n",
    "        return res\n",
    "    def columns_names(name):\n",
    "        columns_names = []\n",
    "        for i in range(block_size):\n",
    "            columns_names.append(name + '_' + str(i+1))\n",
    "        return columns_names\n",
    "    wellname = df['well'][0]\n",
    "    fmname = df['FORMATION_up'][0]\n",
    "\n",
    "    blocks = [df.iloc[i:i + block_size] for i in range(0, len(df), 1)]\n",
    "    phit_names = columns_names('phit')\n",
    "    htst_names = columns_names('htst')\n",
    "    not_htst_names = columns_names('not_htst')\n",
    "    columns_names_list = phit_names + htst_names + not_htst_names\n",
    "\n",
    "    res_lists = []\n",
    "    for i in range(len(blocks)):\n",
    "        res = rows_selection(blocks[i], block_size)\n",
    "        res_lists.append(res)\n",
    "    res = np.concatenate(res_lists)\n",
    "    df_res = pd.DataFrame(res, columns = columns_names_list)\n",
    "    df_res['well'] = wellname\n",
    "    df_res['FORMATION_up'] = fmname\n",
    "    return df_res\n",
    "ntd_val_final8 = ntd_val_final8_clean\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    df8 = ntd_val_final8[(ntd_val_final8.well==wellname)]\n",
    "    res_df8 = stride_for_clustering(df8, 3)\n",
    "    df_lst.append(res_df8)\n",
    "ntd_val_cluster_stride = pd.concat(df_lst)\n",
    "# ntd_val_cluster_stride.to_csv('.\\output\\\\ntd_val_cluster_stride.csv', index=False)\n",
    "ntd_val_cluster_stride = ntd_val_cluster_stride.reset_index().drop('index', axis=1)\n",
    "ntd_val_cluster_stride.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with whole well\n",
    "def stride_for_clustering_wholewell(df, block_size):\n",
    "    block_size = max_blocks_determination(ntd_val_final8)\n",
    "    def rows_selection(dataframe, block_size):\n",
    "        selected_rows = dataframe.iloc[:block_size]   \n",
    "        phit = []\n",
    "        htst = []\n",
    "        not_htst = []\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            phit.append(rows['phit_avg'])\n",
    "            htst.append(rows['htst'])\n",
    "            not_htst.append(rows['not_htst'])\n",
    "        def add_zero_values(var, block_size):\n",
    "            if len(var) < block_size:\n",
    "                zero_add = [np.nan for i in range(0, (block_size-len(var)))]\n",
    "                res_var = np.concatenate([var,np.array(zero_add)], axis=0)\n",
    "            else:\n",
    "                res_var = var\n",
    "            return res_var\n",
    "        phit = add_zero_values(phit, block_size)\n",
    "        htst = add_zero_values(htst, block_size)\n",
    "        not_htst = add_zero_values(not_htst, block_size)\n",
    "        res = np.concatenate([phit, htst, not_htst], axis=0) \n",
    "        res = res.reshape(1, block_size*3)\n",
    "        return res\n",
    "    def columns_names(name):\n",
    "        columns_names = []\n",
    "        for i in range(block_size):\n",
    "            columns_names.append(name + '_' + str(i+1))\n",
    "        return columns_names\n",
    "    wellname = df['well'][0]\n",
    "    fmname = df['FORMATION_up'][0]\n",
    "\n",
    "    blocks = [df.iloc[i:i + block_size] for i in range(0, len(df), 1)]\n",
    "    phit_names = columns_names('phit')\n",
    "    htst_names = columns_names('htst')\n",
    "    not_htst_names = columns_names('not_htst')\n",
    "    columns_names_list = phit_names + htst_names + not_htst_names\n",
    "\n",
    "    res_lists = []\n",
    "    for i in range(len(blocks)):\n",
    "        res = rows_selection(blocks[i], block_size)\n",
    "        res_lists.append(res)\n",
    "    res = np.concatenate(res_lists)\n",
    "    df_res = pd.DataFrame(res, columns = columns_names_list)\n",
    "    df_res['well'] = wellname\n",
    "    df_res['FORMATION_up'] = fmname\n",
    "    return df_res\n",
    "def max_blocks_determination(dataset):\n",
    "    data_lst = []\n",
    "    for wellname in  dataset.well.unique():\n",
    "        layers_total = dataset[dataset.well == wellname].shape[0]\n",
    "        data_lst.append(layers_total)\n",
    "    return max(data_lst)\n",
    "def clustering_wholewell_reduce(dataset):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique():\n",
    "        data = dataset[dataset.well == wellname].iloc[:1]\n",
    "        df_lst.append(data)\n",
    "    result = pd.concat(df_lst).reset_index()\n",
    "    return result\n",
    "\n",
    "ntd_val_final8 = ntd_val_final8_clean\n",
    "block_size = max_blocks_determination(ntd_val_final8)\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    df8 = ntd_val_final8[(ntd_val_final8.well==wellname)]\n",
    "    res_df8 = stride_for_clustering_wholewell(df8, block_size)\n",
    "    df_lst.append(res_df8)\n",
    "ntd_stride_wholewell = pd.concat(df_lst).fillna(0)\n",
    "ntd_stride_wholewell_cut = clustering_wholewell_reduce(ntd_stride_wholewell)\n",
    "# ntd_stride_wholewell_cut.to_csv('.\\output\\\\ntd_stride_wholewell.csv', index=False)\n",
    "ntd_stride_wholewell_cut.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with stride\n",
    "def stride_for_clustering(df, block_size):\n",
    "    def rows_selection(dataframe, block_size):\n",
    "        selected_rows = dataframe.iloc[:block_size]   \n",
    "        phit = []\n",
    "        htst = []\n",
    "        not_htst = []\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            phit.append(rows['phit_avg'])\n",
    "            htst.append(rows['htst'])\n",
    "            not_htst.append(rows['not_htst'])\n",
    "        def add_zero_values(var, block_size):\n",
    "            if len(var) < block_size:\n",
    "                zero_add = [np.nan for i in range(0, (block_size-len(var)))]\n",
    "                res_var = np.concatenate([var,np.array(zero_add)], axis=0)\n",
    "            else:\n",
    "                res_var = var\n",
    "            return res_var\n",
    "        phit = add_zero_values(phit, block_size)\n",
    "        htst = add_zero_values(htst, block_size)\n",
    "        not_htst = add_zero_values(not_htst, block_size)\n",
    "        res = np.concatenate([phit, htst, not_htst], axis=0) \n",
    "        res = res.reshape(1, block_size*3)\n",
    "        return res\n",
    "    def columns_names(name):\n",
    "        columns_names = []\n",
    "        for i in range(block_size):\n",
    "            columns_names.append(name + '_' + str(i+1))\n",
    "        return columns_names\n",
    "    wellname = df['well'][0]\n",
    "    fmname = df['FORMATION_up'][0]\n",
    "\n",
    "    blocks = [df.iloc[i:i + block_size] for i in range(0, len(df), 1)]\n",
    "    phit_names = columns_names('phit')\n",
    "    htst_names = columns_names('htst')\n",
    "    not_htst_names = columns_names('not_htst')\n",
    "    columns_names_list = phit_names + htst_names + not_htst_names\n",
    "\n",
    "    res_lists = []\n",
    "    for i in range(len(blocks)):\n",
    "        res = rows_selection(blocks[i], block_size)\n",
    "        res_lists.append(res)\n",
    "    res = np.concatenate(res_lists)\n",
    "    df_res = pd.DataFrame(res, columns = columns_names_list)\n",
    "    df_res['well'] = wellname\n",
    "    df_res['FORMATION_up'] = fmname\n",
    "    return df_res\n",
    "\n",
    "ntd_val_final8 = ntd_val_final8_clean\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    df8 = ntd_val_final8[(ntd_val_final8.well==wellname)]\n",
    "    res_df8 = stride_for_clustering(df8, 1)\n",
    "    df_lst.append(res_df8)\n",
    "ntd_val_cluster_stride = pd.concat(df_lst)\n",
    "# ntd_val_cluster_stride.to_csv('.\\output\\\\ntd_val_cluster_stride.csv', index=False)\n",
    "ntd_val_cluster_stride = ntd_val_cluster_stride.reset_index().drop('index', axis=1)\n",
    "ntd_val_cluster_stride.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with top-phi-bot\n",
    "def top_phit_bot_clustering(dataset, wellname):\n",
    "    data = dataset[dataset.well == wellname]\n",
    "    data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "    data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "    data['bot_htst'] = data['not_htst']\n",
    "    data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'top_htst','htst','bot_htst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "    return data\n",
    "\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    res_df8 = top_phit_bot_clustering(ntd_val_final8_clean, wellname)\n",
    "    df_lst.append(res_df8)\n",
    "top_phi_bot_cluster = pd.concat(df_lst).reset_index().drop('index', axis=1)\n",
    "# top_phi_bot_cluster.to_csv('.\\output\\\\top_phi_bot_cluster.csv', index=False)\n",
    "top_phi_bot_cluster.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering with top-phi-bot - tough algorithm\n",
    "def top_phit_bot_clustering(df, block_size):\n",
    "    def rows_selection(dataframe, block_size):\n",
    "        selected_rows = dataframe.iloc[:block_size]   \n",
    "        phit = []\n",
    "        top_htst = []\n",
    "        htst = []\n",
    "        bot_htst = []\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            phit.append(rows['phit_avg'])\n",
    "            htst.append(rows['htst'])\n",
    "            bot_htst.append(rows['not_htst'])\n",
    "        top_htst.append(dataframe['top_tst'].iloc[0] - dataframe['fm_top_tst'].iloc[0])\n",
    "        for ind, rows in selected_rows.iterrows():\n",
    "            top_htst.append(rows['not_htst'])\n",
    "        top_htst = top_htst[:-1]\n",
    "        def add_zero_values(var, block_size):\n",
    "            if len(var) < block_size:\n",
    "                zero_add = [np.nan for i in range(0, (block_size-len(var)))]\n",
    "                res_var = np.concatenate([var,np.array(zero_add)], axis=0)\n",
    "            else:\n",
    "                res_var = var\n",
    "            return res_var\n",
    "        phit = add_zero_values(phit, block_size)\n",
    "        htst = add_zero_values(htst, block_size)\n",
    "        bot_htst = add_zero_values(bot_htst, block_size)\n",
    "        top_htst = add_zero_values(top_htst, block_size)\n",
    "        res = np.concatenate([phit, top_htst, htst, bot_htst], axis=0)\n",
    "        res = res.reshape(1, -1) \n",
    "        return res\n",
    "    def columns_names(name):\n",
    "        columns_names = []\n",
    "        for i in range(block_size):\n",
    "            columns_names.append(name + '_' + str(i+1))\n",
    "        return columns_names\n",
    "    wellname = df['well'][0]\n",
    "    fmname = df['FORMATION_up'][0]\n",
    "\n",
    "    blocks = [df.iloc[i:i + block_size] for i in range(0, len(df), 1)]\n",
    "    phit_names = columns_names('phit')\n",
    "    htst_names = columns_names('htst')\n",
    "    top_htst_names = columns_names('top_htst')\n",
    "    bot_htst_names = columns_names('bot_htst')\n",
    "    columns_names_list = phit_names + top_htst_names + htst_names + bot_htst_names\n",
    "\n",
    "    res_lists = []\n",
    "    for i in range(len(blocks)):\n",
    "        res = rows_selection(blocks[i], block_size)\n",
    "        res_lists.append(res)\n",
    "    res = np.concatenate(res_lists)\n",
    "    df_res = pd.DataFrame(res, columns = columns_names_list)\n",
    "    df_res['well'] = wellname\n",
    "    df_res['FORMATION_up'] = fmname\n",
    "    return df_res\n",
    "\n",
    "ntd_val_final8 = ntd_val_final8_clean\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    df8 = ntd_val_final8[(ntd_val_final8.well==wellname)]\n",
    "    res_df8 = top_phit_bot_clustering(df8, 1)\n",
    "    df_lst.append(res_df8)\n",
    "top_phit_bot_cluster = pd.concat(df_lst)\n",
    "top_phit_bot_cluster.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering stride - resulting data analize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading clustering data stride from Farid\n",
    "ntd_val_clustering_stride_many_labels = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering_with_stride_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "ntd_val_clustering_stride_many_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntd_val_clust_stride = ntd_val_clustering_stride_many_labels[[  'phit_1', 'phit_2', 'phit_3', 'htst_1', 'htst_2', 'htst_3',\n",
    "                                                                'not_htst_1', 'not_htst_2', 'not_htst_3', 'well', 'FORMATION_up',\n",
    "                                                                'kmeans_3_labels']]\n",
    "# ntd_val_clust_stride[ntd_val_clust_stride.well=='A01Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntd_val_clust_stride.well.unique()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ntd_val_clust_stride\n",
    "wellname = 'A02Z'\n",
    "\n",
    "fig, ax = plt.subplots(1,9, figsize=(16,2.5))\n",
    "for j in range(9):\n",
    "    try:\n",
    "        data = dataset[dataset.well == wellname].iloc[j]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        cluster_lst = []\n",
    "        for i in range(3):\n",
    "            phit_lst.append(data[i])\n",
    "            phit_lst.append(0)\n",
    "        for i in range(3):\n",
    "            htst_lst.append(data[i+3])\n",
    "            htst_lst.append(data[i+6])\n",
    "        for i in range(6):\n",
    "            cluster_lst.append(data[-1])\n",
    "        htst_lst, phit_lst, cluster_lst\n",
    "        well_collect_cluster = pd.DataFrame(zip(phit_lst, htst_lst, cluster_lst), columns=['phit', 'htst','cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_top = pd.DataFrame({'phit':[0], 'htst':[0], 'cluster':well_collect_cluster['cluster'].iloc[0],'depth':[0]})\n",
    "        wellplot = pd.concat([df_top, well_collect_cluster]).reset_index().drop('index', axis=1)\n",
    "        # wellplot = well_collect_cluster\n",
    "        custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "        ax[j].plot(wellplot['phit'], wellplot['depth'], \n",
    "                drawstyle='steps-post', color=custom_palette[wellplot['cluster'].iloc[0]], alpha=1, lw=2)\n",
    "        ax[j].set_xlim(0, 0.35)\n",
    "        ax[j].invert_yaxis()\n",
    "        ax[j].set_title(wellname)\n",
    "        ax[j].tick_params(axis='both', which='major', labelsize=8)\n",
    "        ax[j].grid()\n",
    "        ax[j].tight_layout()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering 3 - resulting data analize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading clustering data block3 from Farid\n",
    "ntd_val_clustering2_with_labels = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering2_with_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "ntd_val_clustering2_with_labels.iloc[:,:11].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_clustering_display(dataset, columns, rows, clustering, colors_dict, block, output_flag):\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(17,10))\n",
    "    counter = (block-1)*rows*columns\n",
    "    # print('counter init ', counter)\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < block*rows*columns+1:\n",
    "                try:\n",
    "                    data = dataset.iloc[counter]\n",
    "                    title = dataset.iloc[counter]['well']\n",
    "                    colors = dataset.iloc[counter][clustering]\n",
    "                    y = [0, data.htst_1, data.not_htst_1, data.htst_2, data.not_htst_2, data.htst_3, data.not_htst_3]\n",
    "                    x = [0, data.phit_1, 0, data.phit_2, 0, data.phit_3, 0]\n",
    "                    # colors_dict = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "                    depth_summ_list = []\n",
    "                    depth_counter = 0\n",
    "                    for k in range(len(y)):\n",
    "                        depth_counter += y[k]\n",
    "                        depth_summ_list.append(depth_counter) \n",
    "                    ax[j,i].step(x, depth_summ_list, label=title, where='post', color=colors_dict[colors], linestyle='-', linewidth=2)\n",
    "                    ax[j,i].set_ylim(-5,100)\n",
    "                    ax[j,i].set_xlim(0,0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].legend(fontsize=8)\n",
    "                    ax[j,i].grid()\n",
    "                    counter +=1\n",
    "                except:\n",
    "                    pass\n",
    "    plt.tight_layout()\n",
    "    if output_flag == 'print':\n",
    "        plt.savefig('.\\plots\\\\clustering\\\\' + clustering + '_' + str(block) +'.png')\n",
    "    else:\n",
    "        pass\n",
    "    # print('last value ', counter, block)\n",
    "\n",
    "colors_dict_kmeans = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "colors_dict_dbscan = {-1: 'red', 0: 'green', 1: 'blue'}\n",
    "colors_dict_aggl = {0: 'red', 1: 'green', 2: 'blue'}\n",
    "colors_dict_gmm_labels = {1: 'red', 2: 'green', 0: 'blue'}\n",
    "clustering = 'dbscan_labels'\n",
    "colors_dict = colors_dict_dbscan\n",
    "\n",
    "for i in range(1,len(ntd_val_clustering2_with_labels)//(9*4)+2):\n",
    "    block_clustering_display(ntd_val_clustering2_with_labels, 9, 4, clustering, colors_dict, i, 'dont print')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htst_sum_clustering(dataset, clustering_algorithm, cluster):\n",
    "    cluster_num = dataset[dataset[clustering_algorithm]==cluster]\n",
    "    cluster_num_gb = cluster_num.groupby(['well','FORMATION_up'])[['htst_1','htst_2','htst_3']].sum().reset_index()\n",
    "    htst_sum_cluster_name = 'htst_sum_cluster_' + str(cluster)\n",
    "    cluster_num_gb[htst_sum_cluster_name] = cluster_num_gb['htst_1'] + cluster_num_gb['htst_2'] + cluster_num_gb['htst_3']\n",
    "    cluster_num = 'cluster_' + str(cluster)\n",
    "    cluster_num_gb[cluster_num] = cluster\n",
    "    return cluster_num_gb[['well','FORMATION_up',htst_sum_cluster_name,cluster_num]]\n",
    "\n",
    "xy_bal8_init = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up=='Balakhany VIII'][['well','FORMATION_up','X_mean','Y_mean']]\n",
    "xy_bal8 = xy_bal8_init.groupby(['well','FORMATION_up']).apply(lambda x: \n",
    "                                                            x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "kmeans2 = htst_sum_clustering(ntd_val_clustering2_with_labels, 'kmeans_labels', 2)\n",
    "kmeans1 = htst_sum_clustering(ntd_val_clustering2_with_labels, 'kmeans_labels', 1)\n",
    "kmeans0 = htst_sum_clustering(ntd_val_clustering2_with_labels, 'kmeans_labels', 0)\n",
    "kmeans = pd.merge(pd.merge( kmeans2, kmeans1, how='outer', on=['well','FORMATION_up']), \n",
    "                            kmeans0, how='outer', on=['well','FORMATION_up']) \n",
    "kmeans_xy = kmeans.set_index(['well','FORMATION_up']).join(xy_bal8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "kmeans_xy['htst_sum_cluster_2'].fillna(0, inplace=True)\n",
    "kmeans_xy['htst_sum_cluster_1'].fillna(0, inplace=True)\n",
    "kmeans_xy['htst_sum_cluster_0'].fillna(0, inplace=True)\n",
    "kmeans_xy['X_mean'] = kmeans_xy['X_mean'].astype('int')/1000\n",
    "kmeans_xy['Y_mean'] = kmeans_xy['Y_mean'].astype('int')/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart_map3():\n",
    "       fig, ax = plt.subplots(figsize=(13,13))\n",
    "       for ind, row in kmeans_xy[:].iterrows():\n",
    "              ax.pie([row['htst_sum_cluster_2'], row['htst_sum_cluster_1'], row['htst_sum_cluster_0']], \n",
    "                     radius=0.3, center=(row['X_mean'], row['Y_mean']), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.75},\n",
    "                     colors=['red', 'green', 'blue'], frame=True)\n",
    "              plt.grid()\n",
    "pie_chart_map3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pie_chart_map3_pale(color_cluster):\n",
    "       cluster_count = kmeans_xy.groupby('well')[['cluster_2','cluster_1','cluster_0']].count()\n",
    "       cluster_count['cluster_sum'] = cluster_count.sum(axis=1)\n",
    "       cluster_count = cluster_count.reset_index()\n",
    "       cluster_count_1cluster = cluster_count[cluster_count.cluster_sum ==1]['well'].unique()\n",
    "       cluster_count_2cluster = cluster_count[cluster_count.cluster_sum ==2]['well'].unique()\n",
    "       cluster_count_3cluster = cluster_count[cluster_count.cluster_sum ==3]['well'].unique()\n",
    "       kmeans_xy_1cluster = kmeans_xy[kmeans_xy.well.isin(cluster_count_1cluster)]\n",
    "       kmeans_xy_2cluster = kmeans_xy[kmeans_xy.well.isin(cluster_count_2cluster)]\n",
    "       kmeans_xy_3cluster = kmeans_xy[kmeans_xy.well.isin(cluster_count_3cluster)]\n",
    "\n",
    "       fig, ax = plt.subplots(figsize=(13,13))\n",
    "       for ind, row in kmeans_xy_2cluster.iterrows():\n",
    "              ax.pie([row['htst_sum_cluster_2'], row['htst_sum_cluster_1'], row['htst_sum_cluster_0']], \n",
    "                     radius=0.3, center=(row['X_mean'], row['Y_mean']), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.75},\n",
    "                     colors=['red', 'green', 'blue'], frame=True)\n",
    "       for ind, row in kmeans_xy_3cluster.iterrows():\n",
    "              ax.pie([row['htst_sum_cluster_2'], row['htst_sum_cluster_1'], row['htst_sum_cluster_0']], \n",
    "                     radius=0.3, center=(row['X_mean'], row['Y_mean']), wedgeprops={\"linewidth\": 1.5, \"edgecolor\": \"black\", \"alpha\":0.75},\n",
    "                     colors=['red', 'green', 'blue'], frame=True)\n",
    "       if color_cluster == 'blue':\n",
    "              color_list = ['gray', 'gray', 'blue']\n",
    "       if color_cluster == 'green':\n",
    "              color_list = ['gray', 'green', 'gray']\n",
    "       if color_cluster == 'red':\n",
    "              color_list = ['red', 'gray', 'gray']\n",
    "       for ind, row in kmeans_xy_1cluster.iterrows():\n",
    "              ax.pie([row['htst_sum_cluster_2'], row['htst_sum_cluster_1'], row['htst_sum_cluster_0']], \n",
    "                     radius=0.3, center=(row['X_mean'], row['Y_mean']), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.25},\n",
    "                     colors=color_list, frame=True)\n",
    "       plt.grid()\n",
    "pie_chart_map3_pale('blue')\n",
    "pie_chart_map3_pale('green')\n",
    "pie_chart_map3_pale('red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_phit_histo(datatset, cluster_num):\n",
    "    kmeans = datatset[datatset.kmeans_labels == cluster_num]\n",
    "    kmeans_phit = pd.concat([kmeans.phit_1, kmeans.phit_3, kmeans.phit_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_phit['phit'] = kmeans_phit[0]\n",
    "    kmeans_phit['cluster']=cluster_num\n",
    "    kmeans_phit = kmeans_phit.drop(0, axis=1)\n",
    "    return kmeans_phit\n",
    "kmeans0_phit = cluster_phit_histo(ntd_val_clustering2_with_labels, 0)\n",
    "kmeans1_phit = cluster_phit_histo(ntd_val_clustering2_with_labels, 1)\n",
    "kmeans2_phit = cluster_phit_histo(ntd_val_clustering2_with_labels, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_phit = pd.concat([kmeans2_phit, kmeans1_phit, kmeans0_phit])\n",
    "sns.histplot(data=kmeans_phit, x='phit', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_htst_histo(datatset, cluster_num):\n",
    "    kmeans = datatset[datatset.kmeans_labels == cluster_num]\n",
    "    kmeans_htst = pd.concat([kmeans.htst_1, kmeans.htst_2, kmeans.htst_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_htst['htst'] = kmeans_htst[0]\n",
    "    kmeans_htst['cluster']=cluster_num\n",
    "    kmeans_htst = kmeans_htst.drop(0, axis=1)\n",
    "    return kmeans_htst\n",
    "kmeans0_htst = cluster_htst_histo(ntd_val_clustering2_with_labels, 0)\n",
    "kmeans1_htst = cluster_htst_histo(ntd_val_clustering2_with_labels, 1)\n",
    "kmeans2_htst = cluster_htst_histo(ntd_val_clustering2_with_labels, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_htst = pd.concat([kmeans2_htst, kmeans1_htst, kmeans0_htst])\n",
    "sns.histplot(data=kmeans_htst, x='htst', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_nothtst_histo(datatset, cluster_num):\n",
    "    kmeans = datatset[datatset.kmeans_labels == cluster_num]\n",
    "    kmeans_nothtst = pd.concat([kmeans.not_htst_1, kmeans.not_htst_2, kmeans.not_htst_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_nothtst['nothtst'] = kmeans_nothtst[0]\n",
    "    kmeans_nothtst['cluster']=cluster_num\n",
    "    kmeans_nothtst = kmeans_nothtst.drop(0, axis=1)\n",
    "    return kmeans_nothtst\n",
    "kmeans0_nothtst = cluster_nothtst_histo(ntd_val_clustering2_with_labels, 0)\n",
    "kmeans1_nothtst = cluster_nothtst_histo(ntd_val_clustering2_with_labels, 1)\n",
    "kmeans2_nothtst = cluster_nothtst_histo(ntd_val_clustering2_with_labels, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_nothtst = pd.concat([kmeans2_nothtst, kmeans1_nothtst, kmeans0_nothtst])\n",
    "sns.histplot(data=kmeans_nothtst, x='nothtst', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering 3 - clusters more > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading clustering data block3 from Farid\n",
    "ntd_val_clusters_morethen3 = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering2_with_labels_clusters345.csv').drop('Unnamed: 0', axis=1)\n",
    "ntd_val_clusters_morethen3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        data_clusters = data[clustering].unique()\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        cluster_lst = []\n",
    "        for cluster_num in data_clusters:\n",
    "            data_per_cluster = data[data[clustering] == cluster_num]\n",
    "            for ind, row in data_per_cluster.iterrows():\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_1'])\n",
    "                htst_lst.append(row['htst_1'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_1'])\n",
    "                cluster_lst.append(cluster_num)  \n",
    "\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_2'])\n",
    "                htst_lst.append(row['htst_2'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_2'])\n",
    "                cluster_lst.append(cluster_num)\n",
    "\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_3'])\n",
    "                htst_lst.append(row['htst_3'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_3'])\n",
    "                cluster_lst.append(cluster_num)   \n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, cluster_lst ), columns=['well','phit', 'htst', 'cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "ntd_val_kmeans4 = well_collecting_clusters(ntd_val_clusters_morethen3, 'kmeans_4_labels')\n",
    "ntd_val_kmeans4.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_phit_histo4(datatset, cluster_col, cluster_num):\n",
    "    kmeans = datatset[datatset[cluster_col] == cluster_num]\n",
    "    kmeans_phit = pd.concat([kmeans.phit_1, kmeans.phit_3, kmeans.phit_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_phit['phit'] = kmeans_phit[0]\n",
    "    kmeans_phit['cluster']=cluster_num\n",
    "    kmeans_phit = kmeans_phit.drop(0, axis=1)\n",
    "    return kmeans_phit\n",
    "kmeans0_phit = cluster_phit_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 0)\n",
    "kmeans1_phit = cluster_phit_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 1)\n",
    "kmeans2_phit = cluster_phit_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 2)\n",
    "kmeans3_phit = cluster_phit_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 3)\n",
    "custom_palette = {3: 'orange', 2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_phit = pd.concat([kmeans3_phit, kmeans2_phit, kmeans1_phit, kmeans0_phit])\n",
    "sns.histplot(data=kmeans_phit, x='phit', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_htst_histo4(datatset, cluster_col, cluster_num):\n",
    "    kmeans = datatset[datatset[cluster_col] == cluster_num]\n",
    "    kmeans_htst = pd.concat([kmeans.htst_1, kmeans.htst_2, kmeans.htst_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_htst['htst'] = kmeans_htst[0]\n",
    "    kmeans_htst['cluster']=cluster_num\n",
    "    kmeans_htst = kmeans_htst.drop(0, axis=1)\n",
    "    return kmeans_htst\n",
    "kmeans0_htst = cluster_htst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 0)\n",
    "kmeans1_htst = cluster_htst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 1)\n",
    "kmeans2_htst = cluster_htst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 2)\n",
    "kmeans3_htst = cluster_htst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 3)\n",
    "custom_palette = {3: 'orange', 2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_htst = pd.concat([kmeans3_htst, kmeans2_htst, kmeans1_htst, kmeans0_htst])\n",
    "sns.histplot(data=kmeans_htst, x='htst', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_nothtst_histo4(datatset, cluster_col, cluster_num):\n",
    "    kmeans = datatset[datatset[cluster_col] == cluster_num]\n",
    "    kmeans_nothtst = pd.concat([kmeans.not_htst_1, kmeans.not_htst_2, kmeans.not_htst_3]).reset_index().drop('index', axis=1)\n",
    "    kmeans_nothtst['nothtst'] = kmeans_nothtst[0]\n",
    "    kmeans_nothtst['cluster']=cluster_num\n",
    "    kmeans_nothtst = kmeans_nothtst.drop(0, axis=1)\n",
    "    return kmeans_nothtst\n",
    "kmeans0_nothtst = cluster_nothtst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 0)\n",
    "kmeans1_nothtst = cluster_nothtst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 1)\n",
    "kmeans2_nothtst = cluster_nothtst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 2)\n",
    "kmeans3_nothtst = cluster_nothtst_histo4(ntd_val_clusters_morethen3, 'kmeans_4_labels', 3)\n",
    "custom_palette = {3: 'orange', 2: 'red', 1: 'green', 0: 'blue'}\n",
    "kmeans_nothtst = pd.concat([kmeans3_nothtst, kmeans2_nothtst, kmeans1_nothtst, kmeans0_nothtst])\n",
    "sns.histplot(data=kmeans_nothtst, x='nothtst', hue='cluster', bins=25, kde=True, palette=custom_palette);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_display_into_well(dataset, wellname):\n",
    "    welldata = dataset[dataset.well == wellname]\n",
    "    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(2,4))\n",
    "    ax.plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1)\n",
    "    ax.set_xlim(0, 0.35)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(wellname)\n",
    "    ax.grid()\n",
    "    def clusters_indice(welldata):\n",
    "        indices0 = welldata.index[(welldata['cluster'] == 0) & \n",
    "                                    (welldata['cluster'].shift(-1) != 0) | \n",
    "                                    (welldata['cluster'] == 0) & (welldata['cluster'].shift(+1) != 0)].tolist()\n",
    "        indices1 = welldata.index[(welldata['cluster'] == 1) & \n",
    "                                    (welldata['cluster'].shift(-1) != 1) | \n",
    "                                    (welldata['cluster'] == 1) & (welldata['cluster'].shift(+1) != 1)].tolist()\n",
    "        indices2 = welldata.index[(welldata['cluster'] == 2) & \n",
    "                                    (welldata['cluster'].shift(-1) != 2) | \n",
    "                                    (welldata['cluster'] == 2) & (welldata['cluster'].shift(+1) != 2)].tolist()\n",
    "        indices3 = welldata.index[(welldata['cluster'] == 3) & \n",
    "                                    (welldata['cluster'].shift(-1) != 3) | \n",
    "                                    (welldata['cluster'] == 3) & (welldata['cluster'].shift(+1) != 3)].tolist()\n",
    "        cluster_list = [indices0, indices1, indices2, indices3]\n",
    "        return cluster_list\n",
    "    cluster_list = clusters_indice(welldata)\n",
    "    def clusters_rectangle(welldata, i, color):\n",
    "            if cluster_list[i][0] > 0:\n",
    "                cluster_xy = welldata['depth'].iloc[cluster_list[i][0]-1]\n",
    "            else:\n",
    "                cluster_xy = welldata['depth'].iloc[cluster_list[i][0]]\n",
    "            try:\n",
    "                # cluster_h = welldata['depth'].iloc[cluster_list[i][1]] - welldata['depth'].iloc[cluster_list[i][0]]\n",
    "                ind1 = cluster_list[i][0]\n",
    "                ind2 = cluster_list[i][1]\n",
    "                cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "            except:\n",
    "                pass\n",
    "                # cluster_h = welldata['depth'].iloc[cluster_list[i][1]] - welldata['depth'].iloc[cluster_list[i][0]] \n",
    "            # cluster_h = welldata['depth'].iloc[cluster_list[i][1]+1] - welldata['depth'].iloc[cluster_list[i][0]] \n",
    "            rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "            ax.add_patch(rectangle)\n",
    "    def clusters_rectangle2(welldata, i, color):\n",
    "            cluster_xy = welldata['depth'].iloc[cluster_list[i][2]-1]\n",
    "            # try:\n",
    "            #     cluster_h = welldata['depth'].iloc[cluster_list[i][3]+1] - welldata['depth'].iloc[cluster_list[i][2]]\n",
    "            # except:\n",
    "            #     cluster_h = welldata['depth'].iloc[cluster_list[i][3]] - welldata['depth'].iloc[cluster_list[i][2]]\n",
    "            try:\n",
    "                ind1 = cluster_list[i][2]\n",
    "                ind2 = cluster_list[i][3]\n",
    "                cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "            except:\n",
    "                pass\n",
    "            rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "            ax.add_patch(rectangle)\n",
    "\n",
    "    for i in range(len(cluster_list)):\n",
    "        if len(cluster_list[i]) > 0 and i==0:\n",
    "            clusters_rectangle(welldata, i, 'blue')\n",
    "            # print('cluster 0', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'blue')\n",
    "            except:\n",
    "                pass\n",
    "        if len(cluster_list[i]) > 0 and i==1:\n",
    "            clusters_rectangle(welldata, i, 'green')\n",
    "            # print('cluster 1', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'green')\n",
    "            except:\n",
    "                pass\n",
    "        if len(cluster_list[i]) > 0 and i==2:\n",
    "            clusters_rectangle(welldata, i, 'red')\n",
    "            # print('cluster 2', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'red')\n",
    "            except:\n",
    "                pass\n",
    "        if len(cluster_list[i]) > 0 and i==3:\n",
    "            clusters_rectangle(welldata, i, 'orange')\n",
    "            # print('cluster 2', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'orange')\n",
    "            except:\n",
    "                pass\n",
    "    print(welldata)\n",
    "clusters_display_into_well(ntd_val_kmeans4, 'B01Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters4_matrix_plot(dataset, letters_list, rows, columns, output_flag, clustering):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "\n",
    "                    def clusters_indice(welldata):\n",
    "                        indices0 = welldata.index[(welldata['cluster'] == 0) & \n",
    "                                                  (welldata['cluster'].shift(-1) != 0) | \n",
    "                                                  (welldata['cluster'] == 0) & (welldata['cluster'].shift(+1) != 0)].tolist()\n",
    "                        indices1 = welldata.index[(welldata['cluster'] == 1) & \n",
    "                                                  (welldata['cluster'].shift(-1) != 1) | \n",
    "                                                  (welldata['cluster'] == 1) & (welldata['cluster'].shift(+1) != 1)].tolist()\n",
    "                        indices2 = welldata.index[(welldata['cluster'] == 2) & \n",
    "                                                  (welldata['cluster'].shift(-1) != 2) | \n",
    "                                                  (welldata['cluster'] == 2) & (welldata['cluster'].shift(+1) != 2)].tolist()\n",
    "                        indices3 = welldata.index[(welldata['cluster'] == 3) & \n",
    "                                                  (welldata['cluster'].shift(-1) != 3) | \n",
    "                                                  (welldata['cluster'] == 3) & (welldata['cluster'].shift(+1) != 3)].tolist()\n",
    "                        cluster_list = [indices0, indices1, indices2, indices3]\n",
    "                        return cluster_list\n",
    "                    cluster_list = clusters_indice(welldata)\n",
    "                    def clusters_rectangle(welldata, k, color):\n",
    "                        if cluster_list[k][0] > 0:\n",
    "                            cluster_xy = welldata['depth'].iloc[cluster_list[k][0]-1]\n",
    "                        else:\n",
    "                            cluster_xy = welldata['depth'].iloc[cluster_list[k][0]]\n",
    "                        try:\n",
    "                            ind1 = cluster_list[k][0]\n",
    "                            ind2 = cluster_list[k][1]\n",
    "                            cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "                        except:\n",
    "                            pass\n",
    "                        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "                        ax[j,i].add_patch(rectangle)\n",
    "                    def clusters_rectangle2(welldata, k, color):\n",
    "                        cluster_xy = welldata['depth'].iloc[cluster_list[i][2]-1]\n",
    "                        try:\n",
    "                            ind1 = cluster_list[k][2]\n",
    "                            ind2 = cluster_list[k][3]\n",
    "                            cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "                        except:\n",
    "                            pass\n",
    "                        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "                        ax[j,i].add_patch(rectangle)\n",
    "                    \n",
    "                    for k in range(len(cluster_list)):\n",
    "                        if len(cluster_list[k]) > 0 and k==0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'blue')\n",
    "                            except:\n",
    "                                pass\n",
    "                        if len(cluster_list[k]) > 0 and k==1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'green')\n",
    "                            except:\n",
    "                                pass\n",
    "                        if len(cluster_list[k]) > 0 and k==2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'red')\n",
    "                            except:\n",
    "                                pass\n",
    "                        if len(cluster_list[k]) > 0 and k==3:\n",
    "                            clusters_rectangle(welldata, k, 'orange')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'orange')\n",
    "                            except:\n",
    "                                pass\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_3blocks_4\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "coloring_clusters4_matrix_plot(ntd_val_kmeans4, \n",
    "                              ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J'], \n",
    "                              4, 9, 'dont print', 'kmeans4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coloring clustered samples into wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wellname = 'A01Y'\n",
    "ntd_val_clustering2_with_labels = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering2_with_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "ntd_val_clustering2_with_labels[ntd_val_clustering2_with_labels.well==wellname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_per_sample_display(dataset, columns, rows, clustering, colors_dict, block):\n",
    "    fig, ax = plt.subplots(rows, columns, figsize=(17,10))\n",
    "    counter = (block-1)*rows*columns\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < block*rows*columns+1:\n",
    "                try:\n",
    "                    data = dataset.iloc[counter]\n",
    "                    title = dataset.iloc[counter]['well']\n",
    "                    colors = dataset.iloc[counter][clustering]\n",
    "                    y = [0, data.htst_1, data.not_htst_1, data.htst_2, data.not_htst_2, data.htst_3, data.not_htst_3]\n",
    "                    x = [0, data.phit_1, 0, data.phit_2, 0, data.phit_3, 0]\n",
    "\n",
    "                    depth_summ_list = []\n",
    "                    depth_counter = 0\n",
    "                    for k in range(len(y)):\n",
    "                        depth_counter += y[k]\n",
    "                        depth_summ_list.append(depth_counter) \n",
    "                    ax[j,i].step(x, depth_summ_list, label=title, where='post', color=colors_dict[colors], linestyle='-', linewidth=2)\n",
    "                    ax[j,i].set_ylim(-5,100)\n",
    "                    ax[j,i].set_xlim(0,0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].legend(fontsize=8)\n",
    "                    counter +=1\n",
    "                except:\n",
    "                    pass\n",
    "colors_dict = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "for i in range(1,len(ntd_val_clustering2_with_labels)//(9*4)+2)[:10]:\n",
    "    clusters_per_sample_display(ntd_val_clustering2_with_labels, 9, 4, 'kmeans_labels', colors_dict, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        data_clusters = data[clustering].unique()\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        cluster_lst = []\n",
    "        for cluster_num in data_clusters:\n",
    "            data_per_cluster = data[data[clustering] == cluster_num]\n",
    "            for ind, row in data_per_cluster.iterrows():\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_1'])\n",
    "                htst_lst.append(row['htst_1'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_1'])\n",
    "                cluster_lst.append(cluster_num)  \n",
    "\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_2'])\n",
    "                htst_lst.append(row['htst_2'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_2'])\n",
    "                cluster_lst.append(cluster_num)\n",
    "\n",
    "                well_lst.append(wellname)\n",
    "                cluster_lst.append(cluster_num)\n",
    "                phit_lst.append(row['phit_3'])\n",
    "                htst_lst.append(row['htst_3'])\n",
    "                well_lst.append(wellname)\n",
    "                phit_lst.append(0)\n",
    "                htst_lst.append(row['not_htst_3'])\n",
    "                cluster_lst.append(cluster_num)   \n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, cluster_lst ), columns=['well','phit', 'htst', 'cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "ntd_val_kmeans3 = well_collecting_clusters(ntd_val_clustering2_with_labels, 'kmeans_labels')\n",
    "ntd_val_kmeans3[ntd_val_kmeans3.well=='A01Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_display_into_well(dataset, wellname):\n",
    "    welldata = dataset[dataset.well == wellname]\n",
    "    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(2,4))\n",
    "    ax.plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1)\n",
    "    ax.set_xlim(0, 0.35)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(wellname)\n",
    "    ax.grid()\n",
    "\n",
    "    def clusters_indice(welldata):\n",
    "        indices0 = welldata.index[(welldata['cluster'] == 0) & (welldata['cluster'].shift(-1) != 0) | (welldata['cluster'] == 0) & (welldata['cluster'].shift(+1) != 0)].tolist()\n",
    "        indices1 = welldata.index[(welldata['cluster'] == 1) & (welldata['cluster'].shift(-1) != 1) | (welldata['cluster'] == 1) & (welldata['cluster'].shift(+1) != 1)].tolist()\n",
    "        indices2 = welldata.index[(welldata['cluster'] == 2) & (welldata['cluster'].shift(-1) != 2) | (welldata['cluster'] == 2) & (welldata['cluster'].shift(+1) != 2)].tolist()\n",
    "        cluster_list = [indices0, indices1, indices2]\n",
    "        return cluster_list\n",
    "    cluster_list = clusters_indice(welldata)\n",
    "    def clusters_rectangle(welldata, i, color):\n",
    "            if cluster_list[i][0] > 0:\n",
    "                cluster_xy = welldata['depth'].iloc[cluster_list[i][0]-1]\n",
    "            else:\n",
    "                cluster_xy = welldata['depth'].iloc[cluster_list[i][0]]\n",
    "            try:\n",
    "                # cluster_h = welldata['depth'].iloc[cluster_list[i][1]] - welldata['depth'].iloc[cluster_list[i][0]]\n",
    "                ind1 = cluster_list[i][0]\n",
    "                ind2 = cluster_list[i][1]\n",
    "                cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "            except:\n",
    "                pass\n",
    "                # cluster_h = welldata['depth'].iloc[cluster_list[i][1]] - welldata['depth'].iloc[cluster_list[i][0]] \n",
    "            # cluster_h = welldata['depth'].iloc[cluster_list[i][1]+1] - welldata['depth'].iloc[cluster_list[i][0]] \n",
    "            rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "            ax.add_patch(rectangle)\n",
    "    def clusters_rectangle2(welldata, i, color):\n",
    "            cluster_xy = welldata['depth'].iloc[cluster_list[i][2]-1]\n",
    "            # try:\n",
    "            #     cluster_h = welldata['depth'].iloc[cluster_list[i][3]+1] - welldata['depth'].iloc[cluster_list[i][2]]\n",
    "            # except:\n",
    "            #     cluster_h = welldata['depth'].iloc[cluster_list[i][3]] - welldata['depth'].iloc[cluster_list[i][2]]\n",
    "            try:\n",
    "                ind1 = cluster_list[i][2]\n",
    "                ind2 = cluster_list[i][3]\n",
    "                cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "            except:\n",
    "                pass\n",
    "            rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "            ax.add_patch(rectangle)\n",
    "\n",
    "    for i in range(len(cluster_list)):\n",
    "        if len(cluster_list[i]) > 0 and i==0:\n",
    "            clusters_rectangle(welldata, i, 'blue')\n",
    "            # print('cluster 0', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'blue')\n",
    "            except:\n",
    "                pass\n",
    "        if len(cluster_list[i]) > 0 and i==1:\n",
    "            clusters_rectangle(welldata, i, 'green')\n",
    "            # print('cluster 1', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'green')\n",
    "            except:\n",
    "                pass\n",
    "        if len(cluster_list[i]) > 0 and i==2:\n",
    "            clusters_rectangle(welldata, i, 'red')\n",
    "            # print('cluster 2', cluster_list[i], i, cluster_list[i][1])\n",
    "            try:\n",
    "                clusters_rectangle2(welldata, i, 'red')\n",
    "            except:\n",
    "                pass\n",
    "    print(welldata)\n",
    "clusters_display_into_well(ntd_val_kmeans3, 'C14Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_plot(dataset, letters_list, rows, columns, output_flag, clustering):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    \n",
    "                    def clusters_indice(welldata):\n",
    "                        indices0 = welldata.index[(welldata['cluster'] == 0) & (welldata['cluster'].shift(-1) != 0) | (welldata['cluster'] == 0) & (welldata['cluster'].shift(+1) != 0)].tolist()\n",
    "                        indices1 = welldata.index[(welldata['cluster'] == 1) & (welldata['cluster'].shift(-1) != 1) | (welldata['cluster'] == 1) & (welldata['cluster'].shift(+1) != 1)].tolist()\n",
    "                        indices2 = welldata.index[(welldata['cluster'] == 2) & (welldata['cluster'].shift(-1) != 2) | (welldata['cluster'] == 2) & (welldata['cluster'].shift(+1) != 2)].tolist()\n",
    "                        cluster_list = [indices0, indices1, indices2]\n",
    "                        return cluster_list\n",
    "                    cluster_list = clusters_indice(welldata)\n",
    "                    def clusters_rectangle(welldata, k, color):\n",
    "                        if cluster_list[k][0] > 0:\n",
    "                            cluster_xy = welldata['depth'].iloc[cluster_list[k][0]-1]\n",
    "                        else:\n",
    "                            cluster_xy = welldata['depth'].iloc[cluster_list[k][0]]\n",
    "                        try:\n",
    "                            ind1 = cluster_list[k][0]\n",
    "                            ind2 = cluster_list[k][1]\n",
    "                            cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "                        except:\n",
    "                            pass\n",
    "                        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "                        ax[j,i].add_patch(rectangle)\n",
    "                    def clusters_rectangle2(welldata, k, color):\n",
    "                        cluster_xy = welldata['depth'].iloc[cluster_list[i][2]-1]\n",
    "                        try:\n",
    "                            ind1 = cluster_list[k][2]\n",
    "                            ind2 = cluster_list[k][3]\n",
    "                            cluster_h = welldata['htst'].iloc[ind1:ind2+1].sum()\n",
    "                        except:\n",
    "                            pass\n",
    "                        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "                        ax[j,i].add_patch(rectangle)\n",
    "                    \n",
    "                    for k in range(len(cluster_list)):\n",
    "                        if len(cluster_list[k]) > 0 and k==0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'blue')\n",
    "                            except:\n",
    "                                pass\n",
    "                        if len(cluster_list[k]) > 0 and k==1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'green')\n",
    "                            except:\n",
    "                                pass\n",
    "                        if len(cluster_list[k]) > 0 and k==2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                            try:\n",
    "                                clusters_rectangle2(welldata, k, 'red')\n",
    "                            except:\n",
    "                                pass\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_3blocks\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "coloring_clusters_matrix_plot(ntd_val_kmeans3, \n",
    "                              ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J'], \n",
    "                              4, 9, 'dontprint', 'kmeans3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering top_phi_bot layering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading clustering data top_phi_bot from Farid\n",
    "ntd_val_clustering_top_phi_bot = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering_one_layer_with_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "ntd_val_clustering_top_phi_bot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters_top_phi_bot(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        cluster_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            cluster_lst.append(np.nan)\n",
    "            cluster_lst.append(row[clustering])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        cluster_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, cluster_lst ), columns=['well','phit', 'htst', 'cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "ntd_val_tpb_test = well_collecting_clusters_top_phi_bot(ntd_val_clustering_top_phi_bot, 'kmeans_3_labels')\n",
    "\n",
    "def clusters_display_into_well_tpb3(dataset, wellname):\n",
    "    def clusters_rectangle(data, i, color):\n",
    "        # cluster_xy = data['depth'].iloc[i-2]\n",
    "        cluster_xy = data['depth'].iloc[i-1]\n",
    "        # cluster_h = data['depth'].iloc[i+1] - data['depth'].iloc[i-2]\n",
    "        cluster_h = data['depth'].iloc[i] - data['depth'].iloc[i-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax.add_patch(rectangle)\n",
    "    data = dataset[dataset.well == wellname]\n",
    "    top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':data['cluster'].iloc[0],'depth':[0]})\n",
    "    data = pd.concat([top, data])\n",
    "    # print(data)\n",
    "    fig, ax = plt.subplots(figsize=(2,4))\n",
    "    ax.plot(data['phit'], data['depth'], drawstyle='steps-post', color='black', lw=0.75, alpha=0.75)\n",
    "    ax.set_xlim(0, 0.35)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(wellname)\n",
    "    ax.grid()\n",
    "    for i in range(len(data)):\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 0:\n",
    "            clusters_rectangle(data, i, 'blue')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 1:\n",
    "            clusters_rectangle(data, i, 'green')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 2:\n",
    "            clusters_rectangle(data, i, 'red')\n",
    "clusters_display_into_well_tpb3(ntd_val_tpb_test, 'A01W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "# ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']\n",
    "coloring_clusters_matrix_tpb3(ntd_val_tpb_test, ['A'], 4, 9, 'kmeans3_tpb', 'dontprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_htst_concat(datatset, cluster_num):\n",
    "    top_htst = datatset[(datatset.cluster.isna()) & (datatset.cluster.shift(-1)==cluster_num)]\n",
    "    top_htst['cluster'] = cluster_num\n",
    "    return top_htst\n",
    "\n",
    "dataset = ntd_val_tpb_test\n",
    "top_tpb0 = top_htst_concat(dataset, 0)\n",
    "top_tpb1 = top_htst_concat(dataset, 1)\n",
    "top_tpb2 = top_htst_concat(dataset, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "top_tpb3 = pd.concat([top_tpb0, top_tpb1, top_tpb2])\n",
    "ax = sns.histplot(data=top_tpb3, x='htst', hue='cluster', bins=25, kde=True, palette=custom_palette)\n",
    "ax.set_xticks(range(0, 60, 5)), ax.grid(True, axis='x')\n",
    "ax.set(xlabel='top_htst', ylabel='Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_htst_concat(dataset, cluster_num):\n",
    "    bot_htst = dataset[(dataset.cluster.isna()) & (dataset.cluster.shift(1)==cluster_num)]\n",
    "    bot_htst['cluster'] = cluster_num\n",
    "    return bot_htst\n",
    "dataset = ntd_val_tpb_test\n",
    "bot_tpb0 = bot_htst_concat(dataset, 0)\n",
    "bot_tpb1 = bot_htst_concat(dataset, 1)\n",
    "bot_tpb2 = bot_htst_concat(dataset, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "bot_tpb3 = pd.concat([bot_tpb0, bot_tpb1, bot_tpb2])\n",
    "ax = sns.histplot(data=bot_tpb3, x='htst', hue='cluster', bins=25, kde=True, palette=custom_palette)\n",
    "ax.set_xticks(range(0, 50, 5)), ax.grid(True, axis='x')\n",
    "ax.set(xlabel='bot_htst', ylabel='Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topbot_htst_concat(dataset, cluster_num):\n",
    "    top_htst = dataset[(dataset.cluster.isna()) & (dataset.cluster.shift(-1)==cluster_num)]\n",
    "    bot_htst = dataset[(dataset.cluster.isna()) & (dataset.cluster.shift(1)==cluster_num)]\n",
    "    topbot_tpb = pd.concat([top_htst, bot_htst])\n",
    "    topbot_tpb['cluster'] = cluster_num\n",
    "    return topbot_tpb\n",
    "dataset = ntd_val_tpb_test\n",
    "topbot_tpb0 = topbot_htst_concat(dataset, 0)\n",
    "topbot_tpb1 = topbot_htst_concat(dataset, 1)\n",
    "topbot_tpb2 = topbot_htst_concat(dataset, 2)\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "topbot_tpb3 = pd.concat([topbot_tpb0, topbot_tpb1, topbot_tpb2])\n",
    "ax = sns.histplot(data=topbot_tpb3, x='htst', hue='cluster', bins=25, kde=True, palette=custom_palette)\n",
    "ax.set_xticks(range(0, 60, 5)), ax.grid(True, axis='x')\n",
    "ax.set(xlabel='top_bot_htst', ylabel='Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ntd_val_tpb_test\n",
    "phit0 = dataset[dataset.cluster==0][['phit', 'cluster']]\n",
    "phit1 = dataset[dataset.cluster==1][['phit', 'cluster']]\n",
    "phit2 = dataset[dataset.cluster==2][['phit', 'cluster']]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "phit_tpb3 = pd.concat([phit0, phit1, phit2])\n",
    "ax = sns.histplot(data=phit_tpb3, x='phit', hue='cluster', bins=35, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "ax.set_xticks(np.arange(0.12, 0.32, 0.02));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ntd_val_tpb_test\n",
    "htst0 = dataset[dataset.cluster==0]\n",
    "htst1 = dataset[dataset.cluster==1]\n",
    "htst2 = dataset[dataset.cluster==2]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "htst_tpb3 = pd.concat([htst0, htst1, htst2])\n",
    "ax = sns.histplot(data=htst_tpb3, x='htst', hue='cluster', bins=26, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "ax.set_xticks(np.arange(0, 60, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering top_phi_bot layering v2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithms = [\"kmeans\",\"gmm\",\"agglomerative\"]\n",
    "# algorithm_model_map = {}\n",
    "# cluster_max_span = 10\n",
    " \n",
    "# for algo in algorithms:\n",
    "#     for cluster_num in range(3,cluster_max_span+1):\n",
    "#         labels = None\n",
    "#         if algo == \"kmeans\":\n",
    "#             kmeans = KMeans(n_clusters=cluster_num, random_state=42)\n",
    "#             kmeans_labels = kmeans.fit_predict(normalized_data)\n",
    "#             labels = kmeans_labels\n",
    " \n",
    "#         elif algo == \"gmm\":\n",
    "#             gmm = GaussianMixture(n_components=cluster_num, random_state=42)\n",
    "#             gmm.fit(normalized_data)\n",
    "#             gmm_labels = gmm.predict(normalized_data)\n",
    "#             labels = gmm_labels\n",
    "#         elif algo == \"agglomerative\":\n",
    "#             agglomerative = AgglomerativeClustering(n_clusters=cluster_num)\n",
    "#             agglomerative_labels = agglomerative.fit_predict(normalized_data)\n",
    "#             labels = agglomerative_labels\n",
    "\n",
    "#         na_dropped_data[f\"{algo}_{cluster_num}_labels\"]=labels\n",
    " \n",
    "# na_dropped_data.to_csv(\"top_phi_bot_cluster_ntg10_with_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_data_calculation(dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "    def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "        row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "        row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "        return row_change\n",
    "    df_list = []\n",
    "    for wellname in dataset_ntd.well.unique():\n",
    "        df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "        df_list.append(df)\n",
    "    res_df_list = pd.concat(df_list)\n",
    "    result = pd.concat([dataset_ntd, res_df_list])\n",
    "    result = result.sort_values(by=['well','top_tst'])\n",
    "    result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "    return result_final\n",
    "ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering with top-phi-bot v2\n",
    "def top_phit_bot_clustering(dataset, print_flag, path):\n",
    "    def top_phit_bot_collection_run(dataset):\n",
    "        def top_phit_bot_collection(dataset, wellname):\n",
    "            data = dataset[dataset.well == wellname]\n",
    "            data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "            data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "            data['bot_htst'] = data['not_htst']\n",
    "            data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', \n",
    "                         'top_htst','htst','bot_htst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            return data\n",
    "        df_lst = []\n",
    "        for wellname in tqdm(dataset.well.unique()):\n",
    "            res_df = top_phit_bot_collection(dataset, wellname)\n",
    "            df_lst.append(res_df)\n",
    "        top_phi_bot_cluster = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster\n",
    "    top_phi_bot_cluster = top_phit_bot_collection_run(dataset)\n",
    "\n",
    "    def top_phit_bot_ntg_run(dataset):\n",
    "        def top_phit_bot_ntg(dataset, wellname):\n",
    "            ntg = []\n",
    "            data = dataset[dataset.well == wellname].reset_index(drop=True)\n",
    "            for ind, row in data.iterrows():\n",
    "                if ind == 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst']))\n",
    "                if ind != 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst'] + row['top_htst']))\n",
    "                if ind == len(data):\n",
    "                    ntg.append(row['htst']/(row['top_htst'] + row['htst']))\n",
    "            result = pd.concat([data, pd.DataFrame({'ntg':ntg})], axis=1)\n",
    "            return result\n",
    "        df_lst = []\n",
    "        for wellname in dataset.well.unique():\n",
    "            df = top_phit_bot_ntg(dataset, wellname)\n",
    "            df_lst.append(df)\n",
    "        top_phi_bot_cluster_ntg = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster_ntg\n",
    "    top_phi_bot_cluster_ntg = top_phit_bot_ntg_run(top_phi_bot_cluster)\n",
    "\n",
    "    if print_flag == 'print':\n",
    "        top_phi_bot_cluster_ntg.to_csv(path, index=False)\n",
    "    else:\n",
    "        pass\n",
    "    return top_phi_bot_cluster_ntg\n",
    "top_phi_bot_cluster8 = top_phit_bot_clustering(ntd_val_final8_clean, 'dont_print', '.\\output\\\\top_phi_bot_cluster8.csv')\n",
    "top_phi_bot_cluster10 = top_phit_bot_clustering(ntd_val_final10_clean, 'dont_print', '.\\output\\\\top_phi_bot_cluster8.csv')\n",
    "# top_phi_bot_cluster8 = top_phi_bot_cluster8[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg','htst','ntg']]\n",
    "# top_phi_bot_cluster10 = top_phi_bot_cluster10[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg','htst','ntg']]\n",
    "# top_phi_bot_cluster8.to_csv('.\\output\\\\top_phi_bot_cluster_ntg8.csv')\n",
    "# top_phi_bot_cluster10.to_csv('.\\output\\\\top_phi_bot_cluster_ntg10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_top_phi_bot_v2 = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\top_phi_bot_cluster_ntg8_with_labels.csv').drop(['Unnamed: 0.1','Unnamed: 0'], axis=1)\n",
    "cluster_top_phi_bot_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_top_phi_bot_v3 = cluster_top_phi_bot_v2.join(top_phi_bot_cluster8[['top_htst', 'bot_htst']])\n",
    "top_phi_bot_cluster8[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg','htst','ntg']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters_top_phi_bot_v2(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        ntg_lst = []\n",
    "        vsh_lst = []\n",
    "        cluster_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            cluster_lst.append(np.nan)\n",
    "            cluster_lst.append(row[clustering])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "            ntg_lst.append(0)\n",
    "            ntg_lst.append(row['ntg'])\n",
    "\n",
    "            vsh_lst.append(0)\n",
    "            vsh_lst.append(row['vsh_avg'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        cluster_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, ntg_lst, vsh_lst, cluster_lst ), columns=['well','phit', 'htst', \n",
    "                                                                                                               'ntg', 'vsh', 'cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "tpb_kmeans_v3 = well_collecting_clusters_top_phi_bot_v2(cluster_top_phi_bot_v3, 'kmeans_3_labels')\n",
    "\n",
    "def clusters_display_into_well_tpb3(dataset, wellname):\n",
    "    def clusters_rectangle(data, i, color):\n",
    "        # cluster_xy = data['depth'].iloc[i-2]\n",
    "        cluster_xy = data['depth'].iloc[i-1]\n",
    "        # cluster_h = data['depth'].iloc[i+1] - data['depth'].iloc[i-2]\n",
    "        cluster_h = data['depth'].iloc[i] - data['depth'].iloc[i-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax.add_patch(rectangle)\n",
    "    data = dataset[dataset.well == wellname]\n",
    "    top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':data['cluster'].iloc[0],'depth':[0]})\n",
    "    data = pd.concat([top, data])\n",
    "    # print(data)\n",
    "    fig, ax = plt.subplots(figsize=(2,4))\n",
    "    ax.plot(data['phit'], data['depth'], drawstyle='steps-post', color='black', lw=0.75, alpha=0.75)\n",
    "    ax.set_xlim(0, 0.35)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(wellname)\n",
    "    ax.grid()\n",
    "    for i in range(len(data)):\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 0:\n",
    "            clusters_rectangle(data, i, 'blue')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 1:\n",
    "            clusters_rectangle(data, i, 'green')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 2:\n",
    "            clusters_rectangle(data, i, 'red')\n",
    "clusters_display_into_well_tpb3(tpb_kmeans_v3, 'C16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "# ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']        \n",
    "coloring_clusters_matrix_tpb3(tpb_kmeans_v3, ['B'], 4, 9, 'kmeans3_tpb', 'dontprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb_kmeans_v4 = tpb_kmeans_v3[tpb_kmeans_v3.phit !=0]\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 4))\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "sns.histplot(data=tpb_kmeans_v4, x='phit', hue='cluster', ax=axes[0], kde=True,  palette=custom_palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = tpb_kmeans_v3\n",
    "def histo_phit(dataset):\n",
    "    phit0 = dataset[dataset.cluster==0][['phit', 'cluster']]\n",
    "    phit1 = dataset[dataset.cluster==1][['phit', 'cluster']]\n",
    "    phit2 = dataset[dataset.cluster==2][['phit', 'cluster']]\n",
    "    custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "    phit_tpb3 = pd.concat([phit0, phit1, phit2])\n",
    "    ax = sns.histplot(data=phit_tpb3, x='phit', hue='cluster', bins=35, kde=True, palette=custom_palette)\n",
    "    ax.grid(True, axis='x')\n",
    "    ax.set_xticks(np.arange(0.12, 0.32, 0.02));\n",
    "histo_phit(tpb_kmeans_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_kmeans_v3\n",
    "ntg0 = dataset[dataset.cluster==0][['ntg', 'cluster']]\n",
    "ntg1 = dataset[dataset.cluster==1][['ntg', 'cluster']]\n",
    "ntg2 = dataset[dataset.cluster==2][['ntg', 'cluster']]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "ntg_tpb_v3 = pd.concat([ntg0, ntg1, ntg2])\n",
    "ax = sns.histplot(data=ntg_tpb_v3, x='ntg', hue='cluster', bins=35, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "# ax.set_xticks(np.arange(0.12, 0.32, 0.02));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_kmeans_v3\n",
    "htst0 = dataset[dataset.cluster==0]\n",
    "htst1 = dataset[dataset.cluster==1]\n",
    "htst2 = dataset[dataset.cluster==2]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "htst_tpb_v3 = pd.concat([htst0, htst1, htst2])\n",
    "ax = sns.histplot(data=htst_tpb_v3, x='htst', hue='cluster', bins=50, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "ax.set_xticks(np.arange(0, 60, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_kmeans_v3\n",
    "vsh0 = dataset[dataset.cluster==0]\n",
    "vsh1 = dataset[dataset.cluster==1]\n",
    "vsh2 = dataset[dataset.cluster==2]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "vsh_tpb_v3 = pd.concat([vsh0, vsh1, vsh2])\n",
    "ax = sns.histplot(data=vsh_tpb_v3, x='vsh', hue='cluster', bins=50, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kmeans.to_csv('cluster_kmeans.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data_to_map(dataset, dataset_coord, comment):\n",
    "    def dataset_groupby(dataset):\n",
    "        result = dataset.groupby(['well','cluster'])['htst'].sum().reset_index()\n",
    "        return result\n",
    "    tpb_test_v3_piechart = dataset_groupby(dataset)\n",
    "\n",
    "    def cluster_transpose(dataset, wellname):\n",
    "        result = dataset[dataset.well==wellname]\n",
    "        result.loc[result.cluster == 0, 'cluster_0'] = result.htst\n",
    "        result.loc[result.cluster == 1, 'cluster_1'] = result.htst\n",
    "        result.loc[result.cluster == 2, 'cluster_2'] = result.htst\n",
    "        result.fillna(0)\n",
    "        result = result.groupby('well').sum().reset_index()\n",
    "        result = result[['well', 'cluster_0', 'cluster_1','cluster_2']]\n",
    "        return result\n",
    "    df_lst = []\n",
    "    for wellname in tpb_test_v3_piechart.well.unique():\n",
    "        df = cluster_transpose(tpb_test_v3_piechart, wellname)\n",
    "        df_lst.append(df)\n",
    "    data_transpose = pd.concat(df_lst).reset_index(drop=True)\n",
    "\n",
    "    def coordinates_calc(dataset_coord, fm):\n",
    "        dataset_coord = dataset_coord[dataset_coord.FORMATION_up == fm]\n",
    "        result = dataset_coord.groupby('well')[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        return result \n",
    "    coord = coordinates_calc(dataset_coord, 'Balakhany VIII')\n",
    "    data_transpose_coord = data_transpose.set_index('well').join(coord.set_index('well'), rsuffix='_coord').reset_index()\n",
    "\n",
    "    def piechart_map(dataset_map):\n",
    "        fig, ax = plt.subplots(figsize=(13,10))\n",
    "        ax.scatter(bal8_1510['X']/1000, bal8_1510['Y']/1000, c=bal8_1510['geobody'])\n",
    "        for ind, row in dataset_map.iterrows():\n",
    "                ax.pie([row['cluster_0'], row['cluster_1'], row['cluster_2']], \n",
    "                        radius=0.3, center=(row['X_mean']/1000, row['Y_mean']/1000), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.75},\n",
    "                        colors=['blue', 'green', 'red'], frame=True)\n",
    "        # plt.grid()\n",
    "        plt.title(comment)     \n",
    "    piechart_map(data_transpose_coord)\n",
    "    return data_transpose_coord\n",
    "\n",
    "cluster_kmeans = cluster_data_to_map(tpb_kmeans_v3, df_bal_net2_kh, 'Well clustering by Kmeans & Bal8 1510 geobodies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters_top_phi_bot_v2(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        ntg_lst = []\n",
    "        vsh_lst = []\n",
    "        cluster_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            cluster_lst.append(np.nan)\n",
    "            cluster_lst.append(row[clustering])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "            ntg_lst.append(0)\n",
    "            ntg_lst.append(row['ntg'])\n",
    "\n",
    "            vsh_lst.append(0)\n",
    "            vsh_lst.append(row['vsh_avg'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        cluster_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, ntg_lst, vsh_lst, cluster_lst ), columns=['well','phit', 'htst', \n",
    "                                                                                                               'ntg', 'vsh', 'cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "tpb_gmm_v3 = well_collecting_clusters_top_phi_bot_v2(cluster_top_phi_bot_v3, 'gmm_3_labels')\n",
    "\n",
    "def clusters_display_into_well_tpb3(dataset, wellname):\n",
    "    def clusters_rectangle(data, i, color):\n",
    "        # cluster_xy = data['depth'].iloc[i-2]\n",
    "        cluster_xy = data['depth'].iloc[i-1]\n",
    "        # cluster_h = data['depth'].iloc[i+1] - data['depth'].iloc[i-2]\n",
    "        cluster_h = data['depth'].iloc[i] - data['depth'].iloc[i-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax.add_patch(rectangle)\n",
    "    data = dataset[dataset.well == wellname]\n",
    "    top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':data['cluster'].iloc[0],'depth':[0]})\n",
    "    data = pd.concat([top, data])\n",
    "    # print(data)\n",
    "    fig, ax = plt.subplots(figsize=(2,4))\n",
    "    ax.plot(data['phit'], data['depth'], drawstyle='steps-post', color='black', lw=0.75, alpha=0.75)\n",
    "    ax.set_xlim(0, 0.35)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(wellname)\n",
    "    ax.grid()\n",
    "    for i in range(len(data)):\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 0:\n",
    "            clusters_rectangle(data, i, 'blue')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 1:\n",
    "            clusters_rectangle(data, i, 'green')\n",
    "        if data['phit'].iloc[i] > 0 and data['cluster'].iloc[i] == 2:\n",
    "            clusters_rectangle(data, i, 'red')\n",
    "clusters_display_into_well_tpb3(tpb_gmm_v3, 'C16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "# ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']\n",
    "coloring_clusters_matrix_tpb3(tpb_gmm_v3, ['A'], 4, 9, 'kmeans3_tpb', 'dontprint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_gmm_v3\n",
    "phit0 = dataset[dataset.cluster==0][['phit', 'cluster']]\n",
    "phit1 = dataset[dataset.cluster==1][['phit', 'cluster']]\n",
    "phit2 = dataset[dataset.cluster==2][['phit', 'cluster']]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "phit_tpb3 = pd.concat([phit0, phit1, phit2])\n",
    "ax = sns.histplot(data=phit_tpb3, x='phit', hue='cluster', bins=35, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "ax.set_xticks(np.arange(0.12, 0.32, 0.02));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_gmm_v3\n",
    "ntg0 = dataset[dataset.cluster==0][['ntg', 'cluster']]\n",
    "ntg1 = dataset[dataset.cluster==1][['ntg', 'cluster']]\n",
    "ntg2 = dataset[dataset.cluster==2][['ntg', 'cluster']]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "ntg_tpb_v3 = pd.concat([ntg0, ntg1, ntg2])\n",
    "ax = sns.histplot(data=ntg_tpb_v3, x='ntg', hue='cluster', bins=35, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_gmm_v3\n",
    "htst0 = dataset[dataset.cluster==0]\n",
    "htst1 = dataset[dataset.cluster==1]\n",
    "htst2 = dataset[dataset.cluster==2]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "htst_tpb_v3 = pd.concat([htst0, htst1, htst2])\n",
    "ax = sns.histplot(data=htst_tpb_v3, x='htst', hue='cluster', bins=30, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x')\n",
    "# ax.set_xlim(0,30);\n",
    "ax.set_xticks(np.arange(0, 60, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tpb_gmm_v3\n",
    "vsh0 = dataset[dataset.cluster==0]\n",
    "vsh1 = dataset[dataset.cluster==1]\n",
    "vsh2 = dataset[dataset.cluster==2]\n",
    "custom_palette = {2: 'red', 1: 'green', 0: 'blue'}\n",
    "vsh_tpb_v3 = pd.concat([vsh0, vsh1, vsh2])\n",
    "ax = sns.histplot(data=vsh_tpb_v3, x='vsh', hue='cluster', bins=50, kde=True, palette=custom_palette)\n",
    "ax.grid(True, axis='x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gmm map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_data_to_map(dataset, dataset_coord, comment):\n",
    "    def dataset_groupby(dataset):\n",
    "        result = dataset.groupby(['well','cluster'])['htst'].sum().reset_index()\n",
    "        return result\n",
    "    tpb_test_v3_piechart = dataset_groupby(dataset)\n",
    "\n",
    "    def cluster_transpose(dataset, wellname):\n",
    "        result = dataset[dataset.well==wellname]\n",
    "        result.loc[result.cluster == 0, 'cluster_0'] = result.htst\n",
    "        result.loc[result.cluster == 1, 'cluster_1'] = result.htst\n",
    "        result.loc[result.cluster == 2, 'cluster_2'] = result.htst\n",
    "        result.fillna(0)\n",
    "        result = result.groupby('well').sum().reset_index()\n",
    "        result = result[['well', 'cluster_0', 'cluster_1','cluster_2']]\n",
    "        return result\n",
    "    df_lst = []\n",
    "    for wellname in tpb_test_v3_piechart.well.unique():\n",
    "        df = cluster_transpose(tpb_test_v3_piechart, wellname)\n",
    "        df_lst.append(df)\n",
    "    data_transpose = pd.concat(df_lst).reset_index(drop=True)\n",
    "\n",
    "    def coordinates_calc(dataset_coord, fm):\n",
    "        dataset_coord = dataset_coord[dataset_coord.FORMATION_up == fm]\n",
    "        result = dataset_coord.groupby('well')[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        return result \n",
    "    coord = coordinates_calc(dataset_coord, 'Balakhany VIII')\n",
    "    data_transpose_coord = data_transpose.set_index('well').join(coord.set_index('well'), rsuffix='_coord').reset_index()\n",
    "\n",
    "    def piechart_map(dataset_map):\n",
    "        fig, ax = plt.subplots(figsize=(13,13))\n",
    "        for ind, row in dataset_map.iterrows():\n",
    "                ax.pie([row['cluster_0'], row['cluster_1'], row['cluster_2']], \n",
    "                        radius=0.3, center=(row['X_mean']/1000, row['Y_mean']/1000), wedgeprops={\"linewidth\": 0.5, \"edgecolor\": \"gray\", \"alpha\":0.75},\n",
    "                        colors=['blue', 'green', 'red'], frame=True)\n",
    "        plt.grid()\n",
    "        plt.title(comment)\n",
    "    piechart_map(data_transpose_coord)\n",
    "\n",
    "    return data_transpose_coord\n",
    "\n",
    "cluster_gmm = cluster_data_to_map(tpb_gmm_v3, df_bal_net2_kh, 'Gmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments with prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_data_calculation(dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "# ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "# ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "# ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "    def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "        row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "        row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "        return row_change\n",
    "    df_list = []\n",
    "    for wellname in dataset_ntd.well.unique():\n",
    "        df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "        df_list.append(df)\n",
    "    res_df_list = pd.concat(df_list)\n",
    "    result = pd.concat([dataset_ntd, res_df_list])\n",
    "    result = result.sort_values(by=['well','top_tst'])\n",
    "    result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "    return result_final\n",
    "# ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "# ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')\n",
    "\n",
    "def top_phit_bot_clustering(dataset, print_flag, path):\n",
    "    def top_phit_bot_collection_run(dataset):\n",
    "        def top_phit_bot_collection(dataset, wellname):\n",
    "            data = dataset[dataset.well == wellname]\n",
    "            data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "            data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "            data['bot_htst'] = data['not_htst']\n",
    "            data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', \n",
    "                         'top_htst','htst','bot_htst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            return data\n",
    "        df_lst = []\n",
    "        for wellname in tqdm(dataset.well.unique()):\n",
    "            res_df = top_phit_bot_collection(dataset, wellname)\n",
    "            df_lst.append(res_df)\n",
    "        top_phi_bot_cluster = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster\n",
    "    top_phi_bot_cluster = top_phit_bot_collection_run(dataset)\n",
    "\n",
    "    def top_phit_bot_ntg_run(dataset):\n",
    "        def top_phit_bot_ntg(dataset, wellname):\n",
    "            ntg = []\n",
    "            data = dataset[dataset.well == wellname].reset_index(drop=True)\n",
    "            for ind, row in data.iterrows():\n",
    "                if ind == 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst']))\n",
    "                if ind != 0:\n",
    "                    ntg.append(row['htst']/(row['bot_htst'] + row['htst'] + row['top_htst']))\n",
    "                if ind == len(data):\n",
    "                    ntg.append(row['htst']/(row['top_htst'] + row['htst']))\n",
    "            result = pd.concat([data, pd.DataFrame({'ntg':ntg})], axis=1)\n",
    "            return result\n",
    "        df_lst = []\n",
    "        for wellname in dataset.well.unique():\n",
    "            df = top_phit_bot_ntg(dataset, wellname)\n",
    "            df_lst.append(df)\n",
    "        top_phi_bot_cluster_ntg = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return top_phi_bot_cluster_ntg\n",
    "    top_phi_bot_cluster_ntg = top_phit_bot_ntg_run(top_phi_bot_cluster)\n",
    "\n",
    "    if print_flag == 'print':\n",
    "        top_phi_bot_cluster_ntg.to_csv(path, index=False)\n",
    "    else:\n",
    "        pass\n",
    "    return top_phi_bot_cluster_ntg\n",
    "# top_phi_bot_cluster8 = top_phit_bot_clustering(ntd_val_final8_clean, 'dont_print', '.\\output\\\\top_phi_bot_cluster8.csv')\n",
    "# top_phi_bot_cluster10 = top_phit_bot_clustering(ntd_val_final10_clean, 'dont_print', '.\\output\\\\top_phi_bot_cluster8.csv')\n",
    "\n",
    "cluster_top_phi_bot8_v2 = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\top_phi_bot_cluster_ntg8_with_labels.csv').drop(['Unnamed: 0.1','Unnamed: 0'], axis=1)\n",
    "cluster_top_phi_bot8_v3 = cluster_top_phi_bot8_v2.join(top_phi_bot_cluster8[['top_htst', 'bot_htst']])\n",
    "\n",
    "def dataset_for_spatial_prediction(dataset_full, dataset_cluster, offset_qty, cluster_algo, cluster_list):\n",
    "    \n",
    "    def joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list):\n",
    "        coordinates = dataset_full.groupby(['well','FORMATION_up'])[['X_mean','Y_mean']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        dataset_cluster = dataset_cluster[(dataset_cluster[cluster_algo].isin(cluster_list))]\n",
    "        result = dataset_cluster.set_index(['well','FORMATION_up']).join(coordinates.set_index(['well','FORMATION_up'])).reset_index()\n",
    "        coordinates = result[['well','FORMATION_up', 'X_mean', 'Y_mean']].groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).reset_index(drop=True)\n",
    "        return coordinates, result\n",
    "    coordinates, dataset_cluster_xy = joining_coordinates(dataset_full, dataset_cluster, cluster_algo, cluster_list)\n",
    "    coordinates = coordinates[~coordinates.well.isin(['A14Y'])]\n",
    "\n",
    "    def well_distance_calculation(coordinates, fm):\n",
    "        coordinates_fm = coordinates[coordinates.FORMATION_up == fm]\n",
    "        df_distance_fm = pd.DataFrame(euclidean_distances(coordinates_fm[['X_mean', 'Y_mean']]), columns=list(coordinates_fm.well))\n",
    "        well_name_rows = coordinates_fm.well.reset_index().drop(['index'], axis=1)\n",
    "        result = df_distance_fm.join(well_name_rows).set_index('well').reset_index()\n",
    "        return result\n",
    "    well_dist_crosstable_8 = well_distance_calculation(coordinates, 'Balakhany VIII')\n",
    "\n",
    "    def offset_well_names_dist(dataset, offset_qty):\n",
    "        df_lst = []\n",
    "        for ind in range(len(dataset.well.unique())):\n",
    "            off_well_series = dataset.iloc[ind]\n",
    "            off_well_selected = pd.DataFrame(off_well_series)[1:].sort_values(by=ind)[:offset_qty+1].T\n",
    "            off_well_selected['well'] = off_well_selected.columns[0]\n",
    "            off_well_selected = off_well_selected.drop(columns= off_well_selected.well, axis=1)\n",
    "\n",
    "            dist_titles = ['dist_' + str(num+1) for num in range(offset_qty)]\n",
    "            well_titles = ['well_' + str(num+1) for num in range(offset_qty)]\n",
    "\n",
    "            col_names = []\n",
    "            for i in range(len(off_well_selected.columns[:-1])):\n",
    "                col = off_well_selected.columns[i]\n",
    "                col_names.append(col)\n",
    "                off_well_selected = off_well_selected.rename(columns={col:dist_titles[i]})\n",
    "\n",
    "            off_well_names = pd.DataFrame(col_names).T\n",
    "            col_names = []\n",
    "            for i in range(len(off_well_names.columns)):\n",
    "                col = off_well_names.columns[i]\n",
    "                col_names.append(col)\n",
    "                off_well_names = off_well_names.rename(columns={col:well_titles[i]})\n",
    "            \n",
    "            concat_well_data = pd.concat([off_well_names.reset_index(drop=True), off_well_selected.reset_index(drop=True)], axis=1)\n",
    "            df_lst.append(concat_well_data)\n",
    "        result = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return result\n",
    "    well_dist_data8 = offset_well_names_dist(well_dist_crosstable_8, offset_qty)\n",
    "\n",
    "    def offset_wells_features_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "        df_lst = []\n",
    "        for wellname in dataset_dist.well.unique():\n",
    "            data = dataset_dist[dataset_dist.well == wellname]\n",
    "            cc = 0\n",
    "            for j in data.columns:\n",
    "                if 'well_' in j:\n",
    "                    cc += 1\n",
    "                    offset_wellname = data[j].values[0]\n",
    "                    data_cluster = dataset_clusters[(dataset_clusters.well == offset_wellname) & \n",
    "                                                        (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "                    var_name = 'phit_wavg_' + str(cc)\n",
    "                    data[var_name] = ((data_cluster['phit_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                    var_name = 'vsh_wavg_' + str(cc)\n",
    "                    data[var_name] = ((data_cluster['vsh_avg'] * data_cluster['htst']).sum()) / (data_cluster['htst'].sum())\n",
    "                    var_name = 'htst_sum_' + str(cc)\n",
    "                    data[var_name] = data_cluster['htst'].sum()                \n",
    "            df_lst.append(data)\n",
    "        result = pd.concat(df_lst).reset_index(drop=True)\n",
    "        result['FORMATION_up'] = fm\n",
    "        return result\n",
    "    well_features8 = offset_wells_features_calculation(well_dist_data8, cluster_top_phi_bot8_v3, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "\n",
    "    def target_wells_variable_calculation(dataset_dist, dataset_clusters, cluster_algo, cluster_list, fm):\n",
    "        df_lst = []\n",
    "        for wellname in dataset_dist.well.unique():\n",
    "            df = pd.DataFrame({'well': [wellname], 'FORMATION_up': [fm], 'phit_wavg_target': [0]})\n",
    "            data = dataset_clusters[(dataset_clusters.well == wellname) & \n",
    "                                    (dataset_clusters[cluster_algo].isin(cluster_list))]\n",
    "            df['phit_wavg_target'] = ((data['phit_avg'] * data['htst']).sum()) / (data['htst'].sum())\n",
    "            df_lst.append(df)\n",
    "        result = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return result\n",
    "    well_target8 = target_wells_variable_calculation(well_dist_data8, cluster_top_phi_bot8_v3, cluster_algo, cluster_list, 'Balakhany VIII')\n",
    "    \n",
    "    dataset8 = well_target8.set_index(['well','FORMATION_up']).join(well_features8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "\n",
    "    result = {'dataset8':dataset8, 'cluster_xy':dataset_cluster_xy, 'well_dist8':well_dist_data8, 'coordinates':coordinates,\n",
    "              'target8':well_target8, 'feature8':well_features8, 'dist_crosstable8':well_dist_crosstable_8}\n",
    "    return result\n",
    "\n",
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [0,1,2])['dataset8']\n",
    "dataset8.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Archiv models\n",
    "# model3 = model_preiction_split(dataset8, SVR(), 0.05, 'dont_display')\n",
    "# model6 = model_preiction_split(dataset8, SGDRegressor(random_state=42), 0.05, 'dont_display')\n",
    "# model7 = model_preiction_split(dataset8, GaussianProcessRegressor(random_state=42), 0.05, 'dont_display')\n",
    "# model8 = model_preiction_split(dataset8, DecisionTreeRegressor(random_state=42), 0.05, 'dont_display')\n",
    "# model9 = model_preiction_split(dataset8, GradientBoostingRegressor(random_state=42), 0.05, 'dont_display')\n",
    "# model11 = model_preiction_split(dataset8, MLPRegressor(random_state=1, max_iter=1000), 0.05, 'dont_display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_preiction_split(dataset, selected_model, tolerance, display_flag='display'):\n",
    "    drop_lst_X = ['well','FORMATION_up', 'well_1', 'well_2', 'well_3']\n",
    "    drop_lst_y = ['well','FORMATION_up']\n",
    "\n",
    "    X = dataset.drop('phit_wavg_target', axis=1)\n",
    "    y = dataset[['well','FORMATION_up', 'phit_wavg_target']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "    y_train_wnames = y_train[['well','FORMATION_up']].reset_index(drop=True)\n",
    "    y_test_wnames = y_test[['well','FORMATION_up']].reset_index(drop=True)\n",
    "\n",
    "    X_train = X_train.drop(drop_lst_X, axis=1)\n",
    "    X_test = X_test.drop(drop_lst_X, axis=1)\n",
    "    y_train = y_train.drop(drop_lst_y, axis=1)\n",
    "    y_test = y_test.drop(drop_lst_y, axis=1)\n",
    "\n",
    "    model = Pipeline([(\"scaler\",StandardScaler()),(\"model\", selected_model)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_train = np.array(y_train).flatten()\n",
    "    y_test = np.array(y_test).flatten()\n",
    "    train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "    train = pd.concat([y_train_wnames, train], axis=1)\n",
    "    test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "    test = pd.concat([y_test_wnames, test], axis=1)\n",
    "\n",
    "    train['up'] = train['y_orig']*(1 + tolerance)\n",
    "    train['down'] = train['y_orig']*(1 - tolerance)\n",
    "    train['qc'] = 'out'\n",
    "    train['dataset'] = 'train'\n",
    "    train.loc[(train['y_pred'] <= train.up) & (train['y_pred'] >= train.down), 'qc'] = 'in'\n",
    "    trainqc = train.qc.value_counts(normalize=True)\n",
    "\n",
    "    test['up'] = test['y_orig']*(1 + tolerance)\n",
    "    test['down'] = test['y_orig']*(1 - tolerance)\n",
    "    test['qc'] = 'out'\n",
    "    test['dataset'] = 'test'\n",
    "    test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "    testqc = test.qc.value_counts(normalize=True)\n",
    "    df = pd.concat([train, test])\n",
    "    df['y_pred'] = df['y_pred'].astype('float')\n",
    "\n",
    "    result = {'result':df, 'testqc':testqc, 'trainqc':trainqc, 'train_df':X_train.columns}\n",
    "    if display_flag == 'display':\n",
    "        print('model:\\n', model.named_steps['model'])\n",
    "        print('features dataset: \\n', list(X_train.columns))\n",
    "        print(f'result test:\\n with tolerance {tolerance} \"in\":', result['testqc']['in'].round(2))\n",
    "    else:\n",
    "        print('test \"in\":', result['testqc']['in'].round(2), model.named_steps['model'])\n",
    "    return result\n",
    "\n",
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [0,1,2])['dataset8']\n",
    "tolerance = 0.05\n",
    "model1 = model_preiction_split(dataset8, RandomForestRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model2 = model_preiction_split(dataset8, BayesianRidge(), tolerance, 'dont_display')\n",
    "model4 = model_preiction_split(dataset8, XGBRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model5 = model_preiction_split(dataset8, CatBoostRegressor(random_state=42, verbose=False), tolerance, 'dont_display')\n",
    "model10 = model_preiction_split(dataset8, AdaBoostRegressor(random_state=42), tolerance, 'dont_display')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1['train_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xplot_qc2(data, y_orig, y_pred, max_val, rng, margin, round, comment):\n",
    "    data = data.round({y_orig: round, y_pred: round})\n",
    "    ds_train = data[data.dataset == 'train']\n",
    "    ds_test = data[data.dataset == 'test']\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "    qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "    scatter_train = go.Scatter( x=ds_train['y_orig'], y=ds_train['y_pred'],\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_tr, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_train[['well',y_orig, y_pred, 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                )\n",
    "    scatter_test = go.Scatter(  x=ds_test[y_orig], y=ds_test[y_pred], \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_ts, size=6, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = data[['well', y_orig, y_pred, 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, f:%{customdata[3]}<extra></extra>\"])\n",
    "                                )\n",
    "    line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('train ds', 'test ds'))\n",
    "    fig.add_trace(scatter_train,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "    fig.add_trace(scatter_test,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "    fig.update_layout(  title_text= (comment), width=700, height=350, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "\n",
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [0,1,2])['dataset8']\n",
    "tolerance = 0.05\n",
    "model1 = model_preiction_split(dataset8, RandomForestRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model2 = model_preiction_split(dataset8, BayesianRidge(), tolerance, 'dont_display')\n",
    "model4 = model_preiction_split(dataset8, XGBRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model5 = model_preiction_split(dataset8, CatBoostRegressor(random_state=42, verbose=False), tolerance, 'dont_display')\n",
    "model10 = model_preiction_split(dataset8, AdaBoostRegressor(random_state=42), tolerance, 'dont_display')\n",
    "\n",
    "xplot_qc2(model1['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'RandomForestRegressor [0,1,2]')\n",
    "xplot_qc2(model2['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'BayesianRidge [0,1,2]')\n",
    "xplot_qc2(model4['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'XGBRegressor [0,1,2]')\n",
    "xplot_qc2(model5['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'CatBoostRegressor [0,1,2]')\n",
    "xplot_qc2(model10['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'AdaBoostRegressor [0,1,2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [0])['dataset8']\n",
    "tolerance = 0.05\n",
    "model1 = model_preiction_split(dataset8, RandomForestRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model2 = model_preiction_split(dataset8, BayesianRidge(), tolerance, 'dont_display')\n",
    "model4 = model_preiction_split(dataset8, XGBRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model5 = model_preiction_split(dataset8, CatBoostRegressor(random_state=42, verbose=False), tolerance, 'dont_display')\n",
    "model10 = model_preiction_split(dataset8, AdaBoostRegressor(random_state=42), tolerance, 'dont_display')\n",
    "\n",
    "xplot_qc2(model1['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'RandomForestRegressor [0]')\n",
    "xplot_qc2(model2['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'BayesianRidge [0]')\n",
    "xplot_qc2(model4['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'XGBRegressor [0]')\n",
    "xplot_qc2(model5['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'CatBoostRegressor [0]')\n",
    "xplot_qc2(model10['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'AdaBoostRegressor [0]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [1])['dataset8']\n",
    "tolerance = 0.05\n",
    "model1 = model_preiction_split(dataset8, RandomForestRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model2 = model_preiction_split(dataset8, BayesianRidge(), tolerance, 'dont_display')\n",
    "model4 = model_preiction_split(dataset8, XGBRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model5 = model_preiction_split(dataset8, CatBoostRegressor(random_state=42, verbose=False), tolerance, 'dont_display')\n",
    "model10 = model_preiction_split(dataset8, AdaBoostRegressor(random_state=42), tolerance, 'dont_display')\n",
    "\n",
    "xplot_qc2(model1['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'RandomForestRegressor [1]')\n",
    "xplot_qc2(model2['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'BayesianRidge [1]')\n",
    "xplot_qc2(model4['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'XGBRegressor [1]')\n",
    "xplot_qc2(model5['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'CatBoostRegressor [1]')\n",
    "xplot_qc2(model10['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'AdaBoostRegressor [1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset8 = dataset_for_spatial_prediction(df_bal_net2_kh, cluster_top_phi_bot8_v3, 3, 'kmeans_3_labels', [2])['dataset8']\n",
    "tolerance = 0.05\n",
    "model1 = model_preiction_split(dataset8, RandomForestRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model2 = model_preiction_split(dataset8, BayesianRidge(), tolerance, 'dont_display')\n",
    "model4 = model_preiction_split(dataset8, XGBRegressor(n_jobs=-1, random_state=42), tolerance, 'dont_display')\n",
    "model5 = model_preiction_split(dataset8, CatBoostRegressor(random_state=42, verbose=False), tolerance, 'dont_display')\n",
    "model10 = model_preiction_split(dataset8, AdaBoostRegressor(random_state=42), tolerance, 'dont_display')\n",
    "\n",
    "xplot_qc2(model1['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'RandomForestRegressor [2]')\n",
    "xplot_qc2(model2['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'BayesianRidge [2]')\n",
    "xplot_qc2(model4['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'XGBRegressor [2]')\n",
    "xplot_qc2(model5['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'CatBoostRegressor [2]')\n",
    "xplot_qc2(model10['result'], 'y_orig', 'y_pred', 0.3, tolerance, 0, 3, 'AdaBoostRegressor [2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering top_phi_bot cross-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntd_val_clustering_top_phi_bot = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering_one_layer_with_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "tpb_crosscluster = ntd_val_clustering_top_phi_bot.copy()\n",
    "ntd_val_clustering_top_phi_bot.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb_crosscluster.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_collecting_clusters_top_phi_bot(dataset, clustering):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        topbot_htst_lst = []\n",
    "        cluster_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            cluster_lst.append(np.nan)\n",
    "            cluster_lst.append(row[clustering])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "            topbot_htst_lst.append(0)\n",
    "            topbot_htst_lst.append((row['top_htst'] + row['bot_htst']))\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        cluster_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, topbot_htst_lst, cluster_lst ), columns=['well','phit', 'htst', 'topbot_htst','cluster'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "tpb_crosscluster_kmeans = well_collecting_clusters_top_phi_bot(tpb_crosscluster, 'kmeans_3_labels')\n",
    "tpb_crosscluster_gmm = well_collecting_clusters_top_phi_bot(tpb_crosscluster, 'gmm_3_labels')\n",
    "\n",
    "tpb_crosscluster_kmeans = tpb_crosscluster_kmeans.rename(columns = {'cluster':'cluster_kmeans'}).reset_index(drop=True)\n",
    "tpb_crosscluster_gmm = tpb_crosscluster_gmm.rename(columns = {'cluster':'cluster_gmm'}).reset_index(drop=True)\n",
    "tpb_crosscluster_final = tpb_crosscluster_kmeans.join(tpb_crosscluster_gmm['cluster_gmm'])\n",
    "tpb_crosscluster_final = tpb_crosscluster_final[['well', 'phit', 'htst', 'topbot_htst', 'depth', 'cluster_kmeans', 'cluster_gmm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb_crosscluster_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_big = tpb_crosscluster_final[  (tpb_crosscluster_final.cluster_kmeans == 0) & \n",
    "                                    (tpb_crosscluster_final.cluster_gmm == 0) &\n",
    "                                    (tpb_crosscluster_final.htst >= 12.5) &\n",
    "                                    (tpb_crosscluster_final.phit >=0.21)]\n",
    "green = tpb_crosscluster_final[     (tpb_crosscluster_final.cluster_gmm == 1) &\n",
    "                                    (tpb_crosscluster_final.topbot_htst >=7.5)]\n",
    "red = tpb_crosscluster_final[       (tpb_crosscluster_final.cluster_kmeans == 2) &\n",
    "                                    (tpb_crosscluster_final.topbot_htst < 7.5) &\n",
    "                                    (tpb_crosscluster_final.phit < 0.21)]\n",
    "all = tpb_crosscluster_final[tpb_crosscluster_final.phit != 0]\n",
    "plt.scatter(blue_big['phit'], blue_big['htst'], alpha=0.75, ec='black', zorder=4)\n",
    "plt.scatter(green['phit'], green['htst'], c='green', alpha=0.75, ec='black',zorder=3)\n",
    "plt.scatter(red['phit'], red['htst'], c='red', alpha=0.75, ec='black',zorder=2)\n",
    "plt.scatter(all['phit'], all['htst'], c='gray', alpha=0.1, ec='black', zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpb_crosscluster_final['cluster'] = np.nan\n",
    "tpb_crosscluster_final.loc[ ((tpb_crosscluster_final.cluster_kmeans == 0) & \n",
    "                            (tpb_crosscluster_final.cluster_gmm == 0) &\n",
    "                            (tpb_crosscluster_final.htst >= 12.5) &\n",
    "                            (tpb_crosscluster_final.phit >=0.21)), 'cluster'] = 0\n",
    "tpb_crosscluster_final.loc[ ((tpb_crosscluster_final.cluster_kmeans == 1) & \n",
    "                            (tpb_crosscluster_final.cluster_gmm == 1) &\n",
    "                            (tpb_crosscluster_final.htst >= 7.5)), 'cluster'] = 1\n",
    "tpb_crosscluster_final.loc[ ((tpb_crosscluster_final.cluster_kmeans == 2) & \n",
    "                            (tpb_crosscluster_final.topbot_htst < 7.5) &\n",
    "                            (tpb_crosscluster_final.phit < 0.21)), 'cluster'] = 2\n",
    "tpb_crosscluster_final[tpb_crosscluster_final.well == 'A01W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coloring_clusters_matrix_tpb3(dataset, letters_list, rows, columns, clustering, output_flag):\n",
    "    def clusters_rectangle(data, k, color):\n",
    "        # cluster_xy = data['depth'].iloc[k-2]\n",
    "        cluster_xy = data['depth'].iloc[k-1]\n",
    "        # cluster_h = data['depth'].iloc[k+1] - data['depth'].iloc[k-2]\n",
    "        cluster_h = data['depth'].iloc[k] - data['depth'].iloc[k-1]\n",
    "        rectangle = patches.Rectangle((0, cluster_xy) , 1, cluster_h, edgecolor=color, facecolor=color, alpha=0.25)\n",
    "        ax[j,i].add_patch(rectangle)\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit':[0], 'htst':[0], 'cluster':welldata['cluster'].iloc[0],'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=0.75)\n",
    "                    ax[j,i].set_xlim(0, 0.35)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)\n",
    "                    ax[j,i].grid()\n",
    "                    for k in range(len(welldata)):\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 0:\n",
    "                            clusters_rectangle(welldata, k, 'blue')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 1:\n",
    "                            clusters_rectangle(welldata, k, 'green')\n",
    "                        if welldata['phit'].iloc[k] > 0 and welldata['cluster'].iloc[k] == 2:\n",
    "                            clusters_rectangle(welldata, k, 'red')\n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "        if output_flag == 'print':\n",
    "            plt.savefig('.\\plots\\\\clustering_wells_tpb\\\\' + clustering + '_' + str(letter) +'.png')\n",
    "        else:\n",
    "            pass\n",
    "coloring_clusters_matrix_tpb3(tpb_crosscluster_final, ['A','B','C'], 4, 9, 'cluster_cross', 'dontprint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering whole well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading clustering data whole well from Farid\n",
    "ntd_val_clust_wholewell_init = pd.read_csv('.\\inputoutput\\\\ntd_val_clustering_wholewell_stride_with_labels.csv').drop('Unnamed: 0', axis=1)\n",
    "# ntd_val_clust_wholewell_init.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntd_val_clust_wholewell = ntd_val_clust_wholewell_init[['phit_1', 'phit_2', 'phit_3', 'phit_4', 'phit_5', 'phit_6', 'phit_7',\n",
    "                                                        'phit_8', 'phit_9', 'phit_10', 'phit_11', 'phit_12', 'phit_13',\n",
    "                                                        'phit_14', 'htst_1', 'htst_2', 'htst_3', 'htst_4', 'htst_5', 'htst_6',\n",
    "                                                        'htst_7', 'htst_8', 'htst_9', 'htst_10', 'htst_11', 'htst_12',\n",
    "                                                        'htst_13', 'htst_14', 'not_htst_1', 'not_htst_2', 'not_htst_3',\n",
    "                                                        'not_htst_4', 'not_htst_5', 'not_htst_6', 'not_htst_7', 'not_htst_8',\n",
    "                                                        'not_htst_9', 'not_htst_10', 'not_htst_11', 'not_htst_12',\n",
    "                                                        'not_htst_13', 'not_htst_14', 'well', 'FORMATION_up', 'kmeans_3_labels',\n",
    "                                                        'kmeans_4_labels', 'kmeans_5_labels']]\n",
    "xy_bal8_init = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up=='Balakhany VIII'][['well','FORMATION_up','X_mean','Y_mean']]\n",
    "xy_bal8 = xy_bal8_init.groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "ntd_val_clust_wholewell_xy = ntd_val_clust_wholewell.set_index(['well','FORMATION_up']).join(xy_bal8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "ntd_val_clust_wholewell_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,10))\n",
    "data = ntd_val_clust_wholewell_xy\n",
    "color_dict = {0: 'blue', 1: 'green', 2: 'red'}\n",
    "colors = [color_dict[val] for val in data['kmeans_3_labels']]\n",
    "ax.scatter(data['X_mean'], data['Y_mean'], c=colors, marker='o', s=150, edgecolors='gray', alpha=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction based on layering plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_bal_net2_kh\n",
    "def clustering_data_calculation(dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    df_net2_bal8 = dataset[[    'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[[   'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    def ntd_calculation_brief(dataset,well,desired_fm, net_var):\n",
    "        data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        tops = zip(tst_top, tst_bot)\n",
    "        df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "        df_htst['FORMATION_up'] = desired_fm\n",
    "        df_htst['well'] = well\n",
    "        df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "        df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "        return df_htst\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "\n",
    "    print('Calculation values for NTD Bal8 and Bal10')\n",
    "    def ntd_properties_dataframe(dataset_ntd, dataset_logs, fmname):\n",
    "        well_data = []\n",
    "        well_formation = fmname\n",
    "        df_lst = []\n",
    "        for well in tqdm(dataset_ntd.well.unique()[:]):\n",
    "            ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "            well_avgprop_sel = dataset_logs[(dataset_logs.well==well)]\n",
    "            fm_top = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[0]\n",
    "            fm_bot = dataset_logs[(dataset_logs.well==well)]['TST'].iloc[-1]\n",
    "            well_phit = []\n",
    "            well_vsh = []\n",
    "            well_gperm = []\n",
    "            well_top = []\n",
    "            well_bot = []\n",
    "            well_h = []\n",
    "            well_fm_top = []\n",
    "            well_fm_bot = []\n",
    "            well_name = []\n",
    "            well_fm = []\n",
    "            for layers in range(len(ntd_well_avgprop.well)):\n",
    "                ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "                ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "                ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "                phit_lst = []\n",
    "                vsh_lst = []\n",
    "                perm_lst = []\n",
    "                for depth in range(len(well_avgprop_sel.TST)):\n",
    "                    well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                    if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                        phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                        vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                        perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "                well_name.append(well)\n",
    "                well_fm.append(well_formation)\n",
    "                well_phit.append(mean(phit_lst))\n",
    "                well_vsh.append(mean(vsh_lst))\n",
    "                well_gperm.append(gmean(perm_lst))\n",
    "                well_h.append(ntd_h)\n",
    "                well_top.append(ntd_top)\n",
    "                well_bot.append(ntd_bot)\n",
    "                well_fm_top.append(fm_top)\n",
    "                well_fm_bot.append(fm_bot)\n",
    "                well_data = zip(well_name,well_fm,well_phit, well_vsh, well_gperm, well_h, well_top, well_bot, well_fm_top, well_fm_bot)\n",
    "                well_df = pd.DataFrame(well_data, columns=[ 'well','FORMATION_up',        \n",
    "                                                            'phit_avg',\n",
    "                                                            'vsh_avg', \n",
    "                                                            'perm_avg',\n",
    "                                                            'htst',\n",
    "                                                            'top_tst',\n",
    "                                                            'bot_tst',\n",
    "                                                            'fm_top_tst',\n",
    "                                                            'fm_bot_tst'])\n",
    "                well_df['not_htst'] = well_df['top_tst'].shift(-1)-well_df['bot_tst']\n",
    "                well_df = well_df[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'perm_avg', 'htst', 'not_htst','top_tst', 'bot_tst', 'fm_top_tst', 'fm_bot_tst']]\n",
    "            df_lst.append(well_df)\n",
    "        result = pd.concat(df_lst)\n",
    "        return result\n",
    "    ntd_val_bal8 = ntd_properties_dataframe(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    ntd_val_bal10 = ntd_properties_dataframe(ntd_net2_10, df_net2_bal10, 'Balakhany X')\n",
    "    ntd_val_final = pd.concat([ntd_val_bal8, ntd_val_bal10])\n",
    "    return ntd_val_final\n",
    "ntd_val_final = clustering_data_calculation(df_bal_net2_kh)\n",
    "ntd_val_final8 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany VIII']\n",
    "ntd_val_final10 = ntd_val_final[ntd_val_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "def nothtst_nan_fill(dataset_ntd, fmname):\n",
    "    def nan_change_diff_fmbottom(dataset, wellname, fmname):\n",
    "        row_change = dataset[(dataset.well == wellname) & (dataset.FORMATION_up == fmname) & (dataset.not_htst.isna())]\n",
    "        row_change['not_htst'] = row_change['fm_bot_tst'] - row_change['bot_tst']\n",
    "        return row_change\n",
    "    df_list = []\n",
    "    for wellname in dataset_ntd.well.unique():\n",
    "        df = nan_change_diff_fmbottom(dataset_ntd, wellname, fmname)\n",
    "        df_list.append(df)\n",
    "    res_df_list = pd.concat(df_list)\n",
    "    result = pd.concat([dataset_ntd, res_df_list])\n",
    "    result = result.sort_values(by=['well','top_tst'])\n",
    "    result_final = result.dropna(subset=['not_htst'], axis=0)\n",
    "    return result_final\n",
    "ntd_val_final8_clean = nothtst_nan_fill(ntd_val_final8, 'Balakhany VIII')\n",
    "ntd_val_final10_clean = nothtst_nan_fill(ntd_val_final10, 'Balakhany X')\n",
    "\n",
    "def top_phit_bot_clustering(dataset, wellname):\n",
    "    data = dataset[dataset.well == wellname]\n",
    "    data['top_htst'] = data['top_tst'] - data['fm_top_tst']\n",
    "    data['top_htst'].iloc[1:] = data['not_htst'].iloc[:-1]\n",
    "    data['bot_htst'] = data['not_htst']\n",
    "    data['depth'] = data['top_htst'] + data['htst']/2\n",
    "    data['depth'] = data['depth'].cumsum()\n",
    "    data = data[['well', 'FORMATION_up', 'phit_avg', 'vsh_avg', 'top_htst','htst','bot_htst', 'depth']]\n",
    "    return data\n",
    "df_lst = []\n",
    "for wellname in tqdm(ntd_val_final8.well.unique()):\n",
    "    res_df8 = top_phit_bot_clustering(ntd_val_final8_clean, wellname)\n",
    "    df_lst.append(res_df8)\n",
    "top_phi_bot_cluster = pd.concat(df_lst).reset_index().drop('index', axis=1)\n",
    "top_phi_bot_cluster.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_bal8_init = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up=='Balakhany VIII'][['well','FORMATION_up','X_mean','Y_mean']]\n",
    "xy_bal8 = xy_bal8_init.groupby(['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "top_phi_bot8_xy = top_phi_bot_cluster.set_index(['well','FORMATION_up']).join(xy_bal8.set_index(['well','FORMATION_up'])).reset_index()\n",
    "top_phi_bot8_xy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction - base line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype == 'string' or dataset[col].dtype == 'object':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "training_data_tpb8 = top_phi_bot8_xy.copy(deep=True)\n",
    "training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                        'FORMATION_up',\n",
    "                                        'X_mean', 'Y_mean',\n",
    "                                        'phit_avg', \n",
    "                                        'vsh_avg',\n",
    "                                        'htst',\n",
    "                                        'depth',\n",
    "                                        ]]\n",
    "def clusters_joining(ntd_val_kmeans3, training_data_tpb):\n",
    "    cluster_nums = ntd_val_kmeans3[(ntd_val_kmeans3.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "    cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "    cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "    training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "    training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "    training_data_tpb_gm = training_data_tpb_gm[training_data_tpb_gm.cluster.notna()]\n",
    "    return training_data_tpb_gm\n",
    "training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "training_data_tpb_corr, training_data_tpb = join_df_prediction(training_data_tpb_gm, 'phit_avg')\n",
    "pairplot_special(training_data_tpb_corr, 7, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tpb_notcl = training_data_tpb.drop('cluster', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-split block to get an optimized hyper parameters for 1-to-all function\n",
    "gs_set = {  'bootstrap': [True, False],\n",
    "            'max_depth': [10, 30, 100, 150],\n",
    "            'min_samples_leaf': [2, 5, 10],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'n_estimators': [100, 150, 200]}\n",
    "scorer = make_scorer(r2, greater_is_better=True)\n",
    "hyperdict = run_rfr_train_test_split(training_data_tpb_notcl, gs_set, scorer, 'phit_avg', \n",
    "                                     0.05, 0, 'clusters_baseline.txt', 'clusters_baseline_grid_search', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tpb_notcl = training_data_tpb_notcl.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1-to-all block to to predict target vaiable\n",
    "phit_pred_cluster_baseline = run_rfr_1_to_all(training_data_tpb_notcl, hyperdict, 'phit_avg', \n",
    "                                         0.05, 0, 'clusters_baseline.txt', 'clusters_baseline_1-to-all', 1, 0.3, 0)\n",
    "phit_pred_cluster_baseline =  phit_pred_cluster_baseline.rename(columns={'predict':'phit_cluster_bl'})[\n",
    "                                            ['well','FORMATION_up','phit_cluster_bl','qc']].sort_values(\n",
    "                                            by=['well','FORMATION_up']).reset_index().drop('index', axis=1)\n",
    "# Result 0.511 v/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred_bl_joining(top_phi_bot_xy, phit_pred_cluster_baseline):\n",
    "    add_info1 = top_phi_bot_xy[['well','phit_avg','top_htst', 'htst', 'bot_htst']]\n",
    "    add_info1['phit_avg'] = add_info1['phit_avg'].round(5)\n",
    "    phit_pred_cluster_bl_collect = phit_pred_cluster_baseline.join(training_data_tpb_notcl.drop(['well','FORMATION_up','htst'],axis=1))\n",
    "    phit_cluster_orig_pred = phit_pred_cluster_bl_collect.set_index(['well','phit_avg']).join(add_info1.set_index(['well','phit_avg'])).reset_index()\n",
    "    return phit_cluster_orig_pred\n",
    "def phit_orig_pred_bl_collecting(dataset):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        phit_pred_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            phit_pred_lst.append(np.nan)\n",
    "            phit_pred_lst.append(row['phit_cluster_bl'])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        phit_pred_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, phit_pred_lst ), \n",
    "                                            columns=['well','phit_orig', 'htst', 'phit_pred'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "phit_cluster_orig_pred = phit_orig_pred_bl_joining(top_phi_bot8_xy, phit_pred_cluster_baseline)\n",
    "phit_orig_pred_cluster_bl = phit_orig_pred_bl_collecting(phit_cluster_orig_pred)\n",
    "\n",
    "# ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']\n",
    "def coloring_clusters_matrix_plot(dataset, letters_list, rows, columns):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit_orig':[0], \n",
    "                                           'htst':[0], 'phit_pred':[0], 'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit_orig'], welldata['depth'], drawstyle='steps-post', color='gray', alpha=1, lw=1.5)\n",
    "                    ax[j,i].plot(welldata['phit_pred'], welldata['depth'], drawstyle='steps-post', color='black', alpha=1, lw=2)\n",
    "                    ax[j,i].set_xlim(0.1, 0.3)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)   \n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "coloring_clusters_matrix_plot(phit_orig_pred_cluster_bl, ['A'], 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction by clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clusters 0,1,2 - convolution to points + offsets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_cluster(dataset, dataset_clustering, cluster_num):\n",
    "    def conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num):\n",
    "        training_data_tpb8 = dataset.copy(deep=True)\n",
    "        training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                                'FORMATION_up',\n",
    "                                                'X_mean', 'Y_mean',\n",
    "                                                'phit_avg', \n",
    "                                                'vsh_avg',\n",
    "                                                'top_htst',\n",
    "                                                'htst',\n",
    "                                                'bot_htst',\n",
    "                                                'depth',\n",
    "                                                ]]\n",
    "        def clusters_joining(dataset_clustering, training_data_tpb):\n",
    "            cluster_nums = dataset_clustering[(dataset_clustering.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "            cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "            cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "            training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "            training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "            return training_data_tpb_gm\n",
    "        training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "        training_data_tpb_gm0 = training_data_tpb_gm[training_data_tpb_gm.cluster.isin(cluster_num)]\n",
    "\n",
    "        def closest_wells_per_cluster(training_data_tpb_gm0):\n",
    "            def dist_prop_calc_xy(dataset, dist_formation, dist_cutoff, value):\n",
    "                \"\"\"\n",
    "                dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "                \"\"\"\n",
    "                data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "                row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "                distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean']]), columns=list(data.well))\n",
    "                distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "                distance_fm_well.reset_index(inplace=True)\n",
    "                def well_kh_accum(wells, dataset, kh_formation):\n",
    "                    well_kh_accum = []\n",
    "                    well_x_accum = []\n",
    "                    well_y_accum = []\n",
    "                    for i in wells:\n",
    "                        well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "                        well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "                        well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "                    well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "                    well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "                    well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "                    well_x3.columns = ['x1','x2','x3']\n",
    "                    well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "                    well_y3.columns = ['y1','y2','y3']\n",
    "                    final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                                        well_x3.reset_index().drop('index',axis=1), \n",
    "                                        well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "                    return final\n",
    "                df_collect = []\n",
    "                for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "                    well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "                    well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "                    well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "                    well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "                    well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "                    well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "                    well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "                    well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "                    concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "                    result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "                    df_collect.append(result)     \n",
    "                df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "                df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "                return df_well_kh_dist\n",
    "            tpb8_cluster0 = training_data_tpb_gm0.groupby('well').apply(lambda x: x.iloc[0]).drop('well',axis=1).reset_index()\n",
    "            list_well_offsets = dist_prop_calc_xy(tpb8_cluster0, 'Balakhany VIII', 0, 'phit_avg')[\n",
    "                            ['well','FORMATION_up','well1', 'well2', 'well3','dist1', 'dist2', 'dist3']]\n",
    "            return list_well_offsets\n",
    "        list_wells_offsets0 = closest_wells_per_cluster(training_data_tpb_gm0)\n",
    "\n",
    "        def loop_cluster_data_to_sample(dataset, list_wells_offsets_cluster, wellname):\n",
    "            target_train = dataset[dataset.well == wellname]\n",
    "            offset_wells = list_wells_offsets_cluster[list_wells_offsets_cluster.well == wellname]\n",
    "\n",
    "            ftr1 = dataset[dataset.well == offset_wells.well1.iloc[0]]\n",
    "            ftr2 = dataset[dataset.well == offset_wells.well2.iloc[0]]\n",
    "            ftr3 = dataset[dataset.well == offset_wells.well3.iloc[0]]\n",
    "\n",
    "            def conv_cluster_target_to_sample(dataset):\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg_target': [phit_wavg_cl]})\n",
    "                return result\n",
    "\n",
    "            target_wavg = conv_cluster_target_to_sample(target_train)\n",
    "\n",
    "            def conv_cluster_feature_to_sample(dataset, offset_wells, well_seq, num):\n",
    "                suff = '_' + str(num)\n",
    "                dist = offset_wells[well_seq].iloc[0]\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                def summ_h(x1,x2):\n",
    "                    result = x1+x2\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                dataset['vshh'] =  dataset.apply(lambda x: w_avg(x['vsh_avg'], x['htst']), axis=1)\n",
    "                dataset['sumh'] =  dataset.apply(lambda x: summ_h(x['top_htst'], x['htst']), axis=1)\n",
    "                vsh_wavg_cl = dataset['vshh'].sum() / dataset['htst'].sum()\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                gross = dataset['sumh'].sum() + dataset['bot_htst'].iloc[-1]\n",
    "                net = dataset['htst'].sum()\n",
    "                ntg = net/gross\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg' + suff: [phit_wavg_cl],\n",
    "                                        'net' + suff:[net], \n",
    "                                        'gross' + suff:[gross], \n",
    "                                        'ntg' + suff:[ntg], \n",
    "                                        'vsh_wavg' + suff:[vsh_wavg_cl], \n",
    "                                        'dist' + suff:dist})\n",
    "                return result\n",
    "\n",
    "            ftr1_wavg = conv_cluster_feature_to_sample(ftr1, offset_wells, 'dist1', 1)\n",
    "            ftr2_wavg = conv_cluster_feature_to_sample(ftr2, offset_wells, 'dist2', 2)\n",
    "            ftr3_wavg = conv_cluster_feature_to_sample(ftr3, offset_wells, 'dist3', 3)\n",
    "\n",
    "            return target_wavg, ftr1_wavg, ftr2_wavg, ftr3_wavg\n",
    "        df_lst = []\n",
    "        for wellname in training_data_tpb_gm0.well.unique():\n",
    "            t, f1, f2, f3 = loop_cluster_data_to_sample(training_data_tpb_gm0, list_wells_offsets0, wellname)\n",
    "            result_dataset = pd.concat([t.iloc[:,3:], f1.iloc[:,3:], f2.iloc[:,3:], f3.iloc[:,3:]], axis=1)\n",
    "            df_lst.append(result_dataset)\n",
    "        data_Xy = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return data_Xy\n",
    "    data_Xy = conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num)\n",
    "    X = data_Xy.iloc[:,1:]\n",
    "    y = data_Xy['phit_wavg_target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "    model = XGBRegressor()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "    test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "    tolerance = 0.05\n",
    "    def test_processing(tolerance):\n",
    "        test['up'] = test['y_orig']*(1 + tolerance)\n",
    "        test['down'] = test['y_orig']*(1 - tolerance)\n",
    "        test['qc'] = 'out'\n",
    "        test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "        return test\n",
    "    test = test_processing(tolerance)\n",
    "    metric = test.qc.value_counts(normalize=True)['in'].round(2)\n",
    "    def train_test_display(tolerance, min_phit):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train['y_orig'], train['y_pred'], label = 'train', ec='gray', alpha=0.5)\n",
    "        ax.scatter(test['y_orig'], test['y_pred'], label = 'test', ec='gray', alpha=0.9)\n",
    "        line = mlines.Line2D([min_phit, 0.28], [min_phit, 0.28], color='red', ls='--')\n",
    "        line1 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 + tolerance), 0.28*(1 + tolerance)], color='blue', ls='--')\n",
    "        line2 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 - tolerance), 0.28*((1 - tolerance))], color='blue', ls='--')\n",
    "        ax.add_line(line), ax.add_line(line1), ax.add_line(line2), ax.grid(), ax.legend(), ax.set_ylabel('pred'), ax.set_xlabel('orig'),\n",
    "        ax.set_xlim(0.14, 0.28), ax.set_ylim(0.14, 0.28) \n",
    "        ax.set_title(f'Cluster # {cluster_num} metric \"test in\": {metric}')\n",
    "    train_test_display(tolerance, 0.15)\n",
    "    return  data_Xy, train, test\n",
    "\n",
    "data_Xy, train, test = xgboost_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_cluster(dataset, dataset_clustering, cluster_num):\n",
    "    def conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num):\n",
    "        training_data_tpb8 = dataset.copy(deep=True)\n",
    "        training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                                'FORMATION_up',\n",
    "                                                'X_mean', 'Y_mean',\n",
    "                                                'phit_avg', \n",
    "                                                'vsh_avg',\n",
    "                                                'top_htst',\n",
    "                                                'htst',\n",
    "                                                'bot_htst',\n",
    "                                                'depth',\n",
    "                                                ]]\n",
    "        def clusters_joining(dataset_clustering, training_data_tpb):\n",
    "            cluster_nums = dataset_clustering[(dataset_clustering.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "            cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "            cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "            training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "            training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "            return training_data_tpb_gm\n",
    "        training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "        training_data_tpb_gm0 = training_data_tpb_gm[training_data_tpb_gm.cluster.isin(cluster_num)]\n",
    "\n",
    "        def closest_wells_per_cluster(training_data_tpb_gm0):\n",
    "            def dist_prop_calc_xy(dataset, dist_formation, dist_cutoff, value):\n",
    "                \"\"\"\n",
    "                dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "                \"\"\"\n",
    "                data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "                row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "                distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean']]), columns=list(data.well))\n",
    "                distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "                distance_fm_well.reset_index(inplace=True)\n",
    "                def well_kh_accum(wells, dataset, kh_formation):\n",
    "                    well_kh_accum = []\n",
    "                    well_x_accum = []\n",
    "                    well_y_accum = []\n",
    "                    for i in wells:\n",
    "                        well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "                        well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "                        well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "                    well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "                    well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "                    well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "                    well_x3.columns = ['x1','x2','x3']\n",
    "                    well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "                    well_y3.columns = ['y1','y2','y3']\n",
    "                    final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                                        well_x3.reset_index().drop('index',axis=1), \n",
    "                                        well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "                    return final\n",
    "                df_collect = []\n",
    "                for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "                    well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "                    well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "                    well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "                    well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "                    well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "                    well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "                    well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "                    well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "                    concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "                    result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "                    df_collect.append(result)     \n",
    "                df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "                df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "                return df_well_kh_dist\n",
    "            tpb8_cluster0 = training_data_tpb_gm0.groupby('well').apply(lambda x: x.iloc[0]).drop('well',axis=1).reset_index()\n",
    "            list_well_offsets = dist_prop_calc_xy(tpb8_cluster0, 'Balakhany VIII', 0, 'phit_avg')[\n",
    "                            ['well','FORMATION_up','well1', 'well2', 'well3','dist1', 'dist2', 'dist3']]\n",
    "            return list_well_offsets\n",
    "        list_wells_offsets0 = closest_wells_per_cluster(training_data_tpb_gm0)\n",
    "\n",
    "        def loop_cluster_data_to_sample(dataset, list_wells_offsets_cluster, wellname):\n",
    "            target_train = dataset[dataset.well == wellname]\n",
    "            offset_wells = list_wells_offsets_cluster[list_wells_offsets_cluster.well == wellname]\n",
    "\n",
    "            ftr1 = dataset[dataset.well == offset_wells.well1.iloc[0]]\n",
    "            ftr2 = dataset[dataset.well == offset_wells.well2.iloc[0]]\n",
    "            ftr3 = dataset[dataset.well == offset_wells.well3.iloc[0]]\n",
    "\n",
    "            def conv_cluster_target_to_sample(dataset):\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg_target': [phit_wavg_cl]})\n",
    "                return result\n",
    "\n",
    "            target_wavg = conv_cluster_target_to_sample(target_train)\n",
    "\n",
    "            def conv_cluster_feature_to_sample(dataset, offset_wells, well_seq, num):\n",
    "                suff = '_' + str(num)\n",
    "                dist = offset_wells[well_seq].iloc[0]\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                def summ_h(x1,x2):\n",
    "                    result = x1+x2\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                dataset['vshh'] =  dataset.apply(lambda x: w_avg(x['vsh_avg'], x['htst']), axis=1)\n",
    "                dataset['sumh'] =  dataset.apply(lambda x: summ_h(x['top_htst'], x['htst']), axis=1)\n",
    "                vsh_wavg_cl = dataset['vshh'].sum() / dataset['htst'].sum()\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                gross = dataset['sumh'].sum() + dataset['bot_htst'].iloc[-1]\n",
    "                net = dataset['htst'].sum()\n",
    "                ntg = net/gross\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg' + suff: [phit_wavg_cl],\n",
    "                                        'net' + suff:[net], \n",
    "                                        'gross' + suff:[gross], \n",
    "                                        'ntg' + suff:[ntg], \n",
    "                                        'vsh_wavg' + suff:[vsh_wavg_cl], \n",
    "                                        'dist' + suff:dist})\n",
    "                return result\n",
    "\n",
    "            ftr1_wavg = conv_cluster_feature_to_sample(ftr1, offset_wells, 'dist1', 1)\n",
    "            ftr2_wavg = conv_cluster_feature_to_sample(ftr2, offset_wells, 'dist2', 2)\n",
    "            ftr3_wavg = conv_cluster_feature_to_sample(ftr3, offset_wells, 'dist3', 3)\n",
    "\n",
    "            return target_wavg, ftr1_wavg, ftr2_wavg, ftr3_wavg\n",
    "        df_lst = []\n",
    "        for wellname in training_data_tpb_gm0.well.unique():\n",
    "            t, f1, f2, f3 = loop_cluster_data_to_sample(training_data_tpb_gm0, list_wells_offsets0, wellname)\n",
    "            result_dataset = pd.concat([t.iloc[:,3:], f1.iloc[:,3:], f2.iloc[:,3:], f3.iloc[:,3:]], axis=1)\n",
    "            df_lst.append(result_dataset)\n",
    "        data_Xy = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return data_Xy\n",
    "    data_Xy = conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num)\n",
    "    X = data_Xy.iloc[:,1:]\n",
    "    y = data_Xy['phit_wavg_target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "    # model = XGBRegressor(n_jobs=-1, random_state=42)\n",
    "    model = cb.CatBoostRegressor()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "    test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "    tolerance = 0.05\n",
    "    def test_processing(tolerance):\n",
    "        test['up'] = test['y_orig']*(1 + tolerance)\n",
    "        test['down'] = test['y_orig']*(1 - tolerance)\n",
    "        test['qc'] = 'out'\n",
    "        test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "        return test\n",
    "    test = test_processing(tolerance)\n",
    "    metric = test.qc.value_counts(normalize=True)['in'].round(2)\n",
    "    def train_test_display(tolerance, min_phit):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train['y_orig'], train['y_pred'], label = 'train', ec='gray', alpha=0.5)\n",
    "        ax.scatter(test['y_orig'], test['y_pred'], label = 'test', ec='gray', alpha=0.9)\n",
    "        line = mlines.Line2D([min_phit, 0.28], [min_phit, 0.28], color='red', ls='--')\n",
    "        line1 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 + tolerance), 0.28*(1 + tolerance)], color='blue', ls='--')\n",
    "        line2 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 - tolerance), 0.28*((1 - tolerance))], color='blue', ls='--')\n",
    "        ax.add_line(line), ax.add_line(line1), ax.add_line(line2), ax.grid(), ax.legend(), ax.set_ylabel('pred'), ax.set_xlabel('orig'),\n",
    "        ax.set_xlim(0.14, 0.28), ax.set_ylim(0.14, 0.28) \n",
    "        ax.set_title(f'Cluster # {cluster_num} metric \"test in\": {metric}')\n",
    "    train_test_display(tolerance, 0.15)\n",
    "    return  data_Xy, train, test\n",
    "data_Xy, train, test = catboost_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred_bl_joining(top_phi_bot_xy, phit_pred_cluster_baseline):\n",
    "    add_info1 = top_phi_bot_xy[['well','phit_avg','top_htst', 'htst', 'bot_htst']]\n",
    "    add_info1['phit_avg'] = add_info1['phit_avg'].round(5)\n",
    "    phit_pred_cluster_bl_collect = phit_pred_cluster_baseline.join(training_data_tpb_notcl.drop(['well','FORMATION_up','htst'],axis=1))\n",
    "    phit_cluster_orig_pred = phit_pred_cluster_bl_collect.set_index(['well','phit_avg']).join(add_info1.set_index(['well','phit_avg'])).reset_index()\n",
    "    return phit_cluster_orig_pred\n",
    "def phit_orig_pred_bl_collecting(dataset):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        phit_pred_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            phit_pred_lst.append(np.nan)\n",
    "            phit_pred_lst.append(row['phit_cluster_bl'])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        phit_pred_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, phit_pred_lst ), \n",
    "                                            columns=['well','phit_orig', 'htst', 'phit_pred'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "phit_cluster_orig_pred = phit_orig_pred_bl_joining(top_phi_bot8_xy, phit_pred_cluster_baseline)\n",
    "phit_orig_pred_cluster_bl = phit_orig_pred_bl_collecting(phit_cluster_orig_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_cluster(dataset, dataset_clustering, cluster_num, grid_search):\n",
    "    def conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num):\n",
    "        training_data_tpb8 = dataset.copy(deep=True)\n",
    "        training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                                'FORMATION_up',\n",
    "                                                'X_mean', 'Y_mean',\n",
    "                                                'phit_avg', \n",
    "                                                'vsh_avg',\n",
    "                                                'top_htst',\n",
    "                                                'htst',\n",
    "                                                'bot_htst',\n",
    "                                                'depth',\n",
    "                                                ]]\n",
    "        def clusters_joining(dataset_clustering, training_data_tpb):\n",
    "            cluster_nums = dataset_clustering[(dataset_clustering.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "            cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "            cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "            training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "            training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "            return training_data_tpb_gm\n",
    "        training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "        training_data_tpb_gm0 = training_data_tpb_gm[training_data_tpb_gm.cluster.isin(cluster_num)]\n",
    "\n",
    "        def closest_wells_per_cluster(training_data_tpb_gm0):\n",
    "            def dist_prop_calc_xy(dataset, dist_formation, dist_cutoff, value):\n",
    "                \"\"\"\n",
    "                dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "                \"\"\"\n",
    "                data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "                row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "                distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean']]), columns=list(data.well))\n",
    "                distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "                distance_fm_well.reset_index(inplace=True)\n",
    "                def well_kh_accum(wells, dataset, kh_formation):\n",
    "                    well_kh_accum = []\n",
    "                    well_x_accum = []\n",
    "                    well_y_accum = []\n",
    "                    for i in wells:\n",
    "                        well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "                        well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "                        well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "                    well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "                    well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "                    well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "                    well_x3.columns = ['x1','x2','x3']\n",
    "                    well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "                    well_y3.columns = ['y1','y2','y3']\n",
    "                    final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                                        well_x3.reset_index().drop('index',axis=1), \n",
    "                                        well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "                    return final\n",
    "                df_collect = []\n",
    "                for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "                    well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "                    well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "                    well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "                    well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "                    well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "                    well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "                    well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "                    well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "                    concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "                    result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "                    df_collect.append(result)     \n",
    "                df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "                df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "                return df_well_kh_dist\n",
    "            tpb8_cluster0 = training_data_tpb_gm0.groupby('well').apply(lambda x: x.iloc[0]).drop('well',axis=1).reset_index()\n",
    "            list_well_offsets = dist_prop_calc_xy(tpb8_cluster0, 'Balakhany VIII', 0, 'phit_avg')[\n",
    "                            ['well','FORMATION_up','well1', 'well2', 'well3','dist1', 'dist2', 'dist3']]\n",
    "            return list_well_offsets\n",
    "        list_wells_offsets0 = closest_wells_per_cluster(training_data_tpb_gm0)\n",
    "\n",
    "        def loop_cluster_data_to_sample(dataset, list_wells_offsets_cluster, wellname):\n",
    "            target_train = dataset[dataset.well == wellname]\n",
    "            offset_wells = list_wells_offsets_cluster[list_wells_offsets_cluster.well == wellname]\n",
    "\n",
    "            ftr1 = dataset[dataset.well == offset_wells.well1.iloc[0]]\n",
    "            ftr2 = dataset[dataset.well == offset_wells.well2.iloc[0]]\n",
    "            ftr3 = dataset[dataset.well == offset_wells.well3.iloc[0]]\n",
    "\n",
    "            def conv_cluster_target_to_sample(dataset):\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg_target': [phit_wavg_cl]})\n",
    "                return result\n",
    "\n",
    "            target_wavg = conv_cluster_target_to_sample(target_train)\n",
    "\n",
    "            def conv_cluster_feature_to_sample(dataset, offset_wells, well_seq, num):\n",
    "                suff = '_' + str(num)\n",
    "                dist = offset_wells[well_seq].iloc[0]\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                def summ_h(x1,x2):\n",
    "                    result = x1+x2\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                dataset['vshh'] =  dataset.apply(lambda x: w_avg(x['vsh_avg'], x['htst']), axis=1)\n",
    "                dataset['sumh'] =  dataset.apply(lambda x: summ_h(x['top_htst'], x['htst']), axis=1)\n",
    "                vsh_wavg_cl = dataset['vshh'].sum() / dataset['htst'].sum()\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                gross = dataset['sumh'].sum() + dataset['bot_htst'].iloc[-1]\n",
    "                net = dataset['htst'].sum()\n",
    "                ntg = net/gross\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg' + suff: [phit_wavg_cl],\n",
    "                                        'net' + suff:[net], \n",
    "                                        'gross' + suff:[gross], \n",
    "                                        'ntg' + suff:[ntg], \n",
    "                                        'vsh_wavg' + suff:[vsh_wavg_cl], \n",
    "                                        'dist' + suff:dist})\n",
    "                return result\n",
    "\n",
    "            ftr1_wavg = conv_cluster_feature_to_sample(ftr1, offset_wells, 'dist1', 1)\n",
    "            ftr2_wavg = conv_cluster_feature_to_sample(ftr2, offset_wells, 'dist2', 2)\n",
    "            ftr3_wavg = conv_cluster_feature_to_sample(ftr3, offset_wells, 'dist3', 3)\n",
    "\n",
    "            return target_wavg, ftr1_wavg, ftr2_wavg, ftr3_wavg\n",
    "        df_lst = []\n",
    "        for wellname in training_data_tpb_gm0.well.unique():\n",
    "            t, f1, f2, f3 = loop_cluster_data_to_sample(training_data_tpb_gm0, list_wells_offsets0, wellname)\n",
    "            result_dataset = pd.concat([t.iloc[:,3:], f1.iloc[:,3:], f2.iloc[:,3:], f3.iloc[:,3:]], axis=1)\n",
    "            df_lst.append(result_dataset)\n",
    "        data_Xy = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return data_Xy\n",
    "    data_Xy = conv_cluster_data_to_sample(dataset, dataset_clustering, cluster_num)\n",
    "    X = data_Xy.iloc[:,1:]\n",
    "    y = data_Xy['phit_wavg_target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "    gs_set = {  'bootstrap': [True],\n",
    "                'max_depth': [100, 150, 200, 250],\n",
    "                'min_samples_leaf': [5, 10, 15, 20],\n",
    "                'min_samples_split': [5, 10, 15, 20],\n",
    "                'n_estimators': [30, 100, 150, 200]}\n",
    "\n",
    "    if grid_search == 'grid_search':\n",
    "        scorer_r2 = make_scorer(r2, greater_is_better=True)\n",
    "        grid_rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "        grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = gs_set, scoring=scorer_r2, cv = 5)\n",
    "        grid_calc_rfr.fit(X_train, y_train)\n",
    "        gd_sr_setting = grid_calc_rfr.best_params_\n",
    "        print('Grid_search cluster # {cluster_num}: ', gd_sr_setting)\n",
    "        rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**gd_sr_setting, n_jobs=-1, random_state=42))])\n",
    "    else:\n",
    "        rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "    rfr.fit(X_train, y_train)\n",
    "    y_pred_train = rfr.predict(X_train)\n",
    "    y_pred_test = rfr.predict(X_test)\n",
    "\n",
    "    train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "    test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "    tolerance = 0.05\n",
    "    def test_processing(tolerance):\n",
    "        test['up'] = test['y_orig']*(1 + tolerance)\n",
    "        test['down'] = test['y_orig']*(1 - tolerance)\n",
    "        test['qc'] = 'out'\n",
    "        test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "        return test\n",
    "    test = test_processing(tolerance)\n",
    "    metric = test.qc.value_counts(normalize=True)['in'].round(2)\n",
    "    def train_test_display(tolerance, min_phit):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train['y_orig'], train['y_pred'], label = 'train', ec='gray', alpha=0.5)\n",
    "        ax.scatter(test['y_orig'], test['y_pred'], label = 'test', ec='gray', alpha=0.9)\n",
    "        line = mlines.Line2D([min_phit, 0.28], [min_phit, 0.28], color='red', ls='--')\n",
    "        line1 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 + tolerance), 0.28*(1 + tolerance)], color='blue', ls='--')\n",
    "        line2 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 - tolerance), 0.28*((1 - tolerance))], color='blue', ls='--')\n",
    "        ax.add_line(line), ax.add_line(line1), ax.add_line(line2), ax.grid(), ax.legend(), ax.set_ylabel('pred'), ax.set_xlabel('orig'),\n",
    "        ax.set_xlim(0.14, 0.28), ax.set_ylim(0.14, 0.28) \n",
    "        ax.set_title(f'Cluster # {cluster_num} metric \"test in\": {metric}')\n",
    "    train_test_display(tolerance, 0.15)\n",
    "    return  data_Xy, train, test\n",
    "data_Xy, train0, test0 = rfr_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [0,1,2], 'dont_gs')\n",
    "# data_Xy0, train0, test0 = rfr_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [0], 'dont_gs')\n",
    "# data_Xy1, train1, test1 = rfr_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [1], 'dont_gs')\n",
    "# data_Xy2, train2, test2 = rfr_cluster(top_phi_bot8_xy, ntd_val_kmeans3, [2], 'dont_gs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype == 'string' or dataset[col].dtype == 'object':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "training_data_tpb8 = top_phi_bot8_xy.copy(deep=True)\n",
    "training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                        'FORMATION_up',\n",
    "                                        'X_mean', 'Y_mean',\n",
    "                                        'phit_avg', \n",
    "                                        'vsh_avg',\n",
    "                                        'htst',\n",
    "                                        'depth',\n",
    "                                        ]]\n",
    "def clusters_joining(ntd_val_kmeans3, training_data_tpb):\n",
    "    cluster_nums = ntd_val_kmeans3[(ntd_val_kmeans3.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "    cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "    cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "    training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "    training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "    return training_data_tpb_gm\n",
    "training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "training_data_tpb_gm0 = training_data_tpb_gm[training_data_tpb_gm.cluster == 0].drop('cluster', axis=1)\n",
    "training_data_tpb_corr, training_data_tpb_gm0 = join_df_prediction(training_data_tpb_gm0, 'phit_avg')\n",
    "training_data_tpb_gm0 = training_data_tpb_gm0.reset_index().drop('index', axis=1)\n",
    "pairplot_special(training_data_tpb_corr, 7, 7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-split block to get an optimized hyper parameters for 1-to-all function\n",
    "gs_set = {  'bootstrap': [True, False],\n",
    "            'max_depth': [10, 30, 100, 150, 200],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'min_samples_split': [1, 5, 10],\n",
    "            'n_estimators': [10, 30, 100, 150, 200]}\n",
    "scorer = make_scorer(r2, greater_is_better=True)\n",
    "hyperdict = run_rfr_train_test_split(training_data_tpb_gm0, gs_set, scorer, 'phit_avg', \n",
    "                                     0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_grid_search', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tpb_gm0 = training_data_tpb_gm0.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1-to-all block to to predict target vaiable\n",
    "phit_pred_cluster_cl0 = run_rfr_1_to_all(training_data_tpb_gm0, hyperdict, 'phit_avg', \n",
    "                                         0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_1-to-all', 1, 0.3, 0)\n",
    "phit_pred_cluster_cl0 =  phit_pred_cluster_cl0.rename(columns={'predict':'phit_cluster0'})[\n",
    "                                            ['well','FORMATION_up','phit_cluster0','qc']].sort_values(\n",
    "                                            by=['well','FORMATION_up']).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred_joining(dataset_xy, dataset_training, dataset_phit_pred):\n",
    "    add_info = dataset_xy[['well','phit_avg','top_htst', 'htst', 'bot_htst']]\n",
    "    add_info['phit_avg'] = add_info['phit_avg'].round(5)\n",
    "    phit_pred_cluster_collect = dataset_phit_pred.join(dataset_training.drop(['well','FORMATION_up','htst'],axis=1))\n",
    "    phit_cluster_orig_pred =    phit_pred_cluster_collect.set_index(['well','phit_avg']).join(\n",
    "                                add_info.set_index(['well','phit_avg'])).reset_index()\n",
    "    return phit_cluster_orig_pred\n",
    "def phit_orig_pred_collecting(dataset, target_column):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        phit_pred_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            phit_pred_lst.append(np.nan)\n",
    "            phit_pred_lst.append(row[target_column])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        phit_pred_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, phit_pred_lst ), \n",
    "                                            columns=['well','phit_orig', 'htst', 'phit_pred'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "phit_cluster_orig_pred0 = phit_orig_pred_joining(top_phi_bot8_xy, training_data_tpb_gm0, phit_pred_cluster_cl0)\n",
    "phit_orig_pred_cluster_cl0 = phit_orig_pred_collecting(phit_cluster_orig_pred0, 'phit_cluster0')\n",
    "\n",
    "def coloring_clusters_matrix_plot(dataset, letters_list, rows, columns):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit_orig':[0], \n",
    "                                           'htst':[0], 'phit_pred':[0], 'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit_orig'], welldata['depth'], drawstyle='steps-post', color='gray', alpha=1, lw=1.5)\n",
    "                    ax[j,i].plot(welldata['phit_pred'], welldata['depth'], drawstyle='steps-post', color='blue', alpha=1, lw=2)\n",
    "                    ax[j,i].set_xlim(0.1, 0.3)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)   \n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "coloring_clusters_matrix_plot(  phit_orig_pred_cluster_cl0, \n",
    "                                ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J'], 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype == 'string' or dataset[col].dtype == 'object':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "training_data_tpb = top_phi_bot8_xy.copy(deep=True)\n",
    "training_data_tpb = training_data_tpb[[ 'well',\n",
    "                                        'FORMATION_up',\n",
    "                                        'X_mean', 'Y_mean',\n",
    "                                        'phit_avg', \n",
    "                                        'vsh_avg',\n",
    "                                        'htst',\n",
    "                                        'depth',\n",
    "                                        ]]\n",
    "def clusters_joining(ntd_val_kmeans3, training_data_tpb):\n",
    "    cluster_nums = ntd_val_kmeans3[(ntd_val_kmeans3.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "    cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "    cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "    training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "    training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "    return training_data_tpb_gm\n",
    "training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb)\n",
    "training_data_tpb_gm1 = training_data_tpb_gm[training_data_tpb_gm.cluster == 1].drop('cluster', axis=1)\n",
    "training_data_tpb_corr, training_data_tpb_gm1 = join_df_prediction(training_data_tpb_gm1, 'phit_avg')\n",
    "training_data_tpb_gm1 = training_data_tpb_gm1.reset_index().drop('index', axis=1)\n",
    "pairplot_special(training_data_tpb_corr, 7, 7, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-split block to get an optimized hyper parameters for 1-to-all function\n",
    "gs_set = {  'bootstrap': [True, False],\n",
    "            'max_depth': [10, 30, 100, 150, 200],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'min_samples_split': [1, 5, 10],\n",
    "            'n_estimators': [10, 30, 100, 150, 200]}\n",
    "scorer = make_scorer(r2, greater_is_better=True)\n",
    "hyperdict = run_rfr_train_test_split(training_data_tpb_gm1, gs_set, scorer, 'phit_avg', \n",
    "                                     0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_grid_search', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tpb_gm1 = training_data_tpb_gm1.reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1-to-all block to to predict target vaiable\n",
    "phit_pred_cluster_cl1 = run_rfr_1_to_all(training_data_tpb_gm1, hyperdict, 'phit_avg', \n",
    "                                         0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_1-to-all', 1, 0.3, 0)\n",
    "phit_pred_cluster_cl1 =  phit_pred_cluster_cl1.rename(columns={'predict':'phit_cluster1'})[\n",
    "                                            ['well','FORMATION_up','phit_cluster1','qc']].sort_values(\n",
    "                                            by=['well','FORMATION_up']).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred_joining(dataset_xy, dataset_training, dataset_phit_pred):\n",
    "    add_info = dataset_xy[['well','phit_avg','top_htst', 'htst', 'bot_htst']]\n",
    "    add_info['phit_avg'] = add_info['phit_avg'].round(5)\n",
    "    phit_pred_cluster_collect = dataset_phit_pred.join(dataset_training.drop(['well','FORMATION_up'],axis=1))\n",
    "    phit_cluster_orig_pred =    phit_pred_cluster_collect.set_index(['well','phit_avg']).join(\n",
    "                                add_info.set_index(['well','phit_avg'])).reset_index()\n",
    "    return phit_cluster_orig_pred\n",
    "def phit_orig_pred_collecting(dataset, target_column):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        phit_pred_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            phit_pred_lst.append(np.nan)\n",
    "            phit_pred_lst.append(row[target_column])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        phit_pred_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, phit_pred_lst ), \n",
    "                                            columns=['well','phit_orig', 'htst', 'phit_pred'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "phit_cluster_orig_pred1 = phit_orig_pred_joining(top_phi_bot_xy, training_data_tpb_gm1, phit_pred_cluster_cl1)\n",
    "phit_orig_pred_cluster_cl1 = phit_orig_pred_collecting(phit_cluster_orig_pred1, 'phit_cluster1')\n",
    "\n",
    "def coloring_clusters_matrix_plot(dataset, letters_list, rows, columns):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit_orig':[0], \n",
    "                                           'htst':[0], 'phit_pred':[0], 'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit_orig'], welldata['depth'], drawstyle='steps-post', color='gray', alpha=1, lw=1.5)\n",
    "                    ax[j,i].plot(welldata['phit_pred'], welldata['depth'], drawstyle='steps-post', color='green', alpha=1, lw=2)\n",
    "                    ax[j,i].set_xlim(0.1, 0.3)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)   \n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "coloring_clusters_matrix_plot(  phit_orig_pred_cluster_cl1, \n",
    "                                ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J'], 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            if dataset[col].dtype == 'string' or dataset[col].dtype == 'object':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "training_data_tpb = top_phi_bot_xy.copy(deep=True)\n",
    "training_data_tpb = training_data_tpb[[ 'well',\n",
    "                                        'FORMATION_up',\n",
    "                                        'X_mean', 'Y_mean',\n",
    "                                        'phit_avg', \n",
    "                                        'vsh_avg',\n",
    "                                        # 'htst',\n",
    "                                        # 'fm_top_tst',\n",
    "                                        ]]\n",
    "def clusters_joining(ntd_val_kmeans3, training_data_tpb):\n",
    "    cluster_nums = ntd_val_kmeans3[(ntd_val_kmeans3.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "    cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "    cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "    training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "    training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "    return training_data_tpb_gm\n",
    "training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb)\n",
    "training_data_tpb_gm2 = training_data_tpb_gm[training_data_tpb_gm.cluster == 2].drop('cluster', axis=1)\n",
    "training_data_tpb_corr, training_data_tpb_gm2 = join_df_prediction(training_data_tpb_gm2, 'phit_avg')\n",
    "training_data_tpb_gm2 = training_data_tpb_gm2.reset_index().drop('index', axis=1)\n",
    "pairplot_special(training_data_tpb_corr, 7, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tpb_gm2 = training_data_tpb_gm2.reset_index().drop('index',axis=1)\n",
    "training_data_tpb_gm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run train-split block to get an optimized hyper parameters for 1-to-all function\n",
    "gs_set = {  'bootstrap': [True, False],\n",
    "            'max_depth': [10, 30, 100, 150, 200],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'min_samples_split': [1, 5, 10],\n",
    "            'n_estimators': [10, 30, 100, 150, 200]}\n",
    "scorer = make_scorer(r2, greater_is_better=True)\n",
    "hyperdict = run_rfr_train_test_split(training_data_tpb_gm2, gs_set, scorer, 'phit_avg', \n",
    "                                     0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_grid_search', 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 1-to-all block to to predict target vaiable\n",
    "phit_pred_cluster_cl2 = run_rfr_1_to_all(training_data_tpb_gm2, hyperdict, 'phit_avg', \n",
    "                                         0.05, 0, 'training_data_tpb_gm.txt', 'training_data_tpb_gm_1-to-all', 1, 0.3, 0)\n",
    "phit_pred_cluster_cl2 =  phit_pred_cluster_cl2.rename(columns={'predict':'phit_cluster2'})[\n",
    "                                            ['well','FORMATION_up','phit_cluster2','qc']].sort_values(\n",
    "                                            by=['well','FORMATION_up']).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phit_orig_pred_joining(dataset_xy, dataset_training, dataset_phit_pred):\n",
    "    add_info = dataset_xy[['well','phit_avg','top_htst', 'htst', 'bot_htst']]\n",
    "    add_info['phit_avg'] = add_info['phit_avg'].round(5)\n",
    "    phit_pred_cluster_collect = dataset_phit_pred.join(dataset_training.drop(['well','FORMATION_up'],axis=1))\n",
    "    phit_cluster_orig_pred =    phit_pred_cluster_collect.set_index(['well','phit_avg']).join(\n",
    "                                add_info.set_index(['well','phit_avg'])).reset_index()\n",
    "    return phit_cluster_orig_pred\n",
    "def phit_orig_pred_collecting(dataset, target_column):\n",
    "    df_lst = []\n",
    "    for wellname in dataset.well.unique()[:]:\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        well_lst = []\n",
    "        phit_lst = []\n",
    "        htst_lst = []\n",
    "        phit_pred_lst = []\n",
    "        for ind, row in data.iterrows():\n",
    "            well_lst.append(wellname)\n",
    "            well_lst.append(wellname)\n",
    "\n",
    "            phit_lst.append(0)\n",
    "            phit_lst.append(row['phit_avg'])\n",
    "\n",
    "            phit_pred_lst.append(np.nan)\n",
    "            phit_pred_lst.append(row[target_column])\n",
    "\n",
    "            htst_lst.append(row['top_htst'])\n",
    "            htst_lst.append(row['htst'])\n",
    "\n",
    "        phit_lst.append(0)\n",
    "        phit_pred_lst.append(np.nan)\n",
    "        htst_lst.append(data['bot_htst'].iloc[-1])\n",
    "        well_lst.append(wellname)\n",
    "\n",
    "        well_collect_cluster = pd.DataFrame(zip(well_lst, phit_lst, htst_lst, phit_pred_lst ), \n",
    "                                            columns=['well','phit_orig', 'htst', 'phit_pred'])\n",
    "        well_collect_cluster['depth'] = well_collect_cluster['htst'].cumsum()\n",
    "        df_lst.append(well_collect_cluster)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "phit_cluster_orig_pred2 = phit_orig_pred_joining(top_phi_bot_xy, training_data_tpb_gm2, phit_pred_cluster_cl2)\n",
    "phit_orig_pred_cluster_cl2 = phit_orig_pred_collecting(phit_cluster_orig_pred2, 'phit_cluster2')\n",
    "\n",
    "def coloring_clusters_matrix_plot(dataset, letters_list, rows, columns):\n",
    "    for letter in letters_list:\n",
    "        wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(letter)]\n",
    "        fig, ax = plt.subplots(rows,columns, figsize=(16,rows*2.5))\n",
    "        counter = 0\n",
    "        for j in range(0, rows):\n",
    "            for i in range(0, columns):\n",
    "                if counter < len(wells_letter):\n",
    "                    wellname = wells_letter[counter]\n",
    "                    welldata = dataset[dataset.well==wellname]\n",
    "                    df_top = pd.DataFrame({'well':[wellname], 'phit_orig':[0], \n",
    "                                           'htst':[0], 'phit_pred':[0], 'depth':[0]})\n",
    "                    welldata = pd.concat([df_top, welldata]).reset_index().drop('index', axis=1)\n",
    "                    ax[j,i].plot(welldata['phit_orig'], welldata['depth'], drawstyle='steps-post', color='gray', alpha=1, lw=1.5)\n",
    "                    ax[j,i].plot(welldata['phit_pred'], welldata['depth'], drawstyle='steps-post', color='red', alpha=1, lw=2)\n",
    "                    ax[j,i].set_xlim(0.1, 0.3)\n",
    "                    ax[j,i].invert_yaxis()\n",
    "                    ax[j,i].set_title(wellname)\n",
    "                    ax[j,i].tick_params(axis='both', which='major', labelsize=10)   \n",
    "                    fig.tight_layout()\n",
    "                    counter +=1\n",
    "coloring_clusters_matrix_plot(  phit_orig_pred_cluster_cl2, \n",
    "                                ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J'], 4, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_cluster(dataset, cluster_num, grid_search):\n",
    "    def conv_cluster_data_to_sample(dataset, cluster_num):\n",
    "        training_data_tpb8 = dataset.copy(deep=True)\n",
    "        training_data_tpb8 = training_data_tpb8[[ 'well',\n",
    "                                                'FORMATION_up',\n",
    "                                                'X_mean', 'Y_mean',\n",
    "                                                'phit_avg', \n",
    "                                                'vsh_avg',\n",
    "                                                'top_htst',\n",
    "                                                'htst',\n",
    "                                                'bot_htst',\n",
    "                                                'depth',\n",
    "                                                ]]\n",
    "        def clusters_joining(ntd_val_kmeans3, training_data_tpb):\n",
    "            cluster_nums = ntd_val_kmeans3[(ntd_val_kmeans3.phit != 0)][['well','phit','cluster']].reset_index().drop('index', axis=1)\n",
    "            cluster_nums = cluster_nums.rename(columns={'phit':'phit_avg'})\n",
    "            cluster_nums['phit_avg'] = cluster_nums['phit_avg'].round(5)\n",
    "            training_data_tpb['phit_avg'] = training_data_tpb['phit_avg'].round(5)\n",
    "            training_data_tpb_gm = training_data_tpb.set_index(['well','phit_avg']).join(cluster_nums.set_index(['well','phit_avg'])).reset_index()\n",
    "            return training_data_tpb_gm\n",
    "        training_data_tpb_gm = clusters_joining(ntd_val_kmeans3, training_data_tpb8)\n",
    "        training_data_tpb_gm0 = training_data_tpb_gm[training_data_tpb_gm.cluster == cluster_num]\n",
    "\n",
    "        def closest_wells_per_cluster(training_data_tpb_gm0):\n",
    "            def dist_prop_calc_xy(dataset, dist_formation, dist_cutoff, value):\n",
    "                \"\"\"\n",
    "                dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "                \"\"\"\n",
    "                data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "                row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "                distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean']]), columns=list(data.well))\n",
    "                distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "                distance_fm_well.reset_index(inplace=True)\n",
    "                def well_kh_accum(wells, dataset, kh_formation):\n",
    "                    well_kh_accum = []\n",
    "                    well_x_accum = []\n",
    "                    well_y_accum = []\n",
    "                    for i in wells:\n",
    "                        well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "                        well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "                        well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "                    well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "                    well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "                    well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "                    well_x3.columns = ['x1','x2','x3']\n",
    "                    well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "                    well_y3.columns = ['y1','y2','y3']\n",
    "                    final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                                        well_x3.reset_index().drop('index',axis=1), \n",
    "                                        well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "                    return final\n",
    "                df_collect = []\n",
    "                for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "                    well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "                    well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "                    well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "                    well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "                    well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "                    well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "                    well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "                    well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "                    concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "                    result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "                    df_collect.append(result)     \n",
    "                df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "                df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "                return df_well_kh_dist\n",
    "            tpb8_cluster0 = training_data_tpb_gm0.groupby('well').apply(lambda x: x.iloc[0]).drop('well',axis=1).reset_index()\n",
    "            list_well_offsets = dist_prop_calc_xy(tpb8_cluster0, 'Balakhany VIII', 0, 'phit_avg')[\n",
    "                            ['well','FORMATION_up','well1', 'well2', 'well3','dist1', 'dist2', 'dist3']]\n",
    "            return list_well_offsets\n",
    "        list_wells_offsets0 = closest_wells_per_cluster(training_data_tpb_gm0)\n",
    "\n",
    "        def loop_cluster_data_to_sample(dataset, list_wells_offsets_cluster, wellname):\n",
    "            target_train = dataset[dataset.well == wellname]\n",
    "            offset_wells = list_wells_offsets_cluster[list_wells_offsets_cluster.well == wellname]\n",
    "\n",
    "            ftr1 = dataset[dataset.well == offset_wells.well1.iloc[0]]\n",
    "            ftr2 = dataset[dataset.well == offset_wells.well2.iloc[0]]\n",
    "            ftr3 = dataset[dataset.well == offset_wells.well3.iloc[0]]\n",
    "\n",
    "            def conv_cluster_target_to_sample(dataset):\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg_target': [phit_wavg_cl]})\n",
    "                return result\n",
    "\n",
    "            target_wavg = conv_cluster_target_to_sample(target_train)\n",
    "\n",
    "            def conv_cluster_feature_to_sample(dataset, offset_wells, well_seq, num):\n",
    "                suff = '_' + str(num)\n",
    "                dist = offset_wells[well_seq].iloc[0]\n",
    "                def w_avg(phit, h):\n",
    "                    result = phit*h\n",
    "                    return result\n",
    "                def summ_h(x1,x2):\n",
    "                    result = x1+x2\n",
    "                    return result\n",
    "                dataset['phith'] =  dataset.apply(lambda x: w_avg(x['phit_avg'], x['htst']), axis=1)\n",
    "                dataset['vshh'] =  dataset.apply(lambda x: w_avg(x['vsh_avg'], x['htst']), axis=1)\n",
    "                dataset['sumh'] =  dataset.apply(lambda x: summ_h(x['top_htst'], x['htst']), axis=1)\n",
    "                vsh_wavg_cl = dataset['vshh'].sum() / dataset['htst'].sum()\n",
    "                phit_wavg_cl = dataset['phith'].sum() / dataset['htst'].sum()\n",
    "                gross = dataset['sumh'].sum() + dataset['bot_htst'].iloc[-1]\n",
    "                net = dataset['htst'].sum()\n",
    "                ntg = net/gross\n",
    "                well = dataset['well'].iloc[0]\n",
    "                fm = dataset['FORMATION_up'].iloc[0]\n",
    "                cluster = dataset['cluster'].iloc[0]\n",
    "                result = pd.DataFrame({ 'well':[well], \n",
    "                                        'FORMATION_up': [fm], \n",
    "                                        'cluster': [cluster], \n",
    "                                        'phit_wavg' + suff: [phit_wavg_cl],\n",
    "                                        'net' + suff:[net], \n",
    "                                        'gross' + suff:[gross], \n",
    "                                        'ntg' + suff:[ntg], \n",
    "                                        'vsh_wavg' + suff:[vsh_wavg_cl], \n",
    "                                        'dist' + suff:dist})\n",
    "                return result\n",
    "\n",
    "            ftr1_wavg = conv_cluster_feature_to_sample(ftr1, offset_wells, 'dist1', 1)\n",
    "            ftr2_wavg = conv_cluster_feature_to_sample(ftr2, offset_wells, 'dist2', 2)\n",
    "            ftr3_wavg = conv_cluster_feature_to_sample(ftr3, offset_wells, 'dist3', 3)\n",
    "\n",
    "            return target_wavg, ftr1_wavg, ftr2_wavg, ftr3_wavg\n",
    "        df_lst = []\n",
    "        for wellname in training_data_tpb_gm0.well.unique():\n",
    "            t, f1, f2, f3 = loop_cluster_data_to_sample(training_data_tpb_gm0, list_wells_offsets0, wellname)\n",
    "            result_dataset = pd.concat([t.iloc[:,3:], f1.iloc[:,3:], f2.iloc[:,3:], f3.iloc[:,3:]], axis=1)\n",
    "            df_lst.append(result_dataset)\n",
    "        data_Xy = pd.concat(df_lst).reset_index(drop=True)\n",
    "        return data_Xy\n",
    "    data_Xy = conv_cluster_data_to_sample(dataset, cluster_num)\n",
    "    X = data_Xy.iloc[:,1:]\n",
    "    y = data_Xy['phit_wavg_target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=55)\n",
    "\n",
    "    gs_set = {  'bootstrap': [True],\n",
    "                'max_depth': [100, 150, 200, 250],\n",
    "                'min_samples_leaf': [5, 10, 15, 20],\n",
    "                'min_samples_split': [5, 10, 15, 20],\n",
    "                'n_estimators': [30, 100, 150, 200]}\n",
    "\n",
    "    if grid_search == 'grid_search':\n",
    "        scorer_r2 = make_scorer(r2, greater_is_better=True)\n",
    "        grid_rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "        grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = gs_set, scoring=scorer_r2, cv = 5)\n",
    "        grid_calc_rfr.fit(X_train, y_train)\n",
    "        gd_sr_setting = grid_calc_rfr.best_params_\n",
    "        print('Grid_search cluster # {cluster_num}: ', gd_sr_setting)\n",
    "        rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**gd_sr_setting, n_jobs=-1, random_state=42))])\n",
    "    else:\n",
    "        rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "\n",
    "    rfr.fit(X_train, y_train)\n",
    "    y_pred_train = rfr.predict(X_train)\n",
    "    y_pred_test = rfr.predict(X_test)\n",
    "\n",
    "    train = pd.DataFrame(zip(y_train,y_pred_train), columns=['y_orig', 'y_pred'])\n",
    "    test = pd.DataFrame(zip(y_test,y_pred_test), columns=['y_orig', 'y_pred'])\n",
    "    tolerance = 0.05\n",
    "    def test_processing(tolerance):\n",
    "        test['up'] = test['y_orig']*(1 + tolerance)\n",
    "        test['down'] = test['y_orig']*(1 - tolerance)\n",
    "        test['qc'] = 'out'\n",
    "        test.loc[(test['y_pred'] <= test.up) & (test['y_pred'] >= test.down), 'qc'] = 'in'\n",
    "        return test\n",
    "    test = test_processing(tolerance)\n",
    "    metric = test.qc.value_counts(normalize=True)['in'].round(2)\n",
    "    def train_test_display(tolerance, min_phit):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(train['y_orig'], train['y_pred'], label = 'train', ec='gray', alpha=0.5)\n",
    "        ax.scatter(test['y_orig'], test['y_pred'], label = 'test', ec='gray', alpha=0.9)\n",
    "        line = mlines.Line2D([min_phit, 0.28], [min_phit, 0.28], color='red', ls='--')\n",
    "        line1 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 + tolerance), 0.28*(1 + tolerance)], color='blue', ls='--')\n",
    "        line2 = mlines.Line2D([min_phit, 0.28], [min_phit*(1 - tolerance), 0.28*((1 - tolerance))], color='blue', ls='--')\n",
    "        ax.add_line(line), ax.add_line(line1), ax.add_line(line2), ax.grid(), ax.legend(), ax.set_ylabel('pred'), ax.set_xlabel('orig'),\n",
    "        ax.set_xlim(0.14, 0.28), ax.set_ylim(0.14, 0.28) \n",
    "        ax.set_title(f'Cluster # {cluster_num} metric \"test in\": {metric}')\n",
    "    train_test_display(tolerance, 0.15)\n",
    "    return  data_Xy, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "X = data_Xy.iloc[:,1:]\n",
    "y = data_Xy['phit_wavg_target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=False, test_size=0.3)\n",
    "X_trainval, X_valid, y_trainval, y_valid = train_test_split(X_train, y_train, shuffle=True,  random_state=42)\n",
    "param = {\n",
    "    'tree_method':'approx',  # this parameter means using the GPU when training our model to speedup the training process\n",
    "    'sampling_method': 'uniform',\n",
    "    # 'lambda': trial.suggest_loguniform('lambda', 7.0, 17.0),\n",
    "    # 'alpha': trial.suggest_loguniform('alpha', 7.0, 17.0),\n",
    "    'eta': trial.suggest_categorical('eta', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "    # 'gamma': trial.suggest_categorical('gamma', [18, 19, 20, 21, 22, 23, 24, 25]),\n",
    "    'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
    "    'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "    # 'colsample_bynode': trial.suggest_categorical('colsample_bynode', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "    # 'min_child_weight': trial.suggest_int('min_child_weight', 8, 600),  \n",
    "    # 'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7]),  \n",
    "    # 'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8,1.0]),\n",
    "    'random_state': 42\n",
    "}\n",
    "model = XGBRegressor(**param)\n",
    "# model = XGBRegressor()      \n",
    "model.fit(X_trainval, y_trainval, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=False)\n",
    "predict = model.predict(X_valid)\n",
    "r_2 = r2_score(predict, y_valid)\n",
    "test_optuna = pd.DataFrame(zip(y_valid,predict), columns=['y_orig', 'y_pred'])\n",
    "plt.scatter(test_optuna['y_orig'], test_optuna['y_pred'])\n",
    "plt.xlim(0.15,0.28), plt.ylim(0.15,0.28)\n",
    "plt.grid()\n",
    "r_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = data_Xy.iloc[:,1:]\n",
    "y = data_Xy['phit_wavg_target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, shuffle=True, test_size=0.3)\n",
    "X_trainval, X_valid, y_trainval, y_valid = train_test_split(X_train, y_train, shuffle=True,  random_state=42)\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'tree_method':'approx',  # this parameter means using the GPU when training our model to speedup the training process\n",
    "        'sampling_method': 'uniform',\n",
    "        # 'lambda': trial.suggest_loguniform('lambda', 7.0, 17.0),\n",
    "        # 'alpha': trial.suggest_loguniform('alpha', 7.0, 17.0),\n",
    "        'eta': trial.suggest_categorical('eta', [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n",
    "        # 'gamma': trial.suggest_categorical('gamma', [18, 19, 20, 21, 22, 23, 24, 25]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        # 'colsample_bynode': trial.suggest_categorical('colsample_bynode', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n",
    "        # 'min_child_weight': trial.suggest_int('min_child_weight', 8, 600),  \n",
    "        # 'max_depth': trial.suggest_categorical('max_depth', [3, 4, 5, 6, 7]),  \n",
    "        # 'subsample': trial.suggest_categorical('subsample', [0.5,0.6,0.7,0.8,1.0]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBRegressor(**param)    \n",
    "    model.fit(X_trainval, y_trainval, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=False)\n",
    "    predict = model.predict(X_valid)\n",
    "    r_2 = r2_score(predict, y_valid)\n",
    "    return r_2\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50,  timeout=600)\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_slice(study)\n",
    "fig.update_layout(title='<b>Slice Plot', title_x=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.update_layout(title='<b>Optimization History Plot', title_x=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.update_layout(title='<b>Hyperparameter Importances', title_x=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = XGBRegressor(**study.best_params)\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best model result in Test: {r2_score(y_test, best_model.predict(X_test))}')\n",
    "print(f'Best model result in Train: {r2_score(y_train, best_model.predict(X_train))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_optuna2 = pd.DataFrame(zip(y_test, best_model.predict(X_test)), columns=['y_orig', 'y_pred'])\n",
    "plt.scatter(test_optuna2['y_orig'], test_optuna2['y_pred'])\n",
    "plt.xlim(0.15,0.28), plt.ylim(0.15,0.28)\n",
    "plt.grid()\n",
    "r2_score(y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geopandas Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf1 = np.loadtxt('C:\\jupyter\\SPP\\input\\surfaces\\PW_H10_Dec22_CACI_5176_M400000_QLSKPrSDM_SCF_balVIIIs_ismat4')\n",
    "X = surf1[:,0] \n",
    "Y = surf1[:,1] \n",
    "Z = surf1[:,2]\n",
    "plt.scatter(X,Y, c=Z)\n",
    "plt.colorbar(label='depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal8_1510_3 = gpd.read_file(r'C:\\jupyter\\SPP\\input\\surfaces\\petrel\\BalakhanyVIII_1510_base_3.shp')\n",
    "bal8_1510_1 = gpd.read_file(r'C:\\jupyter\\SPP\\input\\surfaces\\petrel\\BalakhanyVIII_1510_base_1.shp')\n",
    "bal8_1510_0 = gpd.read_file(r'C:\\jupyter\\SPP\\input\\surfaces\\petrel\\BalakhanyVIII_1510_base_0.shp')\n",
    "bal8_20_3 = gpd.read_file(r'C:\\jupyter\\SPP\\input\\surfaces\\petrel\\BalakhanyVIII_20_base_3.shp')\n",
    "bal8_1510_3 = bal8_1510_3.set_crs('EPSG:2499')\n",
    "bal8_1510_1 = bal8_1510_1.set_crs('EPSG:2499')\n",
    "bal8_1510_0 = bal8_1510_0.set_crs('EPSG:2499')\n",
    "bal8_20_3 = bal8_20_3.set_crs('EPSG:2499')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdl8_xy = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up == 'Balakhany VIII'][['well','X_mean','Y_mean']]\n",
    "bdl8_xy = bdl8_xy.groupby('well').mean().reset_index()\n",
    "geometry = [Point(xy) for xy in zip(bdl8_xy['X_mean'], bdl8_xy['Y_mean'])]\n",
    "bdl8_xy_gpd = gpd.GeoDataFrame(bdl8_xy, geometry=geometry).drop(['X_mean','Y_mean'], axis=1)\n",
    "buffers = bdl8_xy_gpd.buffer(250)\n",
    "buffers = gpd.GeoDataFrame(geometry=buffers)\n",
    "bdl8_xy_gpd = bdl8_xy_gpd.join(buffers, rsuffix='_plgn')\n",
    "bdl8_xy_gpd = gpd.GeoDataFrame(bdl8_xy_gpd, geometry='geometry_plgn').set_crs('EPSG:2499')\n",
    "bdl8_xy_gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_section = gpd.overlay(bal8_1510_3, bdl8_xy_gpd, how='intersection')\n",
    "cross_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "bal8_1510_3.plot(ax=ax, color='red', label='bal8_1510_3')\n",
    "# bal8_1510_1.plot(ax=ax, color='green', label='bal8_1510_1')\n",
    "# bal8_1510_0.plot(ax=ax, color='blue', label='bal8_1510_0')\n",
    "bal8_20_3.plot(ax=ax, color='orange', label='bal8_20_3')\n",
    "bdl8_xy_gpd.plot(ax=ax, markersize = 3, color='green', label='wells', alpha=0.5)\n",
    "# ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('Polygons of Balakhany VIII 20 15 10 body #3 & wells (buffer 250m)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
