{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.spatial import Delaunay\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import networkx as nx\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv,SAGEConv,global_add_pool,global_mean_pool,global_max_pool\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "from torch_geometric.data import Data\n",
    "import community as community_louvain\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import os\n",
    "from torch.utils.data import TensorDataset\n",
    "from scipy.interpolate import interp1d\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import statsmodels.api as sm\n",
    "from scipy.fftpack import fft, ifft\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import exp\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans, DBSCAN, MeanShift\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import seaborn as sns\n",
    "import mplcursors\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_by_depth(one_well,depth):\n",
    "    \n",
    "    one_well = one_well.sort_values(by=depth)\n",
    "    well_name = one_well[\"well\"].iloc[0]\n",
    "    \n",
    "    formation_name = one_well[\"FORMATION\"].iloc[0]\n",
    "    formation_up_name = one_well[\"FORMATION_up\"].iloc[0]\n",
    "    \n",
    "\n",
    "    data_range = np.floor((one_well[depth].max() - one_well[depth].min())/0.1)\n",
    "    starting_tvd = one_well[depth].iloc[0]\n",
    "    \n",
    "    new_depth_values = [starting_tvd + i*0.1 for i in range(1,int(data_range))]\n",
    "    \n",
    "    interp_X = interp1d(one_well[depth], one_well['X'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_Y = interp1d(one_well[depth], one_well['Y'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_PHIT = interp1d(one_well[depth], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_VSH = interp1d(one_well[depth], one_well['VSH'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_GR = interp1d(one_well[depth], one_well['GR_N'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_LPERM = interp1d(one_well[depth], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_KHtst = interp1d(one_well[depth], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "    interp_NET = interp1d(one_well[depth], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "\n",
    "\n",
    "    interpolated_NET = interp_NET(new_depth_values)\n",
    "    \n",
    "    \n",
    "    interpolated_NET = np.where(interpolated_NET >= 0.5, 1, 0)\n",
    "    # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "    new_data = {\n",
    "        'well': [well_name for _ in range(len(new_depth_values))],\n",
    "        depth: new_depth_values,\n",
    "        'X': interp_X(new_depth_values),\n",
    "        'Y': interp_Y(new_depth_values),\n",
    "        'PHIT': interp_PHIT(new_depth_values),\n",
    "        'VSH': interp_VSH(new_depth_values),\n",
    "        'GR_N':interp_GR(new_depth_values),\n",
    "        'LPERM':interp_LPERM(new_depth_values),\n",
    "        'KHtst':interp_KHtst(new_depth_values),\n",
    "        'NET_clp2':interpolated_NET\n",
    "    }\n",
    "\n",
    "    new_df = pd.DataFrame(new_data)\n",
    "    new_df[\"FORMATION\"] = formation_name\n",
    "    new_df[\"FORMATION_up\"] = formation_up_name\n",
    "        \n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def create_directory(parent_dir, new_dir_name):\n",
    "    \"\"\"\n",
    "    Create a new directory under the specified parent directory.\n",
    "\n",
    "    Parameters:\n",
    "    - parent_dir (str): The path to the parent directory.\n",
    "    - new_dir_name (str): The name of the new directory to be created.\n",
    "\n",
    "    Returns:\n",
    "    - str: The full path of the newly created directory.\n",
    "    \"\"\"\n",
    "    # Join the parent directory path with the new directory name\n",
    "    new_dir_path = os.path.join(parent_dir, new_dir_name)\n",
    "\n",
    "    try:\n",
    "        # Create the directory if it does not exist\n",
    "        os.makedirs(new_dir_path)\n",
    "        print(f\"Directory '{new_dir_name}' created successfully at '{parent_dir}'.\")\n",
    "        return new_dir_path\n",
    "    except FileExistsError:\n",
    "        print(f\"Directory '{new_dir_name}' already exists at '{parent_dir}'.\")\n",
    "        return new_dir_path\n",
    "\n",
    "def getPerm(PHI,tops):\n",
    "    PERM = None\n",
    "    \n",
    "    if PHI >=0.20: # EQ5\n",
    "        if  tops == 'Pereriv_D' or tops == 'Pereriv D':\n",
    "            PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\n",
    "        elif  tops == 'Pereriv_D_lower' or tops == 'Pereriv D lower':\n",
    "            PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\t\n",
    "        elif tops == 'Pereriv_E' or tops == 'Pereriv E':\n",
    "            PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\n",
    "        elif tops == 'Pereriv_A' or tops == 'Pereriv A':\n",
    "            PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "        elif tops == 'Pereriv_B' or tops == 'Pereriv B':\n",
    "            PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "        elif tops == 'NKG':\n",
    "            PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "        elif tops == 'Pereriv_C' or tops == 'Pereriv C':\n",
    "            PERM = (4.6201*((PHI*100)**2))-(14661.0*PHI)+1140\n",
    "        elif tops == 'Balakhany_VIII' or tops == 'Balakhany VIII':\n",
    "            PERM = (7.7925*((PHI*100)**2))-(29881.0*PHI)+2891.8\n",
    "        elif tops == 'Balakhany_VIII_sand' or tops == 'Balakhany VIII sand':\n",
    "            PERM = (7.7925*((PHI*100)**2))-(29881.0*PHI)+2891.8\n",
    "        else:\n",
    "            # PERM = (4.6201*((PHI*100)**2))-(14661.0*PHI)+1140 Old balX transform,\n",
    "            PERM = (7.7925*((PHI*100)**2))-(29881.0*PHI)+2891.8\n",
    "    elif PHI >= 0.19 and PHI < 0.20:  # EQ4\n",
    "            if  tops == 'Pereriv_D' or tops == 'Pereriv D':\n",
    "                PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\n",
    "            elif  tops == 'Pereriv_D_lower' or tops == 'Pereriv D lower':\n",
    "                PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\n",
    "            elif tops == 'Pereriv_E' or tops == 'Pereriv E':\n",
    "                PERM = (6.7356*((PHI*100)**2))-(19889.0*PHI)+1433.1\n",
    "            elif tops == 'Pereriv_A' or tops == 'Pereriv A':\n",
    "                PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "            elif tops == 'Pereriv_B' or tops == 'Pereriv B':\n",
    "                PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "            elif tops == 'NKG':\n",
    "                PERM = (9.4372*((PHI*100)**2))-(32902.0*PHI)+2929.7\n",
    "            elif tops == 'Pereriv_C' or tops == 'Pereriv C':\n",
    "                PERM = (4.6201*((PHI*100)**2))-(14661.0*PHI)+1140\n",
    "            elif tops == 'Balakhany_VIII' or tops == 'Balakhany VIII':\n",
    "                PERM = 0.00000002*(exp(PHI*105.56))\n",
    "            elif tops == 'Balakhany_VIII_sand' or tops == 'Balakhany VIII sand':\n",
    "                PERM = 0.00000002*(exp(PHI*105.56))\n",
    "            else:\n",
    "                #PERM = (4.6201*((PHI*100)**2))-(14661.0*PHI)+1140 Old balX transform,\n",
    "                PERM = 0.00000002*(exp(PHI*105.56))\n",
    "                \n",
    "    elif PHI >= 0.16 and PHI < 0.19:#EQ3\n",
    "        if  tops == 'Pereriv_D' or tops == 'Pereriv D':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif  tops == 'Pereriv_D_lower' or tops == 'Pereriv D lower':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_E' or tops == 'Pereriv E':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_A' or tops == 'Pereriv A':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_B' or tops == 'Pereriv B':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'NKG':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_C' or tops == 'Pereriv C':\n",
    "            PERM = 0.00000006*(exp(PHI*103.44))\n",
    "        elif tops == 'Balakhany_VIII' or tops == 'Balakhany VIII':\n",
    "            PERM = 0.00000002*(exp(PHI*105.56))\n",
    "        elif tops == 'Balakhany_VIII_sand' or tops == 'Balakhany VIII sand':\n",
    "            PERM = 0.00000002*(exp(PHI*105.56))\n",
    "        else:\n",
    "            #PERM = 0.0000000006*(exp(PHI*128.04)) Old balX transform,\n",
    "            PERM = 0.00000002*(exp(PHI*105.56))\n",
    "            \n",
    "    elif PHI >= 0.15 and PHI < 0.16:#EQ2\n",
    "        if  tops == 'Pereriv_D' or tops == 'Pereriv D':\n",
    "            PERM =  0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif  tops == 'Pereriv_D_lower' or tops == 'Pereriv D lower':\n",
    "            PERM =  0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_E' or tops == 'Pereriv E':\n",
    "            PERM =  0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_A' or tops == 'Pereriv A':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_B' or tops == 'Pereriv B':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'NKG':\n",
    "            PERM = 0.0000029157*(exp(PHI*89.8991405))\n",
    "        elif tops == 'Pereriv_C' or tops == 'Pereriv C':\n",
    "            PERM = 0.0293*(exp(PHI*22.07))\n",
    "        elif tops == 'Balakhany_VIII' or tops == 'Balakhany VIII':\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "        elif tops == 'Balakhany_VIII_sand' or tops == 'Balakhany VIII sand':\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "        else:\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "    elif PHI == -9999:\n",
    "        PERM = -9999\n",
    "    else:\n",
    "        if  tops == 'Pereriv_D' or tops == 'Pereriv D':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif  tops == 'Pereriv_D_lower' or tops == 'Pereriv D lower':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif tops == 'Pereriv_E' or tops == 'Pereriv E':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif tops == 'Pereriv_A' or tops == 'Pereriv A':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif tops == 'Pereriv_B' or tops == 'Pereriv B':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif tops == 'NKG':\n",
    "            PERM = 0.055*(exp(PHI*24.43))\n",
    "        elif tops == 'Pereriv_C' or tops == 'Pereriv C':\n",
    "            PERM = 0.0293*(exp(PHI*22.07))\n",
    "        elif tops == 'Balakhany_VIII' or tops == 'Balakhany VIII':\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "        elif tops == 'Balakhany_VIII_sand' or tops == 'Balakhany VIII sand':\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "        else:\n",
    "            PERM = 0.0159*(exp(PHI*21.27))\n",
    "            \n",
    "            \n",
    "    return PERM\n",
    "\n",
    "\n",
    "def calculate_KH(well,suffix):\n",
    "    \n",
    "    well = well.copy()\n",
    "    \n",
    "    new_LPERMs = []\n",
    "    for (indx,row) in well.iterrows():\n",
    "        \n",
    "        perm = 0\n",
    "        \n",
    "        if row[\"NET_clp2\"] == 1:\n",
    "            perm = getPerm(row[f\"PHIT_{suffix}\"],row[\"FORMATION\"])\n",
    "            \n",
    "        new_LPERMs.append(perm)\n",
    "        \n",
    "    well[f\"LPERM_{suffix}\"] = new_LPERMs\n",
    "    well[f\"KHtst_{suffix}\"] =  well[f\"LPERM_{suffix}\"] * 0.1\n",
    "    well[f\"KHtst_{suffix}_sum\"] = well.loc[::-1, f\"KHtst_{suffix}\"].cumsum()[::-1]\n",
    "    \n",
    "    \n",
    "    return well\n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(1, 256, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64, 128, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(128, 256, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(256, 1, kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def slice_dataframe(df, col_selection1=\"VSH\", col_selection2=\"PHIT\", chunk_size=100, stride=100):\n",
    "    selection1_slices = []\n",
    "    selection2_slices = []\n",
    "    general_slice_data = []\n",
    "\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < len(df):\n",
    "        end_index = start_index + chunk_size\n",
    "        slice_data = df.iloc[start_index:end_index]\n",
    "\n",
    "        if len(slice_data) == chunk_size:\n",
    "            selection1_slices.append(slice_data[col_selection1].values.flatten())\n",
    "            selection2_slices.append(slice_data[col_selection2].values.flatten())\n",
    "            general_slice_data.append(slice_data)\n",
    "\n",
    "        start_index += stride\n",
    "\n",
    "    return selection1_slices, selection2_slices, general_slice_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def slice_dataframe_multiple(df, col_selection1=[\"X\",\"Y\",\"TST\",\"VSH\"], col_selection2=\"PHIT\", chunk_size=100, stride=100):\n",
    "    selection1_slices = []\n",
    "    selection2_slices = []\n",
    "    general_slice_data = []\n",
    "\n",
    "    start_index = 0\n",
    "\n",
    "    while start_index < len(df):\n",
    "        end_index = start_index + chunk_size\n",
    "        slice_data = df.iloc[start_index:end_index]\n",
    "\n",
    "        if len(slice_data) == chunk_size:\n",
    "        \n",
    "            selection1_slices.append(slice_data[col_selection1].values.T)\n",
    "            selection2_slices.append(slice_data[col_selection2].values.flatten())\n",
    "            general_slice_data.append(slice_data)\n",
    "\n",
    "        start_index += stride\n",
    "\n",
    "    return selection1_slices, selection2_slices, general_slice_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def seeding(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_column(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Normalize a specific column in a DataFrame using MinMaxScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame\n",
    "    - column_name: str, the name of the column to be normalized\n",
    "\n",
    "    Returns:\n",
    "    - normalized_column: pandas Series, the normalized column\n",
    "    - scaler: MinMaxScaler, the scaler used for normalization\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    column_data = dataframe[column_name].values.reshape(-1, 1)\n",
    "    normalized_column = scaler.fit_transform(column_data).flatten()\n",
    "    return pd.Series(normalized_column, name=column_name + '_normalized'), scaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def denormalize_column(normalized_column, scaler):\n",
    "    \"\"\"\n",
    "    Denormalize a column using the provided MinMaxScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - normalized_column: pandas Series, the normalized column to be denormalized\n",
    "    - scaler: MinMaxScaler, the scaler used for normalization\n",
    "\n",
    "    Returns:\n",
    "    - denormalized_column: pandas Series, the denormalized column\n",
    "    \"\"\"\n",
    "    normalized_data = normalized_column.reshape(-1, 1)\n",
    "    denormalized_column = scaler.inverse_transform(normalized_data).flatten()\n",
    "    return denormalized_column\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dataset = pd.read_parquet(\"data/ACG_wells_JOINT_BEST_v10.parquet.gzip\")\n",
    "# dataset.rename(columns={'wellName': 'well'}, inplace=True)\n",
    "\n",
    "dataset = pd.read_csv(\"df_bal_net2_kh.csv\")\n",
    "dataset = dataset[[\"well\",\"FORMATION\",\"FORMATION_up\",\"PHIT\",\"VSH\",\"GR_N\",\"LPERM\",\"NET_clp2\",\"KHtst\",\"TVD_SCS\",\"TST\",\"X\",\"Y\"]]\n",
    "valid_wells = pd.read_csv('well_clean_v2_df.csv')\n",
    "valid_wells = valid_wells.groupby('FORMATION_up')['well'].agg(list).to_dict()\n",
    "\n",
    "well_names_to_exclude = sorted(list(set(valid_wells[\"Balakhany VIII\"]).union(set(valid_wells[\"Balakhany X\"]))))\n",
    "# well_names_to_exclude = valid_wells[\"Balakhany X\"]\n",
    "\n",
    "normalized_phit , phit_scaler = normalize_column(dataset[(dataset[\"PHIT\"] >= 0.05) & (dataset[\"PHIT\"] <= 0.3)],\"PHIT\")\n",
    "\n",
    "\n",
    "for exclude_well in well_names_to_exclude:\n",
    "    \n",
    "    print(exclude_well)\n",
    "    \n",
    "    well_names = list(dataset[\"well\"].unique())\n",
    "    formations = dataset['FORMATION_up'].unique()\n",
    "#     formations = [\"Balakhany X\"]\n",
    "    all_formations_smoothings = []\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    general = []\n",
    "\n",
    "    for i,formation in enumerate(formations):\n",
    "        print(formation)\n",
    "\n",
    "\n",
    "        current_dataset = dataset[dataset['FORMATION_up'].str.contains(formation)]\n",
    "\n",
    "\n",
    "        current_well_names = list(current_dataset[\"well\"].unique())\n",
    "\n",
    "\n",
    "        for name in valid_wells[formation]:\n",
    "\n",
    "            if name == exclude_well:\n",
    "                continue\n",
    "\n",
    "\n",
    "            one_well = current_dataset[current_dataset[\"well\"]==name].copy()\n",
    "\n",
    "            one_well = interpolate_by_depth(one_well,\"TST\")\n",
    "\n",
    "\n",
    "            one_well[\"PHIT_normalized\"] = phit_scaler.transform(one_well[\"PHIT\"].values.reshape(-1, 1)).flatten()\n",
    "\n",
    " \n",
    "            VSH_slices , phit_slices, general_slices = slice_dataframe_multiple(one_well, col_selection1 =\"VSH\", col_selection2 = \"PHIT_normalized\" ,chunk_size=100,stride=10)\n",
    "\n",
    "\n",
    "            X = X + VSH_slices\n",
    "            y = y + phit_slices\n",
    "\n",
    "            general = general + general_slices\n",
    "\n",
    "\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, shuffle=True, random_state=1234)\n",
    "\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "    Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "    num_epochs = 200\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    seeding(1234)\n",
    "    \n",
    "\n",
    "    model = Autoencoder()\n",
    "\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch_x, batch_y in train_dataloader:\n",
    "            batch_x, batch_y = batch_x.unsqueeze(1).to(device), batch_y.unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch_x)\n",
    "            loss = criterion(out.view(-1), batch_y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_dataloader:\n",
    "                batch_x, batch_y = batch_x.unsqueeze(1).to(device), batch_y.unsqueeze(1).to(device)\n",
    "                out = model(batch_x)\n",
    "                loss = criterion(out.view(-1), batch_y.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        average_train_loss = total_train_loss / len(train_dataloader)\n",
    "        average_val_loss = total_val_loss / len(test_dataloader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {average_train_loss:.8f}, Validation Loss: {average_val_loss:.8f}')\n",
    "\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            checkpoint_path = f'Base/{exclude_well}_base_autoencoder_checkpoint.pth'\n",
    "            torch.save({'epoch': epoch + 1, 'model_state_dict': best_model_state, 'optimizer_state_dict': optimizer.state_dict(), 'val_loss': best_val_loss}, checkpoint_path)\n",
    "            print(f'Saved checkpoint at epoch {epoch + 1} with validation loss: {best_val_loss:.8f}')\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "    for i,formation in enumerate(formations):\n",
    "        print(formation)\n",
    "\n",
    "\n",
    "        current_dataset = dataset[dataset['FORMATION_up'].str.contains(formation)]\n",
    "\n",
    "\n",
    "        current_well_names = list(current_dataset[\"well\"].unique())\n",
    "\n",
    "\n",
    "        for name in valid_wells[formation]:\n",
    "\n",
    "            if name != exclude_well:\n",
    "                continue\n",
    "\n",
    "\n",
    "            one_well = current_dataset[current_dataset[\"well\"]==name].copy()\n",
    "\n",
    "        one_well = interpolate_by_depth(one_well,\"TST\")\n",
    "\n",
    "\n",
    "        one_well[\"PHIT_normalized\"] = phit_scaler.transform(one_well[\"PHIT\"].values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "\n",
    "        VSH_slices , phit_slices, general_slices = slice_dataframe_multiple(one_well, col_selection1 =\"VSH\", col_selection2 = \"PHIT_normalized\" ,chunk_size=100,stride=100)\n",
    "\n",
    "        VSH_slices = np.array(VSH_slices)\n",
    "        phit_slices = np.array(phit_slices)\n",
    "\n",
    "\n",
    "        X_tensor = torch.tensor(VSH_slices, dtype=torch.float32)\n",
    "        Y_tensor = torch.tensor(phit_slices, dtype=torch.float32)\n",
    "\n",
    "        dataset_ = TensorDataset(X_tensor, Y_tensor)\n",
    "        dataloader = DataLoader(dataset_, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in dataloader:\n",
    "                batch_x = batch_x.unsqueeze(1).to(device)\n",
    "                out = model(batch_x)\n",
    "                predictions.append(out.cpu().numpy().flatten())\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "        predictions = denormalize_column(predictions, phit_scaler)\n",
    "\n",
    "        one_well = one_well.iloc[:len(predictions),:]\n",
    "        one_well[\"PHIT_predicted\"] = predictions\n",
    "        one_well = calculate_KH(one_well,\"predicted\")\n",
    "\n",
    "        print(\"Original KH: \",one_well[\"KHtst\"].dropna().max())\n",
    "        print(\"Predicted KH: \",one_well[\"KHtst_predicted_sum\"].dropna().max())\n",
    "\n",
    "        phit_slices = phit_slices.flatten()\n",
    "        phit_slices = denormalize_column(phit_slices, phit_scaler)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(6, 12))\n",
    "        plt.plot(phit_slices,[i for i in range(len(predictions))], label='Original X', linestyle='--',markersize=3)\n",
    "        plt.plot(predictions,[i for i in range(len(predictions))], label='Predicted X', linestyle='--',markersize=3)\n",
    "        plt.axvline(x=0.13, color='red', linestyle='--', label='PHIT = 0.13')\n",
    "\n",
    "\n",
    "        plt.title(f'{exclude_well} ({formation}) Original KH: {one_well[\"KHtst\"].dropna().max():.1f} | Predicted KH: {one_well[\"KHtst_predicted_sum\"].dropna().max():.1f}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.savefig(f'Base/{exclude_well}_{formation}_base.png')\n",
    "        plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
