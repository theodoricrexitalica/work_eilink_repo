{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libs \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as st\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import gmean\n",
    "from scipy import stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.lines as mlines\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as go_offline\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import textwrap\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score as r2, mean_absolute_error as mae, mean_squared_error as mse, accuracy_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from xgboost import XGBRegressor\n",
    "from datetime import datetime\n",
    "import random\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "pd.set_option('display.max_columns', 15)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, Dense, Dropout, Conv1DTranspose, Conv2DTranspose\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the ACG_wells_JOINT_BEST_v10.csv file\n",
    "# path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "# data_init = pd.read_csv(path + 'ACG_wells_JOINT_BEST_v10.csv', sep=',')\n",
    "# df = data_init.copy()\n",
    "# df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Select only neccessary data\n",
    "# df_cln = df[[   'wellName', 'MD', 'BADPORLOG', 'Casings', 'FORMATION', 'DEVI', 'HAZI',\n",
    "#                 'FLANK', 'FLANK1', 'FLANK2', 'NET', 'NET_VSH','FLUIDS',\n",
    "#                 'LPERM', 'PHIT', \n",
    "#                 'GR_N', 'GRMATRIX', 'GRSHALE','VSH', 'NPSS', 'RHOB', 'RHOF', 'RHOMA', \n",
    "#                 'RDEEP',  'SON', 'SONSH', \n",
    "#                 'TVD_SCS','TST', 'X', 'Y']]\n",
    "# #Fill up nan and -9999 values with 0\n",
    "# df_cln = df_cln.fillna(0)\n",
    "# df_cln = df_cln.replace(-9999, 0)\n",
    "# df_cln = df_cln.replace('-9999', '0')\n",
    "# #Assing proper datatypes for df\n",
    "# dicttypes = {   'wellName':'string', 'MD':'float', 'BADPORLOG':'int', 'Casings':'float', 'FORMATION':'string','DEVI':'float', 'HAZI':'float',\n",
    "#                 'FLANK':'int', 'FLANK1':'int', 'FLANK2':'int', 'NET':'int', 'NET_VSH':'int', 'FLUIDS':'int',\n",
    "#                 'LPERM':'float','PHIT':'float',\n",
    "#                 'GR_N':'float', 'GRMATRIX':'float', 'GRSHALE':'float', 'VSH':'float', 'NPSS':'float','RHOB':'float','RHOF':'float', 'RHOMA':'float',\n",
    "#                 'RDEEP':'float', 'SON':'float', 'SONSH':'float',\n",
    "#                 'TVD_SCS':'float', 'TST':'float', 'X':'float', 'Y':'float'}\n",
    "# df_cln = df_cln.astype(dicttypes, errors='ignore')\n",
    "# df_cln.loc[df_cln.FORMATION=='0', 'FORMATION']='None'\n",
    "# # Save data to parquet\n",
    "# df_cln.to_parquet('ACG_wells_JOINT_BEST_v10.parquet.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading metadata, distribution wells per Platforms and all the that.\n",
    "def metadata_parquet_loading():\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    metadata_init = pd.read_csv(path + 'ACG_wells_metadata.csv', sep=',')\n",
    "    metadata = metadata_init.copy()\n",
    "    metadata = metadata.rename(columns={'X':'X_wellhead', 'Y':'Y_wellhead'})\n",
    "    metadata.Status = metadata.Status.str.strip()\n",
    "    metadata.Status = metadata.Status.str.lower()\n",
    "    metadata.loc[metadata.Status == 'oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'oil producer', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'produiction oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'production_oil', 'Status' ] = 'production oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned production oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned  oil', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'abandoned oi', 'Status' ] = 'abandoned oil'\n",
    "    metadata.loc[metadata.Status == 'injector  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injector water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'injetor  - water', 'Status' ] = 'injector - water'\n",
    "    metadata.loc[metadata.Status == 'abandoned injector - water per b', 'Status' ] = 'abandoned injector - water'\n",
    "    metadata.loc[metadata.Status == 'plugged and abandoned', 'Status' ] = 'p&a'\n",
    "    metadata.loc[metadata.X_wellhead==118.270, 'X_wellhead'] = 526258.84\n",
    "    metadata.loc[metadata.Y_wellhead==526261.510, 'Y_wellhead'] = 4435802.01\n",
    "    metadata.loc[metadata.well=='C39', 'X_wellhead'] = 526258.840\n",
    "    metadata.loc[metadata.well=='C39', 'Y_wellhead'] = 4435802.010\n",
    "    metadata.loc[metadata.field=='West Azeri', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.field=='COP', 'field'] = 'WEST CHIRAG'\n",
    "    metadata.loc[metadata.well=='AZERI2', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='AZERI3', 'field'] = 'WEST AZERI'\n",
    "    metadata.loc[metadata.well=='B31', 'field'] = 'CENTRAL AZERI'\n",
    "    metadata.loc[metadata.well=='J28_bpQIP', 'field'] = 'WEST CHIRAG'\n",
    "\n",
    "    #Read data from parquet\n",
    "    path = 'C:\\\\jupyter\\\\SPP\\\\input\\\\'\n",
    "    df_prq = pd.read_parquet(path + 'ACG_wells_JOINT_BEST_v10.parquet.gzip')\n",
    "    df_prq.rename(columns={'wellName':'well'}, inplace=True)\n",
    "    df_prq = df_prq.set_index('well').join(metadata.set_index('well')).reset_index()\n",
    "    # print('wells in df totally:', len(df_prq.well.unique()))\n",
    "    # Filter data with bad_well_list \n",
    "    bad_well_list = ['E10Z','Predrill_J01Z', 'Predrill_J08', 'J28_bpQIP', 'A01W_2']\n",
    "    df_prq = df_prq[~df_prq.well.isin(bad_well_list)]\n",
    "    #Assign any Fluidcode_mod number by variable gross_pay=1 and gross_pay=0 if Fluidcode_mod as NaN\n",
    "    df_prq.loc[df_prq.FLUIDS>0, 'FLUIDS_int'] = 1\n",
    "    df_prq.loc[df_prq.FLUIDS<=0, 'FLUIDS_int'] = 0\n",
    "    df_prq.FLUIDS_int = df_prq.FLUIDS_int.astype('int')\n",
    "    # Unite of FU for each formation\n",
    "\n",
    "    df_bal = df_prq[df_prq.FORMATION.str.contains('Balakhany')]\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany VIII'), 'FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_bal.loc[df_bal.FORMATION.str.contains('Balakhany X'), 'FORMATION_up'] = 'Balakhany X'\n",
    "    df_bal = df_bal[df_bal.FORMATION_up.notna()]\n",
    "    #Getting XY mean coords of Balakhany formation\n",
    "    xy_coord_mean = df_bal[['well', 'FORMATION_up', 'X', 'Y']]\n",
    "    xy_coord_mean = xy_coord_mean.groupby(['well', 'FORMATION_up']).agg({'X': 'mean', 'Y':'mean'}).reset_index()\n",
    "    xy_coord_mean = xy_coord_mean.rename(columns={'X':'X_mean', 'Y':'Y_mean'})\n",
    "    xy_coord_mean = xy_coord_mean[xy_coord_mean.FORMATION_up.str.contains('Balakhany') & (xy_coord_mean.X_mean>0) & (xy_coord_mean.Y_mean>0)]\n",
    "    df_bal.rename(columns={'X':'X_traj', 'Y':'Y_traj'}, inplace=True)\n",
    "    df_bal = df_bal.set_index(['well', 'FORMATION_up']).join(xy_coord_mean.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "    return df_bal\n",
    "df_bal = metadata_parquet_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display in TST well plots with logging curves\n",
    "def well_display_khtst( dataset, wellname, fmname, net_var, comments, \n",
    "                        ref_depth, fm_flag, depth_step, kh_include, print):\n",
    "    \"\"\"\n",
    "    dataset = df_bal or something else\n",
    "    net_var = NET or FLUIDS_int\n",
    "    comments = put what you want\n",
    "    ref_depth = MD or TST\n",
    "    fm_flag = 1 if you need a FORMATION_up, 0 if just a simple FORMATION\n",
    "    depth_step = step for ticks on the diagramm\n",
    "    kh_include = 1 if we have KHtst in dataset, 0 if there is not KHtst\n",
    "    print = 1 if we want to print the plot\n",
    "    \"\"\"\n",
    "    if fm_flag == 0:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION == fmname)]\n",
    "    if fm_flag == 1:\n",
    "        data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    depth = data[ref_depth]\n",
    "    grn = data['GR_N']\n",
    "    vsh = data['VSH']\n",
    "    rhob = data['RHOB'] \n",
    "    npss = data['NPSS']\n",
    "    rdeep = data['RDEEP']\n",
    "    phit = data['PHIT'] \n",
    "    net = data[net_var]\n",
    "    perm = data['LPERM']\n",
    "    if kh_include == 1:\n",
    "        kh = data['KHtst']\n",
    "    else:\n",
    "        data['KHtst'] = 0\n",
    "        kh = data['KHtst']\n",
    "    fig, ax = plt.subplots(1,4, figsize=(7,7), sharey=True)\n",
    "    well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "    ax[0].plot(grn, depth, color='lightgreen', lw=3, zorder=10)\n",
    "    ax[0].invert_yaxis() \n",
    "    ax[0].set_xlim(0, 150) \n",
    "    ax[0].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "    twin0 = ax[0].twiny()\n",
    "    twin0.plot(vsh, depth, color='black', alpha=0.5, zorder=5)\n",
    "    twin0.set_xlim(0, 1.5)\n",
    "    ax[1].plot(rhob, depth, color='red') \n",
    "    ax[1].invert_yaxis() \n",
    "    ax[1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "    ax[1].set_xlim(1.65, 2.65)\n",
    "    ax[1].grid(axis='y'), ax[1].grid(axis='x')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[1].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "        xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "        ax[1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "    twin1 = ax[1].twiny()\n",
    "    twin1.plot(npss, depth, color='blue')\n",
    "    twin1.set_xlim(0.6, 0)\n",
    "    # ax[2].plot(rdeep, depth, color='black'), ax[2].set_xscale('log'), ax[2].set_xlim(0.1, 50), ax[2].invert_yaxis(), ax[2].grid(axis='x', which='both')\n",
    "    ax[2].plot(phit, depth, color='green', linestyle='dashed'), ax[2].set_xlim(0.3, 0), ax[2].grid(axis='x') \n",
    "    ax[2].invert_yaxis()\n",
    "    ax[2].grid(axis='y')\n",
    "    ax[2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "    twin2 = ax[2].twiny()\n",
    "    twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "    twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "    twin2.set_xlim(0, 1)\n",
    "    twin2.set_ylim(min(depth), max(depth))\n",
    "    ax[3].plot(perm, depth, color='purple', alpha=0.66), ax[3].set_xscale('log'), ax[3].set_xlim(0.1, 1000)\n",
    "    ax[3].invert_yaxis()\n",
    "    ax[3].grid(axis='y')\n",
    "    for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "        ax[3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.66)\n",
    "    twin4 = ax[3].twiny()\n",
    "    twin4.plot(kh, depth, color='black', alpha=1)\n",
    "    fig.suptitle(wellname + ' ' + fmname + ' ' + ref_depth + ' ' + str(round(max(kh.dropna()),0)) + ' ' + str(comments), fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    if print == 1:\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\wellplots\\\\'\n",
    "        fig.savefig(path + fmname.replace(' ','') + '_' + wellname + '.png')\n",
    "    else:\n",
    "        pass\n",
    "# Draw a map\n",
    "def map_value_2plots(metadata, dataset, formation, value, color, multi_chr = 0.001, multi_azr = 0.001):\n",
    "    \"\"\"\n",
    "    metadata, \n",
    "    dataset = dataset with X & Y, \n",
    "    formation = 'Balakhany VIII',  \n",
    "    value = for example 'KHtst' or 'tst_interv'\n",
    "    multi_chr = 0.001, multi_azr = 0.001\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=('crg: ' + str(multi_chr), 'azr: ' + str(multi_azr)), \n",
    "                        vertical_spacing = 0.025)\n",
    "    azr_lst = ['CENTRAL AZERI', 'WEST AZERI', 'EAST AZERI']\n",
    "    chg_lst = ['CHIRAG', 'DWG', 'DDGG', 'WEST CHIRAG']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    field_avg_coord_chg = field_avg_coord[field_avg_coord.field.isin(chg_lst)]\n",
    "    field_avg_coord_azr = field_avg_coord[field_avg_coord.field.isin(azr_lst)] \n",
    "    df_chg = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(chg_lst))]\n",
    "    df_azr = dataset[(dataset.FORMATION_up == formation) & (dataset.field.isin(azr_lst))]\n",
    "    fig.add_trace(go.Scatter(x=df_chg.X, y=df_chg.Y, customdata = df_chg[['well', value, color]],\n",
    "                            marker=dict(color=df_chg[color], size=df_chg[value]*multi_chr, colorscale='Viridis_r',  showscale=True,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_chg.X_wellhead, y=field_avg_coord_chg.Y_wellhead, customdata = field_avg_coord_chg[['field']],\n",
    "                            text=field_avg_coord_chg['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df_azr.X, y=df_azr.Y, customdata = df_azr[['well', value, color]],\n",
    "                            marker=dict(color=df_azr[color], size=df_azr[value]*multi_azr, colorscale='Viridis_r',  showscale=False,\n",
    "                            line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            mode='markers', hovertemplate=\"\".join([\"well:%{customdata[0]}, value:%{customdata[1]}, color:%{customdata[2]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=field_avg_coord_azr.X_wellhead, y=field_avg_coord_azr.Y_wellhead, customdata = field_avg_coord_azr[['field']],\n",
    "                            text=field_avg_coord_azr['field'], textposition=\"middle right\",\n",
    "                            marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                            mode='markers+text', \n",
    "                            marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])),\n",
    "                            row=2, col=1)\n",
    "    fig.update_layout(  title_text= ('formation: ' + str(formation) + ' value: ' + str(value) + ' color: ' + str(color)),\n",
    "                        autosize=True, width=1300, height=1400, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Calculation NTD\n",
    "def ntd_calculation_big(dataset, desired_fm, net_var='NET'):\n",
    "    df_lst = []\n",
    "    for well_in_loop in tqdm(dataset.well.unique()[:]):\n",
    "        well_lst = []\n",
    "        data = dataset[(dataset.well==well_in_loop)]\n",
    "        data.iloc[0, 3] = 0\n",
    "        data.iloc[-1, 3] = 0\n",
    "        tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "        tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                    if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "        for k in range(len(tst_top)):\n",
    "            if (round(tst_top[k],1) == round(tst_bot[k],1)):\n",
    "                h_tst = 0 \n",
    "            elif (round(tst_bot[k],1) == round(tst_top[k]+0.1,1)):\n",
    "                h_tst = 0\n",
    "            else:\n",
    "                h_tst = (round((tst_bot[k] - tst_top[k]),1))\n",
    "                md_perm = []\n",
    "                md_phit = []\n",
    "                md_vsh = []\n",
    "                for i in range(len(data)):\n",
    "                    if round(data.iloc[i]['TST'],1) >= round(tst_top[k],1) and round(data.iloc[i]['TST'],1) <= round(tst_bot[k],1):\n",
    "                        md_perm.append(data.iloc[i]['LPERM'])\n",
    "                        md_phit.append(data.iloc[i]['PHIT'])\n",
    "                        md_vsh.append(data.iloc[i]['VSH'])\n",
    "                if len(md_perm) == 0:\n",
    "                    md_perm.append(0)\n",
    "                if len(md_phit) == 0:\n",
    "                    md_phit.append(0)\n",
    "                if len(md_vsh) == 0:\n",
    "                    md_vsh.append(0)\n",
    "                well_lst.append([data.iloc[0]['well'], h_tst, tst_top[k], tst_bot[k], round(mean(md_perm),0), round(mean(md_phit),2), round(mean(md_vsh),2)])\n",
    "            df_tst = pd.DataFrame(well_lst, columns = ['well', 'h_tst', 'top_tst', 'bot_tst', 'md_perm_avg', 'md_phit_avg', 'md_vsh_avg'])\n",
    "        df_lst.append(df_tst)\n",
    "    ntd_bal = pd.concat(df_lst)\n",
    "    ntd_bal['FORMATION_up'] = desired_fm\n",
    "    return ntd_bal\n",
    "def ntd_calculation_brief(dataset,well,desired_fm, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==desired_fm)]\n",
    "    data.iloc[0, 3] = 0\n",
    "    data.iloc[-1, 3] = 0\n",
    "    tst_top = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i-1][net_var]==0)]\n",
    "    tst_bot = [data.iloc[i]['TST'] for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 1 and data.iloc[i+1][net_var]==0)]\n",
    "    tops = zip(tst_top, tst_bot)\n",
    "    df_htst = pd.DataFrame(tops, columns=['tst_top', 'tst_bot'])\n",
    "    df_htst['FORMATION_up'] = desired_fm\n",
    "    df_htst['well'] = well\n",
    "    df_htst['h_tst'] = df_htst.tst_bot - df_htst.tst_top\n",
    "    df_htst = df_htst[['well','FORMATION_up','tst_top','tst_bot','h_tst']]\n",
    "    return df_htst\n",
    "# Calculation NTD zero\n",
    "def ntd_calculation_zero(dataset,well,formation, net_var='NET'):\n",
    "    data = dataset[(dataset.well==well) & (dataset.FORMATION_up==formation)]\n",
    "    data.iloc[0, 3] = 1\n",
    "    data.iloc[-1, 3] = 1\n",
    "    tst_zero_top = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1)\n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i-1][net_var] == 1)]\n",
    "    tst_zero_bot = [data.iloc[i]['TST'].round(3) for i in range(len(data)-1) \n",
    "                if (data.iloc[i][net_var] == 0 and data.iloc[i+1][net_var] == 1)]\n",
    "    tops_zero = zip(tst_zero_top, tst_zero_bot)\n",
    "    df_zero_htst = pd.DataFrame(tops_zero, columns=['tst_zero_top', 'tst_zero_bot'])\n",
    "    df_zero_htst['FORMATION_up'] = formation\n",
    "    df_zero_htst['well'] = well\n",
    "    df_zero_htst['h_tst_zero'] = df_zero_htst.tst_zero_bot - df_zero_htst.tst_zero_top\n",
    "    df_zero_htst = df_zero_htst[['well','FORMATION_up','tst_zero_top','tst_zero_bot','h_tst_zero']]\n",
    "    return df_zero_htst\n",
    "# Print numerical table with layers\n",
    "def ntd_numerical(dataset, wellname, fmname):\n",
    "    \"\"\"\n",
    "    dataset = ntd_final\n",
    "    \"\"\"\n",
    "    df = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname) ][['well','h_tst','top_tst', 'bot_tst','FORMATION_up']]\n",
    "    q50 = df['h_tst'].quantile(q=0.5, interpolation='nearest')\n",
    "    df['q50'] = q50\n",
    "    return df\n",
    "#Cleaning NET variable and making up NET_clp with clipped data\n",
    "def ntd_htst_cleaning(dataset, cutoff):\n",
    "    \"\"\"\n",
    "    dataset - any updated dataset like df_bal...\n",
    "    cutoff - value in TST to remove layers with thickness below cutoff\n",
    "    \"\"\"\n",
    "    df_list_ntd = []\n",
    "    for well in tqdm(dataset.well.unique()):\n",
    "        ntd_well = dataset[(dataset.well ==well)]\n",
    "        ntd_well_cutoff = ntd_well[ntd_well.h_tst >= cutoff]\n",
    "        well_short = df_bal[['well', 'FORMATION_up', 'MD', 'TST', 'GR_N', 'NET', 'FORMATION']]\n",
    "        net_well = well_short[(well_short.well==well)]\n",
    "        net_well['NET_clp'] = 0\n",
    "        for j in range(len(ntd_well_cutoff.well)):\n",
    "            ntd_top = ntd_well_cutoff.iloc[j, 2].round(3)\n",
    "            ntd_bot = ntd_well_cutoff.iloc[j, 3].round(3)\n",
    "            for i in range(len(net_well.TST)):\n",
    "                well_tst = net_well['TST'].iloc[i].round(3)\n",
    "                if well_tst >= ntd_top and well_tst <= ntd_bot:\n",
    "                    net_well['NET_clp'].iloc[i] = 1\n",
    "        df_list_ntd.append(net_well)\n",
    "    net_clp = pd.concat(df_list_ntd)\n",
    "    return net_clp\n",
    "# Cleaning NET_clp variable from zero values with zero_samples <=cutoff\n",
    "def ntd_htst_zero_cleaning(dataset_zero, dataset, cutoff, net_var1, net_var2):\n",
    "    df_list_ntd_zero = []\n",
    "    for well in tqdm(dataset_zero.well.unique()):\n",
    "        ntd_well_zero = dataset_zero[(dataset_zero.well ==well)]\n",
    "        ntd_well_zero_sel = ntd_well_zero[ntd_well_zero.h_tst_zero <= cutoff]\n",
    "        well_zero_short = dataset[['well','FORMATION_up','MD','TST', net_var1, 'GR_N', 'NET', 'FORMATION']]\n",
    "        well_zero_short[net_var2] = well_zero_short[net_var1]\n",
    "        well_zero_sel = well_zero_short[(well_zero_short.well==well)]\n",
    "        for j in range(len(ntd_well_zero_sel.well)):\n",
    "            ntd_zero_top = ntd_well_zero_sel.iloc[j, 2].round(3)\n",
    "            ntd_zero_bot = ntd_well_zero_sel.iloc[j, 3].round(3)\n",
    "            for i in range(len(well_zero_sel.TST)):\n",
    "                well_zero_tst = well_zero_sel['TST'].iloc[i].round(3)\n",
    "                if well_zero_tst >= ntd_zero_top and well_zero_tst <= ntd_zero_bot:\n",
    "                    well_zero_sel[net_var2].iloc[i] = 1\n",
    "        df_list_ntd_zero.append(well_zero_sel)\n",
    "    result = pd.concat(df_list_ntd_zero)\n",
    "    return result\n",
    "# View desired TST-interval\n",
    "def net_view1(dataset, well, top, bot):\n",
    "    dataset = dataset[dataset.well==well][['well','TST','GR_N', 'RHOB', 'NET','NET_clp']]\n",
    "    return dataset[(dataset.TST >= top) & (dataset.TST <= bot)].head(50)\n",
    "#TST sampling & TST KH curve calculation per formation/well\n",
    "def proph_calculation(dataset, net_var):\n",
    "    df_smpl_lst = []\n",
    "    print('TST sampling calculation')\n",
    "    for well_smpl in tqdm(dataset.well.unique()[:]):\n",
    "        tst_sampl = dataset[dataset.well==well_smpl]['TST'].diff()\n",
    "        df_new = dataset[dataset.well==well_smpl].join(tst_sampl, rsuffix='_smpl')    \n",
    "        df_smpl_lst.append(df_new)\n",
    "    df_bal_tst_smpl = pd.concat(df_smpl_lst)\n",
    "    df_kh_lst_fm = []\n",
    "    print('KHtst calculation')\n",
    "    for fm_kh in ['Balakhany VIII', 'Balakhany X']:\n",
    "        df_kh_lst = []\n",
    "        for well_kh in tqdm(dataset.well.unique()[:]):\n",
    "            well_tst_perm = df_bal_tst_smpl[(df_bal_tst_smpl.well==well_kh) & \n",
    "                                            (df_bal_tst_smpl.FORMATION_up==fm_kh)].sort_values(by='MD', ascending=False)\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'LPERM'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'PHIT'] = 0\n",
    "            well_tst_perm.loc[well_tst_perm[net_var] == 0, 'VSH'] = 0\n",
    "            well_tst_perm['khtst'] = well_tst_perm.LPERM*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['phithtst'] = well_tst_perm.PHIT*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['vshhtst'] = well_tst_perm.VSH*well_tst_perm.TST_smpl\n",
    "            well_tst_perm['KHtst'] = well_tst_perm.khtst.cumsum()\n",
    "            well_tst_perm['PHITHtst'] = well_tst_perm.phithtst.cumsum()\n",
    "            well_tst_perm['VSHHtst'] = well_tst_perm.vshhtst.cumsum()\n",
    "            well_tst_perm = well_tst_perm.sort_values(by='MD')\n",
    "            df_kh_lst.append(well_tst_perm)\n",
    "        df_khlst = pd.concat(df_kh_lst)\n",
    "        df_kh_lst_fm.append(df_khlst)\n",
    "    df_khlst_fm = pd.concat(df_kh_lst_fm)\n",
    "    # df_khlst_fm = df_khlst_fm.dropna()\n",
    "    return df_khlst_fm[['well', 'FORMATION_up', 'MD', 'TST', 'TST_smpl','KHtst','PHITHtst','VSHHtst']]\n",
    "# Comparison NET_clp and NET_clp2\n",
    "def well_display_net(dataset, well, formation, net1='NET_clp', net2_flag=0, net2='NET_clp_v2'):\n",
    "    well_sel = dataset[(dataset.well == well) & (dataset.FORMATION_up == formation)]\n",
    "    depth = well_sel['TST']\n",
    "    grn = well_sel['GR_N']\n",
    "    net = well_sel['NET']\n",
    "    net_clp = well_sel[net1]\n",
    "    if net2_flag == 0:\n",
    "        fig, ax = plt.subplots(1,3, figsize=(4.5,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange'), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange'), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    if net2_flag == 1:\n",
    "        net_clp2 = well_sel[net2]\n",
    "        fig, ax = plt.subplots(1,4, figsize=(6,8), sharey=True)\n",
    "        ax[0].yaxis.set_ticks(np.arange(min(depth), max(depth), 5))\n",
    "        ax[0].plot(grn, depth, color='green'), ax[0].invert_yaxis(), ax[0].set_xlim(0, 150), ax[0].grid(axis='y')\n",
    "        well_bal_tops = well_sel.groupby('FORMATION')['TST'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for i in well_bal_tops[well_bal_tops.FORMATION.str.contains('Balakhany VIII')].FORMATION:\n",
    "            ax[0].hlines(well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0], xmin=0, xmax=150, color='black', lw=2, alpha=0.66)\n",
    "            ax[0].text(10, well_bal_tops[well_bal_tops.FORMATION==i]['TST'].iloc[0]+3, i, fontsize = 7, color =\"black\")\n",
    "        ax[1].plot(net, depth, color='orange', lw=0.25), ax[1].set_xlim(0, 1), ax[1].grid(axis='y')\n",
    "        ax[1].fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "        ax[2].plot(net_clp, depth, color='orange', lw=0.25), ax[2].set_xlim(0, 1), ax[2].grid(axis='y')\n",
    "        ax[2].fill_betweenx(depth,net_clp, color='orange', alpha=0.33)\n",
    "        ax[3].plot(net_clp2, depth, color='orange', lw=0.25), ax[3].set_xlim(0, 1), ax[3].grid(axis='y')\n",
    "        ax[3].fill_betweenx(depth,net_clp2, color='orange', alpha=0.33)\n",
    "        fig.suptitle(well_sel.well.unique()[0], fontsize=14)\n",
    "        fig.tight_layout()\n",
    "    return fig.show()\n",
    "# Run RFR model with train/test split\n",
    "def rfr_train_test_split(train_dataset, gs_set, scorer, target='KHtst', rng=0.25, margin=0.005):\n",
    "    \"\"\"\n",
    "    'train_ds', \n",
    "    'metrics: r2_train, r2_test, mae_train, mae_test, test_in', \n",
    "    'grid_search', \n",
    "    'result_df', \n",
    "    'train_df', \n",
    "    'test_df'\n",
    "    --------\n",
    "    scorer = make_scorer(mse, greater_is_better=False) <- format scorer like this\n",
    "    \"\"\"\n",
    "    train_dataset_list = []\n",
    "    grids_setting_list = []\n",
    "    metrics_dict = []\n",
    "    # X_train/x_test data splitting\n",
    "    y = np.array(train_dataset[['well','FORMATION_up',target]])\n",
    "    x = np.array(train_dataset.drop(target, axis=1))\n",
    "    num = random.randint(0,100)\n",
    "    # num=42\n",
    "    train_dataset_list.append(train_dataset.drop(['FORMATION_up', target], axis=1).columns[1:].values.tolist())\n",
    "    x_train_init, x_test_init, y_train_init, y_test_init = train_test_split(x, y, test_size=0.3, random_state=num)\n",
    "    # Taking well names from train/test datasets\n",
    "    # x_train_wells = x_train_init[:,2]\n",
    "    # x_test_wells = x_test_init[:,2]\n",
    "    y_train_wells = y_train_init[:,0:2]\n",
    "    y_test_wells = y_test_init[:,0:2]\n",
    "    x_train = x_train_init[:,2:]\n",
    "    x_test = x_test_init[:,2:]\n",
    "    y_train = y_train_init[:,2]\n",
    "    y_test = y_test_init[:,2]\n",
    "    # GridSearch for ML-model\n",
    "    grid_rfr = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "    grid_calc_rfr = GridSearchCV(estimator = grid_rfr, param_grid = gs_set, scoring=scorer, cv = 5)\n",
    "    grid_calc_rfr.fit(x_train, y_train)\n",
    "    gd_sr_setting = grid_calc_rfr.best_params_\n",
    "    grids_setting_list.append(gd_sr_setting)\n",
    "    print('Grid_search: ', grid_rfr)\n",
    "    # Applying Pipeline for ML-model\n",
    "    rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**gd_sr_setting, n_jobs=-1, random_state=42))])\n",
    "    rfr.fit(x_train, y_train)\n",
    "    y_pred_train = rfr.predict(x_train)\n",
    "    y_pred_test = rfr.predict(x_test)\n",
    "    # Reporting\n",
    "    print('Pipeline: ', rfr.steps[1][1])\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    well_fm_train = pd.DataFrame(y_train_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_train = pd.DataFrame(zip(y_train, y_pred_train), columns=['actual','predict'])\n",
    "    df_rfr_train = well_fm_train.join(rfr_train)\n",
    "    df_rfr_train['l_limit'] = df_rfr_train.actual*dwn_range - margin\n",
    "    df_rfr_train['h_limit'] = df_rfr_train.actual*up_range + margin\n",
    "    df_rfr_train['qc'] = 'out'\n",
    "    df_rfr_train['dataset'] = 'train'\n",
    "    df_rfr_train.loc[(df_rfr_train.predict >= df_rfr_train.l_limit) & (df_rfr_train.predict <= df_rfr_train.h_limit), 'qc'] = 'in'\n",
    "    well_fm_test = pd.DataFrame(y_test_wells, columns=['well', 'FORMATION_up'])\n",
    "    rfr_test = pd.DataFrame(zip(y_test, y_pred_test), columns=['actual','predict'])\n",
    "    df_rfr_test = well_fm_test.join(rfr_test)\n",
    "    df_rfr_test['l_limit'] = df_rfr_test.actual*dwn_range - margin\n",
    "    df_rfr_test['h_limit'] = df_rfr_test.actual*up_range + margin\n",
    "    df_rfr_test['qc'] = 'out'\n",
    "    df_rfr_test['dataset'] = 'test'\n",
    "    df_rfr_test.loc[(df_rfr_test.predict >= df_rfr_test.l_limit) & (df_rfr_test.predict <= df_rfr_test.h_limit), 'qc'] = 'in'\n",
    "    df_rfr_result = pd.concat([df_rfr_train,df_rfr_test])\n",
    "    df_rfr_result['diff'] = (df_rfr_result.actual - df_rfr_result.predict).round(3)\n",
    "    metrics_dict = {    'r2_train':     r2(y_train, y_pred_train).round(2), \n",
    "                        'r2_test':      r2(y_test, y_pred_test).round(2),\n",
    "                        'mae_train':    mae(y_train, y_pred_train).round(2), \n",
    "                        'mae_test':     mae(y_test, y_pred_test).round(2),\n",
    "                        'train_in':     df_rfr_train['qc'].value_counts(normalize=True)['in'].round(2),\n",
    "                        'test_in':      df_rfr_test['qc'].value_counts(normalize=True)['in'].round(2)}\n",
    "    feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=train_dataset_list[0]).sort_values(ascending=True)\n",
    "    return {'train_ds':train_dataset_list[0], \n",
    "            'metrics':metrics_dict, \n",
    "            'grid_search' : grids_setting_list, \n",
    "            'result_df' : df_rfr_result,\n",
    "            'train_df' : df_rfr_train,\n",
    "            'test_df' : df_rfr_test,\n",
    "            'feature_imp' : feature_imp}\n",
    "# Run RFR model with loop\n",
    "def rfr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        # Statement of ML-model\n",
    "        rfr = Pipeline([(\"scaler\",StandardScaler()),(\"rfr\",RandomForestRegressor(**hyperdict, n_jobs=-1, random_state=42))])                                                                                  \n",
    "        # Fitting the ML-model\n",
    "        rfr.fit(x_train, y_train)\n",
    "        y_pred_train = rfr.predict(x_train)\n",
    "        y_pred_test = rfr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(rfr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "    # Building up of dataframe\n",
    "    print(rfr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin\n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff', 'l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 3, 'predict': 3, 'diff': 3})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Run XGBR model with loop \n",
    "def xgbr_loop(dataset, fmname, target, hyperdict, rng, margin):\n",
    "    \"\"\"\n",
    "    'train_ds', 'train_ftrs', 'result_df', 'grid_search', 'metrics'\n",
    "    \"\"\"\n",
    "    y_test_lst = []\n",
    "    y_pred_test_lst = []\n",
    "    well_exclude_lst = []\n",
    "    fm_exclude_lst = []\n",
    "    gs_settings_lst = []\n",
    "    metrics_r2_lst = []\n",
    "    metrics_mae_lst = []\n",
    "    ftr_imp_lst = []\n",
    "    for i in tqdm(range(len(dataset))[:]):\n",
    "        #Making up the feature and target datasets\n",
    "        df_wo_well = dataset.drop([i])\n",
    "        well_exclude = dataset.iloc[i]['well']\n",
    "        well_exclude_lst.append(well_exclude)\n",
    "        fm_exclude = dataset.iloc[i][fmname]\n",
    "        fm_exclude_lst.append(fm_exclude)\n",
    "        y_train = np.array(df_wo_well[target])\n",
    "        x_train = np.array(df_wo_well.drop(['well',fmname, target], axis=1))\n",
    "        well_train = np.array(df_wo_well['well'])\n",
    "        y_test = np.array(dataset.iloc[i][target])\n",
    "        y_test_lst.append(y_test)\n",
    "        x_test = np.array(dataset.drop(['well', fmname, target], axis=1).iloc[i])\n",
    "        xgbr = Pipeline([(\"scaler\",StandardScaler()),(\"xgbr\",XGBRegressor(**hyperdict, n_jobs=-1, random_state=42))])\n",
    "        # Fitting the ML-model\n",
    "        xgbr.fit(x_train, y_train)\n",
    "        y_pred_train = xgbr.predict(x_train)\n",
    "        y_pred_test = xgbr.predict([x_test])\n",
    "        y_pred_test_lst.append(y_pred_test[0])\n",
    "        # Metrics computation for the ML-model\n",
    "        r2_train = r2(y_train, y_pred_train).round(5)\n",
    "        mae_train = mae(y_train, y_pred_train)\n",
    "        metrics_r2_lst.append(r2_train)\n",
    "        metrics_mae_lst.append(mae_train.round(5))\n",
    "        feature_imp = pd.Series(xgbr.steps[1][1].feature_importances_, index=df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist()).sort_values(ascending=True)\n",
    "        ftr_imp_lst.append(feature_imp)\n",
    "\n",
    "    # Building up of dataframe\n",
    "    print(xgbr.steps[1][1])\n",
    "    res_rfr_sha = pd.DataFrame( zip(y_test_lst, y_pred_test_lst, well_exclude_lst, fm_exclude_lst, metrics_r2_lst, metrics_mae_lst, ftr_imp_lst), \n",
    "                            columns = ['actual','predict','well', 'FORMATION_up','metrics_r2', 'metrics_mae','features_imp'])\n",
    "    res_rfr_sha['l_range'] = res_rfr_sha.actual*(1-rng) - margin \n",
    "    res_rfr_sha['h_range'] = res_rfr_sha.actual*(1+rng) + margin \n",
    "    res_rfr_sha['qc'] = 'out'\n",
    "    res_rfr_sha.loc[(res_rfr_sha.predict >= res_rfr_sha.l_range) & (res_rfr_sha.predict <= res_rfr_sha.h_range), 'qc'] = 'in'\n",
    "    wells_tot = res_rfr_sha.shape[0]\n",
    "    wells_unpred = res_rfr_sha['qc'].value_counts()['out']\n",
    "    wells_unpred_vv = (res_rfr_sha['qc'].value_counts()['out']/res_rfr_sha.shape[0]).round(3)\n",
    "    try:\n",
    "        wells_pred = res_rfr_sha['qc'].value_counts()['in']\n",
    "        wells_pred_vv =  (res_rfr_sha['qc'].value_counts()['in']/res_rfr_sha.shape[0]).round(3)\n",
    "    except:\n",
    "        wells_pred = 0\n",
    "        wells_pred_vv = 0\n",
    "    res_rfr_sha['diff'] = res_rfr_sha.actual - res_rfr_sha.predict\n",
    "    res_rfr_sha = res_rfr_sha[['well','FORMATION_up','actual','predict', 'diff','l_range', 'h_range', 'qc', 'metrics_r2', 'metrics_mae', 'features_imp']]\n",
    "    types_dict = {'actual': 'float64', 'predict': 'float64', 'diff': 'float64', 'l_range': 'float64', 'h_range': 'float64'}\n",
    "    res_rfr_sha = res_rfr_sha.astype(types_dict)\n",
    "    res_rfr_sha = res_rfr_sha.round({'actual': 0, 'predict': 0, 'diff': 0})\n",
    "    metrics_dict = {    'wells_total':          wells_tot, \n",
    "                        'wells_unpred':         wells_unpred,\n",
    "                        'wells_unpred_v/v':     wells_unpred_vv,\n",
    "                        'wells_pred':           wells_pred,\n",
    "                        'wells_pred_v/v':       wells_pred_vv\n",
    "                    }\n",
    "    return {    'train_ds': dataset.columns.tolist(),\n",
    "                'train_ftrs': df_wo_well.drop(['well', fmname,target], axis=1).columns.tolist(),\n",
    "                'result_df': res_rfr_sha,\n",
    "                'grid_search' : hyperdict,\n",
    "                'metrics':metrics_dict,\n",
    "                'feature_imp' : feature_imp\n",
    "            }\n",
    "# Display results of ML-modeling\n",
    "def xplot_qc(dataset, dataframe, max_val, rng=0.25):\n",
    "    fig1_ml = px.scatter(dataset[dataframe], x='actual', y='predict', \n",
    "                        color='qc', \n",
    "                        hover_data=['well'], \n",
    "                        width=400, height=400,\n",
    "                        #  color_discrete_sequence=[\"red\", \"green\"]\n",
    "                        )\n",
    "    up_range = rng+1\n",
    "    dwn_range = 1- rng\n",
    "    fig1_ml.update_traces(marker=dict(size=10,opacity=0.75,line=dict(color='rgb(47, 57, 61)', width=1)))\n",
    "    fig2_ml=px.line(x=[0,max_val], y=[0,max_val])\n",
    "    fig2_1_ml=px.line(x=[0,max_val], y=[0,max_val*up_range])\n",
    "    fig2_2_ml=px.line(x=[0,max_val], y=[0,max_val*dwn_range])\n",
    "    fig2_ml.update_traces(line=dict(color = 'blue'))\n",
    "    fig2_1_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig2_2_ml.update_traces(line=dict(color = 'blue', dash='dash'))\n",
    "    fig3_ml = go.Figure(data = fig1_ml.data + fig2_ml.data + fig2_1_ml.data + fig2_2_ml.data)\n",
    "    fig3_ml.update_layout(  title = 'Comparison Actual vs Pred' + \n",
    "                                    ' QC_train: ' + str(dataset['metrics']['train_in']) +\n",
    "                                    ' QC_test: ' + str(dataset['metrics']['test_in']),\n",
    "                            width=600,height=400, xaxis_title='actual', yaxis_title='predict',\n",
    "                            margin=dict(l=10,r=10,b=10,t=40))\n",
    "    return fig3_ml.show()\n",
    "# Calculate weighted avg properties \n",
    "def avg_prop_calculation(dataset_ntd, dataset, formation):\n",
    "    well_data = []\n",
    "    well_formation = formation\n",
    "    for well in tqdm(dataset_ntd.well.unique()):\n",
    "        # print(well)\n",
    "        ntd_well_avgprop = dataset_ntd[(dataset_ntd.well ==well)]\n",
    "        well_avgprop_sel = dataset[(dataset.well==well)]\n",
    "        well_phit = []\n",
    "        well_phit10 = []\n",
    "        well_phit50 = []\n",
    "        well_phit90 = []\n",
    "        well_vsh = []\n",
    "        well_vsh10 = []\n",
    "        well_vsh50 = []\n",
    "        well_vsh90 = []\n",
    "        well_gperm = []\n",
    "        well_h = []\n",
    "        for layers in range(len(ntd_well_avgprop.well)):\n",
    "            ntd_top = ntd_well_avgprop.iloc[layers, 2].round(3)\n",
    "            ntd_bot = ntd_well_avgprop.iloc[layers, 3].round(3)\n",
    "            ntd_h = ntd_well_avgprop.iloc[layers, 4].round(3)\n",
    "            phit_lst = []\n",
    "            vsh_lst = []\n",
    "            perm_lst = []\n",
    "            for depth in range(len(well_avgprop_sel.TST)):\n",
    "                well_avgprop_tst = well_avgprop_sel['TST'].iloc[depth].round(3)\n",
    "                if well_avgprop_tst >= ntd_top and well_avgprop_tst <= ntd_bot:\n",
    "                    phit_lst.append(well_avgprop_sel['PHIT'].iloc[depth])\n",
    "                    vsh_lst.append(well_avgprop_sel['VSH'].iloc[depth])\n",
    "                    perm_lst.append(well_avgprop_sel['LPERM'].iloc[depth])\n",
    "            well_phit.append(mean(phit_lst)*ntd_h)\n",
    "            well_phit10.append(np.quantile(phit_lst, 0.1)*ntd_h)\n",
    "            well_phit50.append(np.quantile(phit_lst, 0.5)*ntd_h)\n",
    "            well_phit90.append(np.quantile(phit_lst, 0.9)*ntd_h)\n",
    "            well_vsh.append(mean(vsh_lst)*ntd_h)\n",
    "            well_vsh10.append(np.quantile(vsh_lst, 0.1)*ntd_h)\n",
    "            well_vsh50.append(np.quantile(vsh_lst, 0.5)*ntd_h)\n",
    "            well_vsh90.append(np.quantile(vsh_lst, 0.9)*ntd_h)\n",
    "            well_gperm.append(gmean(perm_lst)*ntd_h)\n",
    "            well_h.append(ntd_h)\n",
    "        well_phit_wavg = sum(well_phit)/sum(well_h)\n",
    "        well_phit10_wavg = sum(well_phit10)/sum(well_h)\n",
    "        well_phit50_wavg = sum(well_phit50)/sum(well_h)\n",
    "        well_phit90_wavg = sum(well_phit90)/sum(well_h)\n",
    "        well_vsh_wavg = sum(well_vsh)/sum(well_h)\n",
    "        well_vsh10_wavg = sum(well_vsh10)/sum(well_h)\n",
    "        well_vsh50_wavg = sum(well_vsh50)/sum(well_h)\n",
    "        well_vsh90_wavg = sum(well_vsh90)/sum(well_h)\n",
    "        well_perm_wavg = sum(well_gperm)/sum(well_h)\n",
    "        well_hmax = max(well_h)\n",
    "        well_h_p50 = np.quantile(well_h, 0.5)\n",
    "        well_layers_count =len(well_h)\n",
    "        well_hsum = sum(well_h)\n",
    "        well_data.append([  well, well_formation, \n",
    "                            well_hmax, well_h_p50, well_layers_count, well_hsum,\n",
    "                            well_phit_wavg, well_phit10_wavg, well_phit50_wavg, well_phit90_wavg,\n",
    "                            well_vsh_wavg, well_vsh10_wavg, well_vsh50_wavg, well_vsh90_wavg,\n",
    "                            well_perm_wavg])\n",
    "    result = pd.DataFrame(well_data, columns=[  'well','FORMATION_up',\n",
    "                                                'htst_max', 'htst_p50','htst_count', 'htst_sum',            \n",
    "                                                'phit_wavg', 'phit10_wavg','phit50_wavg','phit90_wavg',\n",
    "                                                'vsh_wavg', 'vsh10_wavg', 'vsh50_wavg', 'vsh90_wavg',\n",
    "                                                'perm_wavg'])\n",
    "    return result\n",
    "# Euclidian dist calculation with prop\n",
    "def dist_prop_calc(dataset, dist_formation, dist_cutoff, value):\n",
    "    \"\"\"\n",
    "    dataset have to contain 'X_mean', 'Y_mean', 'TVD_SCS' and 'KHtst', if you assing value as KHtst\n",
    "    \"\"\"\n",
    "    data = dataset[(dataset.FORMATION_up == dist_formation)]\n",
    "    row_name = data.well.reset_index().drop(['index'], axis=1)\n",
    "    distance_fm = pd.DataFrame(euclidean_distances(data[['X_mean', 'Y_mean', 'TVD_SCS']]), columns=list(data.well))\n",
    "    distance_fm_well = distance_fm.join(row_name).set_index('well')\n",
    "    distance_fm_well.reset_index(inplace=True)\n",
    "    def well_kh_accum(wells, dataset, kh_formation):\n",
    "        well_kh_accum = []\n",
    "        well_x_accum = []\n",
    "        well_y_accum = []\n",
    "        for i in wells:\n",
    "            well_kh_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)][value].reset_index())    \n",
    "            well_x_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['X_mean'].reset_index())\n",
    "            well_y_accum.append(dataset[(dataset.well==i)&(dataset.FORMATION_up == kh_formation)]['Y_mean'].reset_index())\n",
    "        well_kh3 = pd.concat(well_kh_accum).T[1:]\n",
    "        well_kh3.columns = [value + '_1',value + '_2', value + '_3']\n",
    "        well_x3 = pd.concat(well_x_accum).T[1:]\n",
    "        well_x3.columns = ['x1','x2','x3']\n",
    "        well_y3 = pd.concat(well_y_accum).T[1:]\n",
    "        well_y3.columns = ['y1','y2','y3']\n",
    "        final = pd.concat([ well_kh3.reset_index().drop('index',axis=1), \n",
    "                            well_x3.reset_index().drop('index',axis=1), \n",
    "                            well_y3.reset_index().drop('index',axis=1)], axis=1)\n",
    "        return final\n",
    "    df_collect = []\n",
    "    for num, well_name in enumerate(distance_fm_well.well[:]):\n",
    "        well_dist3 = distance_fm_well[distance_fm_well.well == well_name].T[1:].sort_values(by=num)\n",
    "        well_dist3_s2 = well_dist3[well_dist3[num] > dist_cutoff][:3].reset_index()\n",
    "        well_dist3_tuple = tuple(well_dist3_s2['index'])\n",
    "        well_dist3_res = well_dist3_s2.T[1:].reset_index().drop('index', axis=1)   \n",
    "        well_name3_res = well_dist3_s2.T[:1].reset_index().drop('index', axis=1)\n",
    "        well_kh3_res = well_kh_accum(well_dist3_tuple,dataset, dist_formation)\n",
    "        well_dist3_res.columns =['dist1', 'dist2', 'dist3']\n",
    "        well_name3_res.columns =['well1', 'well2', 'well3']\n",
    "        concat_df = pd.concat([well_dist3_res, well_kh3_res, well_name3_res], axis=1)\n",
    "        result = concat_df.join(pd.DataFrame([well_name], columns=['well']))\n",
    "        df_collect.append(result)     \n",
    "    df_well_kh_dist = pd.concat(df_collect).reset_index().drop('index', axis=1)\n",
    "    df_well_kh_dist['FORMATION_up'] = dist_formation\n",
    "    return df_well_kh_dist\n",
    "# Feature importance bar chart for 1-to-all algorithm\n",
    "def feature_imp_loop(dataset, wellname, fmname, xsize, ysize):\n",
    "    # dataset = test['result_df']\n",
    "    data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "    ftr_imp = data['features_imp'].values[0]\n",
    "    f, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ftr_imp.plot.barh()\n",
    "    ax.set_title('RFR feature imp  ' + wellname + ' ' + fmname)\n",
    "    ax.tick_params(axis='y', labelsize=8, rotation=0)\n",
    "    return f.show()\n",
    "# Save datafram to csv\n",
    "def save_tocsv(dataframe, filename, flag):\n",
    "    if flag == 1:\n",
    "        # Saving avg_prop dataframe to .csv\n",
    "        path = 'C:\\\\jupyter\\\\SPP\\\\inputoutput\\\\'\n",
    "        dataframe.to_csv(path + filename)\n",
    "    else:\n",
    "        pass\n",
    "# Feature importance bar chart for split dataframe\n",
    "def feature_imp_split(dataset, xsize, ysize):\n",
    "    fig, ax = plt.subplots(figsize=(xsize, ysize))\n",
    "    ax = dataset.plot.barh()\n",
    "    ax.set_title(\"RFR Feature Importances\")\n",
    "    ax.tick_params(axis='y', labelsize=9, rotation=0)\n",
    "    ax.figure.tight_layout()\n",
    "    return fig.show()\n",
    "# Logging results of ml\n",
    "def write_res_file(finename, comments, target, trainds, metrics, gridsearch):\n",
    "    with open(finename, 'a') as file:\n",
    "        # Get the current date and time\n",
    "        current_datetime = datetime.now()\n",
    "        # Write the result to the file\n",
    "        file.write(f'\\n{current_datetime} \\n {comments} target: {target}')\n",
    "        file.write(f'\\n training_ds_{trainds} \\n metrics_{[metrics]} \\n grid_search_{gridsearch}')\n",
    "    file.close()\n",
    "# Remover categorical values from datasets\n",
    "def cat_finder(dataset):\n",
    "    \"\"\"\n",
    "    cat_list: categorical columns to drop out\n",
    "    get_dum_list: categorical columns to run via pd.get_dummies\n",
    "    \"\"\"\n",
    "    cat_list = []\n",
    "    gm_list = []\n",
    "    for col in dataset.columns:\n",
    "        # print(i)\n",
    "        if dataset[col].dtype == 'string':\n",
    "            cat_list.append(col)\n",
    "            if col != 'well':\n",
    "                gm_list.append(col)\n",
    "    # return {'cat_list':cat_list,\n",
    "    #         'get_dum_list': gm_list}\n",
    "    return cat_list, gm_list\n",
    "# Display results of ML-modeling ver2\n",
    "def xplot_qc2(data, max_val, rng, margin, round):\n",
    "    data = data.round({'actual': round, 'predict': round, 'diff': round})\n",
    "    ds_train = data[data.dataset == 'train']\n",
    "    ds_test = data[data.dataset == 'test']\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors_tr = [colors[qc] for qc in ds_train.qc]\n",
    "    qc_colors_ts = [colors[qc] for qc in ds_test.qc]\n",
    "    scatter_train = go.Scatter( x=ds_train.actual, y=ds_train.predict,\n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_tr, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_train[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    scatter_test = go.Scatter(  x=ds_test.actual, y=ds_test.predict, \n",
    "                                mode='markers',\n",
    "                                marker=dict(color=qc_colors_ts, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                customdata = ds_test[['well','actual','predict','diff', 'FORMATION_up']],\n",
    "                                hovertemplate=\"\".join(\n",
    "                                [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]},d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                                )\n",
    "    line_trace_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_trace_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('train ds', 'test ds'))\n",
    "    fig.add_trace(scatter_train,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=1)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=1)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=1)\n",
    "    fig.add_trace(scatter_test,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_up,  row=1, col=2)\n",
    "    fig.add_trace(line_trace_dw,  row=1, col=2)\n",
    "    fig.update_xaxes(title_text='actual', row=1, col=2)\n",
    "    fig.update_yaxes(title_text='predict', row=1, col=2)\n",
    "    fig.update_layout(  title_text= ('rfr_train_test_split'), width=900, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling ver2 via loop    \n",
    "def xplot_qc2_loop(data, max_val, rng, margin=0.005):\n",
    "    data = data.round({'actual': 3, 'predict': 3, 'diff ': 3})\n",
    "    up_range = rng + 1\n",
    "    dwn_range = 1 - rng\n",
    "    colors = {'in': 'green', 'out': 'red'}\n",
    "    qc_colors = [colors[qc] for qc in data.qc]\n",
    "    scatter = go.Scatter( x=data.actual, y=data.predict,\n",
    "                            mode='markers',\n",
    "                            marker=dict(color=qc_colors, size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = data[['well','actual','predict', 'diff', 'FORMATION_up']],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},a:%{customdata[1]}, p:%{customdata[2]}, d:%{customdata[3]}, f:%{customdata[4]}<extra></extra>\"])\n",
    "                            )\n",
    "    fig = go.Figure()\n",
    "    line_up = go.Scatter(x=[0, max_val], y=[0 + margin, max_val*up_range + margin], mode='lines+markers', line=dict(color='blue'))\n",
    "    line_dw = go.Scatter(x=[0, max_val], y=[0 - margin, max_val*dwn_range - margin], mode='lines+markers', marker=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line_up)\n",
    "    fig.add_trace(line_dw)\n",
    "    fig.update_xaxes(title_text='actual')\n",
    "    fig.update_yaxes(title_text='predict')\n",
    "    fig.update_layout(  title_text= ('rfr_loop'), width=450, height=450, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Display results of ML-modeling on map\n",
    "def map_qc(metadata, data, fmname, scale):\n",
    "    data['diff'] = abs(data['diff'])\n",
    "    data = data[data.FORMATION_up == fmname]\n",
    "    data_in = data[data.qc=='in']\n",
    "    data_out = data[data.qc=='out']\n",
    "    field_avg_coord = metadata.groupby('field')[['X_wellhead','Y_wellhead']].mean().reset_index()\n",
    "    platform  = go.Scatter(         x=field_avg_coord.X_wellhead, y=field_avg_coord.Y_wellhead, customdata = field_avg_coord[['field']],\n",
    "                                    text=field_avg_coord['field'], textposition=\"middle right\",\n",
    "                                    marker=dict(color='rgb(0, 0,0)', size=12),\n",
    "                                    mode='markers+text', \n",
    "                                    marker_symbol='square', hovertemplate=\"\".join([\"%{customdata[0]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_in = go.Scatter(   x=data_in.X, y=data_in.Y,\n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='circle', color='green', size=data_in['actual']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)\n",
    "                                    ),\n",
    "                                    customdata = data_in[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    scatter_data_out = go.Scatter(  x=data_out.X, y=data_out.Y, \n",
    "                                    mode='markers',\n",
    "                                    marker=dict(symbol='diamond', color='red', size=data_out['diff']*scale,\n",
    "                                    opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                                    customdata = data_out[['well', 'diff']],\n",
    "                                    hovertemplate=\"\".join([\"well:%{customdata[0]}, diff:%{customdata[1]}<extra></extra>\"])\n",
    "                                    )\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(platform)\n",
    "    fig.add_trace(scatter_data_in)\n",
    "    fig.add_trace(scatter_data_out)\n",
    "    fig.update_layout(title_text= ('rfr_train_test_split'),autosize=True, width=1000, height=600, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Pairplot new version\n",
    "def pairplot_special(dataset, xsize, ysize, flag=1):\n",
    "    if flag == 1:\n",
    "        def corrfunc(x, y, **kws):\n",
    "            r, _ = stats.pearsonr(x, y)\n",
    "            ax = plt.gca()\n",
    "            ax.annotate(\"r = {:.2f}\".format(r),\n",
    "                        xy=(.1, .9), xycoords=ax.transAxes)\n",
    "        sns.set_context(rc={'axes.labelsize':10, 'lines.linewidth': 0.75})\n",
    "        g = sns.PairGrid(dataset)\n",
    "        g.fig.set_size_inches(xsize,ysize)\n",
    "        g.set(xticklabels=[], yticklabels=[]) \n",
    "        g.map_upper(plt.scatter, s=10, alpha=0.5)\n",
    "        g.map_diag(sns.distplot, kde=False)\n",
    "        g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
    "        g.map_lower(corrfunc)\n",
    "    else:\n",
    "        pass\n",
    "# Columns reorder for better display of variables\n",
    "def columns_reorder(dataset, selected_column):\n",
    "    new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "    dataset = dataset[new_order]\n",
    "    return dataset\n",
    "# Just simple x-plot for 1 dataframe\n",
    "def log_map_plot(dataframe, x_var, y_var, min_val, max_val):\n",
    "    fig = go.Figure()\n",
    "    scatter = go.Scatter(   x=dataframe[x_var], y=dataframe[y_var], \n",
    "                            mode='markers',\n",
    "                            marker=dict(color='orange', size=10, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                            customdata = dataframe[['well',x_var,y_var]],\n",
    "                            hovertemplate=\"\".join(\n",
    "                            [\"w:%{customdata[0]},x:%{customdata[1]}, y:%{customdata[2]}<extra></extra>\"])\n",
    "                            )\n",
    "    line = go.Scatter(x=[min_val, max_val], y=[min_val, max_val], mode='lines', line=dict(color='blue'))\n",
    "    fig.add_trace(scatter)\n",
    "    fig.add_trace(line)\n",
    "    fig.update_layout(  title_text= ('scatter plot'), width=600, height=600, \n",
    "                        margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    return fig.show()\n",
    "# Joining main and additional dataframes for predictions\n",
    "def join_add_df_prediction(base_dataframe, add_dataframe, target_var):\n",
    "    \"\"\"\n",
    "    Both dataframes have contain 'well' & 'FORMATION_up' for joining\n",
    "    \"\"\"\n",
    "    join_dataframe = base_dataframe.set_index(['well','FORMATION_up']).join(add_dataframe.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    col_names, gm_list = cat_finder(join_dataframe)\n",
    "    df_corr = join_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(join_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    join_dataframe_gm = pd.concat([join_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, join_dataframe_gm\n",
    "# Preparation dataframes for pairplot and for predictions\n",
    "def join_df_prediction(base_dataframe, target_var):\n",
    "    def columns_reorder(dataset, selected_column):\n",
    "        new_order = [col for col in dataset.columns if col != selected_column] + [selected_column]\n",
    "        dataset = dataset[new_order]\n",
    "        return dataset\n",
    "    def cat_finder(dataset):\n",
    "        \"\"\"\n",
    "        cat_list: categorical columns to drop out\n",
    "        get_dum_list: categorical columns to run via pd.get_dummies\n",
    "        \"\"\"\n",
    "        cat_list = []\n",
    "        gm_list = []\n",
    "        for col in dataset.columns:\n",
    "            # print(i)\n",
    "            if dataset[col].dtype == 'string':\n",
    "                cat_list.append(col)\n",
    "                if col != 'well':\n",
    "                    gm_list.append(col)\n",
    "        # return {'cat_list':cat_list,\n",
    "        #         'get_dum_list': gm_list}\n",
    "        return cat_list, gm_list\n",
    "    col_names, gm_list = cat_finder(base_dataframe)\n",
    "    df_corr = base_dataframe.drop(col_names, axis=1)\n",
    "    df_corr = columns_reorder(df_corr, target_var)\n",
    "    mem_cell = pd.get_dummies(base_dataframe[gm_list], columns=gm_list, drop_first=True)\n",
    "    mem_cell.rename(columns={'FORMATION_up_Balakhany X':'FORMATION_up_gm'},inplace=True)\n",
    "    dataframe = pd.concat([base_dataframe, mem_cell], axis=1)\n",
    "    return df_corr, dataframe\n",
    "# Function to calculate grid_search via train_split\n",
    "def run_rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin, logtxt_name, comment, xplot_flag, ftr_imp_flag):\n",
    "    model_res = rfr_train_test_split(dataset, gs_set, scorer, target, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, \n",
    "                    model_res['train_ds'], model_res['metrics'], model_res['grid_search'])\n",
    "    print('train_ds: ', model_res['train_ds'])\n",
    "    print('metrics: ', model_res['metrics'])\n",
    "    print('grid_search: ', model_res['grid_search'])\n",
    "    model_res_hyper_par = model_res['grid_search'][0]\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2(dataset['result_df'], 0.3, 0.05, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(dataset['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return model_res_hyper_par\n",
    "# Function to calculate target via 1-to-all\n",
    "def run_rfr_1_to_all(dataset, hyperdict, target, rng, margin, logtxt_name, comment, xplot_flag, max_val, ftr_imp_flag):\n",
    "    loop_res = rfr_loop(dataset, 'FORMATION_up', target, hyperdict, rng, margin)\n",
    "    write_res_file(logtxt_name, comment, target, loop_res['train_ds'], loop_res['metrics'], loop_res['grid_search'])\n",
    "    loop_res_pred = loop_res['result_df']\n",
    "    print('train_ftrs: ',loop_res['train_ftrs'])\n",
    "    print('metrics: ',loop_res['metrics'])\n",
    "    if xplot_flag == 1:\n",
    "        xplot_qc2_loop(loop_res['result_df'], max_val, rng, margin)\n",
    "    else:\n",
    "        pass\n",
    "    if ftr_imp_flag == 1:\n",
    "        feature_imp_split(loop_res['feature_imp'], 6, 4)\n",
    "    else:\n",
    "        pass\n",
    "    return loop_res_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well quality calculation & printing of diagrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_clean_v2():\n",
    "    #Counting of bad quality logs\n",
    "    bal8_list = [   'Balakhany VIII sand', 'Balakhany VIII 20',   'Balakhany VIII 10',   'Balakhany VIII 25',\n",
    "                    'Balakhany VIII 15',    'Balakhany VIII 5']\n",
    "    well_tot8 = []\n",
    "    well_zero8 = []\n",
    "    well_clean8 = []\n",
    "    for j in df_bal[(df_bal.FORMATION.isin(bal8_list) & (df_bal.PHIT>0))].well.unique():\n",
    "        phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII')]))\n",
    "        phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany VIII') & (df_bal.PHIT > 0)]))\n",
    "        well_tot8.append(j)\n",
    "        if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "            well_zero8.append(j)\n",
    "        else:\n",
    "            well_clean8.append(j)\n",
    "            # well_display_ntd(df_bal, j, 'Balakhany VIII', 'NET', round((phit_nonzero/phit_zero),2), 1) #printing well plots with high quality logs\n",
    "\n",
    "    bal10_list = ['Balakhany X', 'Balakhany X sand', 'Balakhany X 40', 'Balakhany X 20', 'Balakhany X 50']\n",
    "    well_tot10 = []\n",
    "    well_zero10 = []\n",
    "    well_clean10 = []\n",
    "    for j in df_bal[(df_bal.FORMATION.isin(bal10_list) & (df_bal.PHIT>0))].well.unique():\n",
    "        phit_zero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X')]))\n",
    "        phit_nonzero = (len(df_bal[(df_bal.well==j) & (df_bal.FORMATION_up == 'Balakhany X') & (df_bal.PHIT > 0)]))\n",
    "        well_tot10.append(j)\n",
    "        if round((phit_nonzero/phit_zero),2)<=0.90:\n",
    "            # well_display_ntd(df_bal, j, 'Balakhany X', 'NET', round((phit_nonzero/phit_zero),2), 1)\n",
    "            well_zero10.append(j)\n",
    "        else:\n",
    "            well_clean10.append(j)\n",
    "    print('well_tot8', len(well_tot8))\n",
    "    print('well_zero8', len(well_zero8))\n",
    "    print('well_clean8', len(well_clean8))\n",
    "    print('----------------------')\n",
    "    print('well_tot10', len(well_tot10))\n",
    "    print('well_zero10', len(well_zero10))\n",
    "    print('well_clean10', len(well_clean10))\n",
    "\n",
    "    # broken wells Bal8\n",
    "    # A08, A19, H01Z, J05 \n",
    "    # broken wells Bal10\n",
    "    # C31, D25\n",
    "    # high tst_interv Bal8\n",
    "    # E30Z\n",
    "    # small tst_interv Bal8\n",
    "    # G01Z, E05, E01, E01Y, E11Z, E07, H01Y, H01Z, A14\n",
    "    # Add wells after review Bal8 \n",
    "    # D04Z, \n",
    "    # Remove wells from clean_list by any reasons\n",
    "    remove_tst8 = ['A08','A19','J05','E30Z','G01Z', 'E05', 'E01', 'E01Y', 'E11Z', 'E07', 'H01Y', 'H01Z', 'A14']\n",
    "    well_clean8_v2 = [i for i in well_clean8 if i not in remove_tst8]\n",
    "    remove_tst10 = ['C31','D25', 'E21A']\n",
    "    well_clean10_v2 = [i for i in well_clean10 if i not in remove_tst10]\n",
    "    print('----------------------')\n",
    "    print('well_clean8_v2: ', len(well_clean8_v2))\n",
    "    print('well_clean10_v2: ', len(well_clean10_v2))\n",
    "    return well_clean8_v2, well_clean10_v2\n",
    "well_clean8_v2, well_clean10_v2 = well_clean_v2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetThicknessDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetThicknessDistribution():\n",
    "    # Limitation dataframe to cleaned wells for Bal8 & Bal10\n",
    "    df_net_bal8 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal8 = df_net_bal8[df_net_bal8.well.isin(well_clean8_v2) & (df_net_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net_bal10 = df_bal[['well', 'MD', 'TST', 'NET', 'FORMATION_up', 'LPERM', 'PHIT', 'VSH']]\n",
    "    df_net_bal10 = df_net_bal10[df_net_bal10.well.isin(well_clean10_v2) & (df_net_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation dataframe with h_tst from MD to TST for Bal8\n",
    "    df_list8 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal8')\n",
    "    for well in tqdm(df_net_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal8, well, 'Balakhany VIII', 'NET')\n",
    "        df_list8.append(df)\n",
    "    ntd_net_8 = pd.concat(df_list8)\n",
    "    df_list10 = []\n",
    "    print('Calculation dataframe with h_tst from MD to TST for Bal10')\n",
    "    for well in tqdm(df_net_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net_bal10, well, 'Balakhany X', 'NET')\n",
    "        df_list10.append(df)\n",
    "    ntd_net_10 = pd.concat(df_list10)\n",
    "    ntd_net_final = pd.concat([ntd_net_8, ntd_net_10])\n",
    "    # Cleaning NET variable and making up NET_clp with clipped data, join NET_clp to main dataframe\n",
    "    print('Cleaning NET variable and making up NET_clp with clipped data')\n",
    "    net_clp =  ntd_htst_cleaning(ntd_net_final, 1)\n",
    "    df_bal_net = df_bal.set_index(['well','MD']).join(net_clp.drop(\n",
    "        ['FORMATION_up','NET','TST', 'FORMATION', 'GR_N'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net = df_bal_net[df_bal_net.NET_clp.notna()]\n",
    "    # Cleaning NET_clp from 1-point zero\n",
    "    print('Cleaning NET_clp from 1-point zero')\n",
    "    for i in tqdm(range(len(df_bal_net.NET_clp))):\n",
    "        if (df_bal_net.NET_clp.iloc[i] == 0 and  \n",
    "            df_bal_net.NET_clp.iloc[i-1] == 1 and \n",
    "            df_bal_net.NET_clp.iloc[i+1] == 1):\n",
    "            df_bal_net.NET_clp.iloc[i] = 1\n",
    "    # NET-zero layers removing\n",
    "    df_zero_bal = df_bal_net[['well', 'MD', 'TST', 'NET_clp', 'FORMATION_up']]\n",
    "    df_zero_bal8 = df_zero_bal[df_zero_bal.well.isin(well_clean8_v2) & (df_zero_bal.FORMATION_up=='Balakhany VIII')]\n",
    "    df_zero_list8 = []\n",
    "    print('NET-zero layers removing Bal8')\n",
    "    for well in tqdm(df_zero_bal8.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal8, well, 'Balakhany VIII', 'NET_clp')\n",
    "        df_zero_list8.append(df)\n",
    "    ntd_zero_8 = pd.concat(df_zero_list8)\n",
    "    df_zero_bal10 = df_zero_bal[df_zero_bal.well.isin(well_clean10_v2) & (df_zero_bal.FORMATION_up=='Balakhany X')]\n",
    "    df_zero_list10 = []\n",
    "    print('NET-zero layers removing Bal10')\n",
    "    for well in tqdm(df_zero_bal10.well.unique()):\n",
    "        df = ntd_calculation_zero(df_zero_bal10, well, 'Balakhany X', 'NET_clp')\n",
    "        df_zero_list10.append(df)\n",
    "    ntd_zero_10 = pd.concat(df_zero_list10)\n",
    "    ntd_zero = pd.concat([ntd_zero_8, ntd_zero_10])\n",
    "    print('Run ntd_htst_zero_cleaning')\n",
    "    net_clp2 = ntd_htst_zero_cleaning(ntd_zero, df_bal_net, 1, 'NET_clp', 'NET_clp2')\n",
    "    #Joining NET_clp2 to main dataframe df_bal_net\n",
    "    df_bal_net2 = df_bal_net.set_index(['well','MD']).join(net_clp2.drop(\n",
    "        ['FORMATION_up','GR_N', 'NET','NET_clp', 'FORMATION','TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    df_bal_net2 = df_bal_net2[df_bal_net2.NET_clp2.notna()]\n",
    "    # Displaying of ramdom well for example\n",
    "    well_display_net(df_bal_net2, 'J28', 'Balakhany VIII', 'NET_clp', 1, 'NET_clp2')\n",
    "    return df_bal_net2\n",
    "df_bal_net2 = NetThicknessDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full set of FU for Bal VIII / X FORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_fullset_bal_fu_calc_prop(dataset):\n",
    "    \"\"\"\n",
    "    Calculation XY coordinates of Bal VIII/X \n",
    "    Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2\n",
    "    Calculation average properties for Bal8 and Bal10 based on NET_clp2\n",
    "    Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    \"\"\"\n",
    "    # Calculation XY coordinates of Bal VIII/X\n",
    "    df_bal_net2_kh_xy8 = dataset[   (dataset.well.isin(well_clean8_v2)) & \n",
    "                                    (dataset.FORMATION_up=='Balakhany VIII')][['well','FORMATION_up','X_mean','Y_mean','TVD_SCS','field']].groupby(\n",
    "                                    ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index() \n",
    "    df_bal_net2_kh_xy10 = dataset[  (dataset.well.isin(well_clean10_v2)) & \n",
    "                                    (dataset.FORMATION_up=='Balakhany X')][['well','FORMATION_up','X_mean','Y_mean','TVD_SCS','field']].groupby(\n",
    "                                    ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "    df_bal_net2_kh_xy = pd.concat([df_bal_net2_kh_xy8, df_bal_net2_kh_xy10]).sort_values(by='well')  \n",
    "\n",
    "    df_net2_bal8 = dataset[[ 'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8 = df_net2_bal8[    (df_net2_bal8.well.isin(well_clean8_v2)) & \n",
    "                                    (df_net2_bal8.FORMATION_up=='Balakhany VIII')]\n",
    "    df_net2_bal10 = dataset[['well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10 = df_net2_bal10[  (df_net2_bal10.well.isin(well_clean10_v2)) & \n",
    "                                    (df_net2_bal10.FORMATION_up=='Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2')\n",
    "    df_recalc_list8 = []\n",
    "    for well in tqdm(df_net2_bal8.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8.append(df)\n",
    "    ntd_net2_8 = pd.concat(df_recalc_list8)\n",
    "    ntd_net2_8.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10 = []\n",
    "    for well in tqdm(df_net2_bal10.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10.append(df)\n",
    "    ntd_net2_10 = pd.concat(df_recalc_list10)\n",
    "    ntd_net2_10.drop_duplicates(inplace=True)\n",
    "    ntd_net2_final = pd.concat([ntd_net2_8, ntd_net2_10])\n",
    "    # Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and available FU Bal VIII/X\n",
    "    df_kh8 = df_net2_bal8.groupby('well')[[ 'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                            'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[1]).reset_index()                              \n",
    "    df_kh10 = df_net2_bal10.groupby('well')[[   'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[1]).reset_index()\n",
    "    df_tst8 = df_net2_bal8.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[1]).reset_index()\n",
    "    df_tst8['FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_tst10 = df_net2_bal10.groupby('well')[['TST','TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[1]).reset_index()\n",
    "    df_tst10['FORMATION_up'] = 'Balakhany X'\n",
    "    df_kh_fin = pd.concat([df_kh8, df_kh10])\n",
    "    df_tst_fin = pd.concat([df_tst8, df_tst10])\n",
    "    df_tst_fin.rename(columns={'TST':'tst_interv', 'TVD_SCS':'tvd_interv'}, inplace=True)\n",
    "    df_tst_kh = df_tst_fin.set_index(['well','FORMATION_up']).join(df_kh_fin.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    # Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation average properties for Bal8 and Bal10 based on NET_clp2')\n",
    "    avg_prop_bal8 = avg_prop_calculation(ntd_net2_8, df_net2_bal8, 'Balakhany VIII')\n",
    "    avg_prop_bal10 = avg_prop_calculation(ntd_net2_10, df_net2_bal10, 'Balakhany X')   \n",
    "    avg_prop_final = pd.concat([avg_prop_bal8, avg_prop_bal10])\n",
    "    avg_prop_final.sort_values(by='well', inplace=True)  \n",
    "    # Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    avg_prop_tst_kh = avg_prop_final.set_index(['well','FORMATION_up']).join(df_tst_kh.set_index(['well','FORMATION_up'])).reset_index().sort_values(by='well')\n",
    "    avg_prop_tst_kh = avg_prop_tst_kh.round({   'htst_max': 0, 'htst_sum': 0, 'tst_interv': 0, 'tvd_interv': 0, 'TVD_SCS': 0, \n",
    "                                                'KHtst': 0, 'PHITHtst': 0, 'VSHHtst': 0, 'perm_wavg':1, 'phit_wavg':2})\n",
    "    return avg_prop_tst_kh\n",
    "def prep_dist_calc_brief(dataset, offset):\n",
    "    avg_prop_targKH = dataset[['well', 'FORMATION_up', 'KHtst']]\n",
    "    df_dist_kh_bal8 = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'KHtst')\n",
    "    df_dist_kh_bal10 = dist_prop_calc(dataset, 'Balakhany X', offset, 'KHtst')\n",
    "    df_dist_kh_bal_concat = pd.concat([df_dist_kh_bal8, df_dist_kh_bal10])\n",
    "    df_dist_kh_bal_fin = df_dist_kh_bal_concat.set_index(['well', 'FORMATION_up']).join(avg_prop_targKH.set_index(['well', 'FORMATION_up'])).reset_index()\n",
    "    df_dist_kh_bal_fin['dist1_op'] = 1/df_dist_kh_bal_fin.dist1\n",
    "    df_dist_kh_bal_fin['dist2_op'] = 1/df_dist_kh_bal_fin.dist2\n",
    "    df_dist_kh_bal_fin['dist3_op'] = 1/df_dist_kh_bal_fin.dist3\n",
    "    return df_dist_kh_bal_fin\n",
    "def prep_remove_balfu_calc_prop(dataset):\n",
    "    \"\"\"\n",
    "    Calculation XY coordinates of Bal VIII sand/X sand\n",
    "    Removing Bal VIII and Bal X from FORMATION\n",
    "    Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    Calculation average properties for Bal8 and Bal10 based on NET_clp2.\n",
    "    Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    \"\"\"\n",
    "    # Calculation XY coordinates of Bal VIII/X\n",
    "    df_bal_net2_kh_cut_xy8 = dataset[   (dataset.well.isin(well_clean8_v2)) & \n",
    "                                        (dataset.FORMATION_up=='Balakhany VIII') &\n",
    "                                        (dataset.FORMATION != 'Balakhany VIII')][['well','FORMATION_up','X_mean','Y_mean','field']].groupby(\n",
    "                                        ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index() \n",
    "    df_bal_net2_kh_cut_xy10 = dataset[  (dataset.well.isin(well_clean10_v2)) & \n",
    "                                        (dataset.FORMATION_up=='Balakhany X') &\n",
    "                                        (dataset.FORMATION != 'Balakhany X')][['well','FORMATION_up','X_mean','Y_mean','field']].groupby(\n",
    "                                        ['well','FORMATION_up']).apply(lambda x: x.iloc[0]).drop(['well','FORMATION_up'], axis=1).reset_index()\n",
    "    df_bal_net2_kh_cut_xy = pd.concat([df_bal_net2_kh_cut_xy8, df_bal_net2_kh_cut_xy10]).sort_values(by='well')  \n",
    "    # Removing Bal VIII and Bal X from FORMATION to calc avg prop\n",
    "    df_net2_bal8_cut = dataset[[ 'well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst', 'VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal8_cut = df_net2_bal8_cut[    df_net2_bal8_cut.well.isin(well_clean8_v2) & \n",
    "                                            (df_net2_bal8_cut.FORMATION_up=='Balakhany VIII') &\n",
    "                                            (df_net2_bal8_cut.FORMATION != 'Balakhany VIII')]\n",
    "    df_net2_bal10_cut = dataset[['well', 'MD', 'TST', 'TVD_SCS','NET_clp2', 'FORMATION_up', 'FORMATION', \n",
    "                                        'LPERM', 'PHIT', 'VSH', 'KHtst','PHITHtst','VSHHtst', 'X_mean','Y_mean','field']]\n",
    "    df_net2_bal10_cut = df_net2_bal10_cut[      df_net2_bal10_cut.well.isin(well_clean10_v2) & \n",
    "                                                (df_net2_bal10_cut.FORMATION_up=='Balakhany X') &\n",
    "                                                (df_net2_bal10_cut.FORMATION != 'Balakhany X')]\n",
    "    # Calculation NTD for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation NTD for Bal8 and Bal10 based on NET_clp2 and renoved FU Bal VIII/X')\n",
    "    df_recalc_list8_cut = []\n",
    "    for well in tqdm(df_net2_bal8_cut.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal8_cut, well, 'Balakhany VIII', 'NET_clp2')\n",
    "        df_recalc_list8_cut.append(df)\n",
    "    ntd_net2_8_cut = pd.concat(df_recalc_list8_cut)\n",
    "    ntd_net2_8_cut.drop_duplicates(inplace=True)\n",
    "    df_recalc_list10_cut = []\n",
    "    for well in tqdm(df_net2_bal10_cut.well.unique()):\n",
    "        df = ntd_calculation_brief(df_net2_bal10_cut, well, 'Balakhany X', 'NET_clp2')\n",
    "        df_recalc_list10_cut.append(df)\n",
    "    ntd_net2_10_cut = pd.concat(df_recalc_list10_cut)\n",
    "    ntd_net2_10_cut.drop_duplicates(inplace=True)\n",
    "    ntd_net2_cut_final = pd.concat([ntd_net2_8_cut, ntd_net2_10_cut])\n",
    "    # Calculation KHtst and tst_interv for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    df_kh8_cut = df_net2_bal8_cut.groupby('well')[[ 'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                    'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[0]).reset_index()                              \n",
    "    df_kh10_cut = df_net2_bal10_cut.groupby('well')[[   'TVD_SCS','KHtst','FORMATION_up',\n",
    "                                                        'PHITHtst','VSHHtst','X_mean','Y_mean','field']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    df_tst8_cut = df_net2_bal8_cut.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index()\n",
    "    df_tst8_cut['FORMATION_up'] = 'Balakhany VIII'\n",
    "    df_tst10_cut = df_net2_bal10_cut.groupby('well')[['TST', 'TVD_SCS']].apply(lambda x: x.iloc[-1] - x.iloc[0]).reset_index()\n",
    "    df_tst10_cut['FORMATION_up'] = 'Balakhany X'\n",
    "    df_kh_cut_fin = pd.concat([df_kh8_cut, df_kh10_cut])\n",
    "    df_tst_cut_fin = pd.concat([df_tst8_cut, df_tst10_cut])\n",
    "    df_tst_cut_fin.rename(columns={'TST':'tst_interv', 'TVD_SCS':'tvd_interv'}, inplace=True)\n",
    "    df_tst_kh_cut = df_tst_cut_fin.set_index(['well','FORMATION_up']).join(df_kh_cut_fin.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    # Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X\n",
    "    print('Calculation average properties for Bal8 and Bal10 based on NET_clp2 and removed FU Bal VIII/X')\n",
    "    avg_prop_bal8_cut = avg_prop_calculation(ntd_net2_8_cut, df_net2_bal8_cut, 'Balakhany VIII')\n",
    "    avg_prop_bal10_cut = avg_prop_calculation(ntd_net2_10_cut, df_net2_bal10_cut, 'Balakhany X')   \n",
    "    avg_prop_final_cut = pd.concat([avg_prop_bal8_cut, avg_prop_bal10_cut])\n",
    "    avg_prop_final_cut.sort_values(by='well', inplace=True)  \n",
    "    # Joing KHtst/tst_interv and avg prop into one dataframe\n",
    "    avg_prop_tst_kh_cut = avg_prop_final_cut.set_index(['well','FORMATION_up']).join(df_tst_kh_cut.set_index(['well','FORMATION_up'])).reset_index().sort_values(by='well')\n",
    "    avg_prop_tst_kh_cut = avg_prop_tst_kh_cut.round({   'htst_max': 0, 'htst_sum': 0, 'tst_interv': 0, 'tvd_interv': 0, 'TVD_SCS': 0, \n",
    "                                                        'KHtst': 0, 'PHITHtst': 3, 'VSHHtst': 3, 'perm_wavg':5, 'phit_wavg':5})\n",
    "    return avg_prop_tst_kh_cut                  \n",
    "def prep_dist_calc(dataset, offset, blacklist):\n",
    "    \"\"\"\n",
    "    Calculation properties based for offset wells: KHtst and phit_wavg\n",
    "    Dataset - dataset with log data and any other variables \n",
    "    Offset - I prefer 0 or 300m, but it could be various \n",
    "    Blacklist - indices of rows which need to be removed\n",
    "    \"\"\"\n",
    "    df_dist_kh_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'KHtst')\n",
    "    df_dist_kh_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'KHtst')\n",
    "    df_dist_kh_bal_cut_fin = pd.concat([df_dist_kh_bal8_cut, df_dist_kh_bal10_cut])\n",
    "    df_dist_ph_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'phit_wavg')\n",
    "    df_dist_ph_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'phit_wavg')\n",
    "    df_dist_ph_bal_cut_fin = pd.concat([df_dist_ph_bal8_cut, df_dist_ph_bal10_cut])\n",
    "    df_dist_htst_bal8_cut = dist_prop_calc(dataset, 'Balakhany VIII', offset, 'htst_sum')\n",
    "    df_dist_htst_bal10_cut = dist_prop_calc(dataset, 'Balakhany X', offset, 'htst_sum')\n",
    "    df_dist_htst_bal10_cut_fin = pd.concat([df_dist_htst_bal8_cut, df_dist_htst_bal10_cut])\n",
    "    htst_dist = df_dist_htst_bal10_cut_fin[['well','FORMATION_up', 'htst_sum_1', 'htst_sum_2', 'htst_sum_3']]\n",
    "    \n",
    "    phit_dist = df_dist_ph_bal_cut_fin[['well','FORMATION_up', 'phit_wavg_1', 'phit_wavg_2', 'phit_wavg_3']]\n",
    "    phit_dist_htst = phit_dist.set_index(['well','FORMATION_up']).join(htst_dist.set_index(['well','FORMATION_up'])).reset_index()\n",
    "    df_dist_ph_kh_cut = df_dist_kh_bal_cut_fin.set_index(['well','FORMATION_up']).join(phit_dist_htst.set_index(['well','FORMATION_up']))\n",
    "    avg_prop_tst_kh_dist_cut = dataset.set_index(['well','FORMATION_up']).join(df_dist_ph_kh_cut).reset_index()\n",
    "\n",
    "    avg_prop_tst_kh_dist_cut['dist1_op'] = 1/avg_prop_tst_kh_dist_cut.dist1\n",
    "    avg_prop_tst_kh_dist_cut['dist2_op'] = 1/avg_prop_tst_kh_dist_cut.dist2\n",
    "    avg_prop_tst_kh_dist_cut['dist3_op'] = 1/avg_prop_tst_kh_dist_cut.dist3\n",
    "\n",
    "    index_to_drop = blacklist\n",
    "    avg_prop_tst_kh_cut_test = avg_prop_tst_kh_dist_cut.drop(index_to_drop).reset_index().drop('index', axis=1)\n",
    "    return avg_prop_tst_kh_cut_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullset_bal8_10():\n",
    "    # KHtst calculation and join to the main dataframe df_bal_net2\n",
    "    df_kh = proph_calculation(df_bal_net2, 'NET_clp2')\n",
    "    df_bal_net2_kh_init = df_bal_net2.set_index(['well','MD']).join(df_kh.drop(['FORMATION_up', 'TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    # Cleaning KHtst dataset according to mentioned well lists\n",
    "    df_bal_net2_kh8 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean8_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany VIII')]\n",
    "    df_bal_net2_kh10 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean10_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany X')]\n",
    "    df_bal_net2_kh = pd.concat([df_bal_net2_kh8, df_bal_net2_kh10])\n",
    "    # Save df_bal_net2_kh to csv-file\n",
    "    save_tocsv(df_bal_net2_kh,'ACG_wells_JOINT_BEST_v10_output.csv', 0)\n",
    "    # Preparation full set of Bal8/10 and save it to csv\n",
    "    avg_prop_tst_kh = prep_fullset_bal_fu_calc_prop(df_bal_net2_kh)\n",
    "    save_tocsv(avg_prop_tst_kh,'avg_prop_tst_kh.csv', 0)\n",
    "    # Dist calculation for full set of Bal8/10\n",
    "    df_dist_kh_bal_fin = prep_dist_calc_brief(avg_prop_tst_kh, 0)\n",
    "    return avg_prop_tst_kh, df_dist_kh_bal_fin, df_bal_net2_kh\n",
    "avg_prop_tst_kh, df_dist_kh_bal_fin, df_bal_net2_kh = fullset_bal8_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 3 offsets wells\n",
    "def display_3offset_wells(well, formation, dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh):\n",
    "    \"\"\"\n",
    "    Pay attention dataset_dist=df_dist_kh_bal_fin, dataset_logs=df_bal_net2_kh\n",
    "    well:       just well name\n",
    "    formation:  just formation\n",
    "    \"\"\"\n",
    "    def well_offset_selection(dataset_dist, fmname, well_target):\n",
    "        try:\n",
    "            well_df = dataset_dist[(dataset_dist.well == well_target) & (dataset_dist.FORMATION_up == fmname)][['well', 'well1', 'well2', 'well3',\n",
    "                                                                                                                        'dist1', 'dist2', 'dist3',\n",
    "                                                                                                                'KHtst','KHtst_1', 'KHtst_2', 'KHtst_3']]\n",
    "            well1 = well_df['well1'].iloc[0]\n",
    "            well2 = well_df['well2'].iloc[0]\n",
    "            well3 = well_df['well3'].iloc[0]\n",
    "            dist1 = well_df['dist1'].astype('int').iloc[0]\n",
    "            dist2 = well_df['dist2'].astype('int').iloc[0]\n",
    "            dist3 = well_df['dist3'].astype('int').iloc[0]\n",
    "            kh = well_df['KHtst'].astype('int').iloc[0]\n",
    "            kh1 = well_df['KHtst_1'].astype('int').iloc[0]\n",
    "            kh2 = well_df['KHtst_2'].astype('int').iloc[0]\n",
    "            kh3 = well_df['KHtst_3'].astype('int').iloc[0]\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return {'target': well_target, 'w1':well1, 'w2':well2, 'w3':well3, \n",
    "                'dist': 0,'d1':dist1, 'd2':dist2,'d3':dist3,\n",
    "                'kh':kh,'kh1':kh1, 'kh2':kh2, 'kh3':kh3}\n",
    "    def display_tracks(dataset, wellname, fmname, ref_depth, depth_step, r, c, kh_value, dist):\n",
    "        try:\n",
    "            data = dataset[(dataset.well==wellname) & (dataset.FORMATION_up == fmname)]\n",
    "            depth = data[ref_depth]\n",
    "            grn = data['GR_N']\n",
    "            vsh = data['VSH']\n",
    "            rhob = data['RHOB'] \n",
    "            npss = data['NPSS']\n",
    "            rdeep = data['RDEEP']\n",
    "            phit = data['PHIT'] \n",
    "            net = data['NET_clp2']\n",
    "            perm = data['LPERM']\n",
    "            kh = data['KHtst']\n",
    "            well_bal_tops = df_bal[(df_bal.well == wellname)].groupby('FORMATION')[ref_depth].apply(lambda x: x.iloc[0]).reset_index()\n",
    "            ax[r,c].plot(grn, depth, color='lightgreen', lw=2, zorder=10)\n",
    "            ax[r,c].set_xlim(0, 150) \n",
    "            ax[r,c].grid(axis='y')\n",
    "            ax[r,c].invert_yaxis()\n",
    "            ax[r,c].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c].set_xticks([])\n",
    "            ax[r,c].tick_params(axis='y', labelsize=8)\n",
    "            ax[r,c].set_title(wellname + ' ' + fmname + ' kh:' + str(kh_value) + ' dist:' + str(dist), fontsize=12) \n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                # ax[r,c].text(10, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            ax[r,c+1].plot(rhob, depth, color='red')\n",
    "            ax[r,c+1].xaxis.set_ticks(np.arange(1.65, 2.65, 0.3))\n",
    "            ax[r,c+1].set_xlim(1.65, 2.65)\n",
    "            ax[r,c+1].grid(axis='y')\n",
    "            ax[r,c+1].grid(axis='x')\n",
    "            ax[r,c+1].invert_yaxis()\n",
    "            ax[r,c+1].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+1].set_xticks([])\n",
    "            ax[r,c+1].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+1].hlines( well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                xmin=0, xmax=150, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "                ax[r,c+1].text(1.67, well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0]+0.5*depth_step, i, fontsize = 7, color =\"black\")\n",
    "            twin1 = ax[r,c+1].twiny()\n",
    "            twin1.plot(npss, depth, color='blue')\n",
    "            twin1.set_xlim(0.6, 0)\n",
    "            twin1.set_xticks([])\n",
    "            ax[r,c+2].plot(phit, depth, color='green', linestyle='dashed')\n",
    "            ax[r,c+2].set_xlim(0.3, 0)\n",
    "            ax[r,c+2].grid(axis='x')\n",
    "            ax[r,c+2].grid(axis='y')\n",
    "            ax[r,c+2].invert_yaxis()\n",
    "            ax[r,c+2].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+2].set_xticks([])\n",
    "            ax[r,c+2].set_yticks([])\n",
    "            ax[r,c+2].vlines(0.13, ymin=min(depth), ymax=max(depth), color='black', linestyle='dashed')\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+2].hlines(    well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], \n",
    "                                    xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.33)\n",
    "            twin2 = ax[r,c+2].twiny()\n",
    "            twin2.plot(net, depth, color='orange', linewidth=0.5)\n",
    "            twin2.fill_betweenx(depth,net, color='orange', alpha=0.33)\n",
    "            twin2.set_xlim(0, 1)\n",
    "            twin2.set_xticks([])\n",
    "            ax[r,c+3].plot(perm, depth, color='purple', alpha=0.66)\n",
    "            ax[r,c+3].set_xscale('log')\n",
    "            ax[r,c+3].set_xlim(0.1, 1000)\n",
    "            ax[r,c+3].grid(axis='y')\n",
    "            ax[r,c+3].grid(axis='x')\n",
    "            ax[r,c+3].invert_yaxis()\n",
    "            ax[r,c+3].yaxis.set_ticks(np.arange(min(depth), max(depth), depth_step))\n",
    "            ax[r,c+3].set_xticks([])\n",
    "            ax[r,c+3].set_yticks([])\n",
    "            for i in well_bal_tops[well_bal_tops.FORMATION.str.contains(fmname)].FORMATION:\n",
    "                ax[r,c+3].hlines(well_bal_tops[well_bal_tops.FORMATION==i][ref_depth].iloc[0], xmin=0, xmax=1000, linewidth=2, color='black', lw=2, alpha=0.5)\n",
    "            twin4 = ax[r,c+3].twiny()\n",
    "            twin4.plot(kh, depth, color='black', alpha=1)\n",
    "            twin4.set_xticks([])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "        return fig.show()\n",
    "    def display_subplots():\n",
    "        try:\n",
    "            well_dist_dict = well_offset_selection(dataset_dist, fmname, well_target)\n",
    "            display_tracks(dataset_logs, well_dist_dict['target'], fmname,'TST', 10, 0,0,well_dist_dict['kh'], well_dist_dict['dist'])\n",
    "            display_tracks(dataset_logs, well_dist_dict['w1'], fmname,'TST', 10 ,0,4, well_dist_dict['kh1'], well_dist_dict['d1'])  \n",
    "            display_tracks(dataset_logs, well_dist_dict['w2'], fmname,'TST', 10,1,0, well_dist_dict['kh2'], well_dist_dict['d2'])      \n",
    "            display_tracks(dataset_logs, well_dist_dict['w3'], fmname,'TST', 10,1,4, well_dist_dict['kh3'], well_dist_dict['d3'])\n",
    "        except Exception as e:\n",
    "            print(f'It looks like the desired formation is absent. The error is \"{e}\"')\n",
    "    well_target = well\n",
    "    fmname = formation\n",
    "    fig, ax = plt.subplots(2,8, figsize=(9,8), constrained_layout=True)\n",
    "    return display_subplots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments wiht NTD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps & 3D view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing maps of well trajectories\n",
    "def well_traj_dataprep(dataset):\n",
    "    map_data = dataset.dropna()\n",
    "    map_data_top = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[0:-100:100]).reset_index()\n",
    "    map_data_bot = map_data.groupby(['well','FORMATION_up'])[['X_traj','Y_traj']].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "    map_data_middle = map_data.groupby(['well','FORMATION_up'])[['X_mean', 'Y_mean', 'KHtst', 'TVD_SCS', 'Status']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "    map_trajectory_display = pd.concat([map_data_top, map_data_bot]).sort_values(by=['well','FORMATION_up']).drop('level_2', axis=1)\n",
    "    return map_trajectory_display, map_data_middle\n",
    "map_trajectory_display, map_data_middle = well_traj_dataprep(df_bal_net2_kh)\n",
    "def display_well_traj(trajectory, map_data_middle, fmname, mult, path, comment, print_flag):\n",
    "    trajectory = trajectory[trajectory.FORMATION_up == fmname]\n",
    "    map_data_middle = map_data_middle[map_data_middle.FORMATION_up == fmname]\n",
    "    map_data_middle['KHtst'] = map_data_middle['KHtst'].round(0)\n",
    "    traj = go.Scatter(  x=trajectory.X_traj, y=trajectory.Y_traj, \n",
    "                        mode='markers',\n",
    "                        marker=dict(color='black', size=1),\n",
    "                        customdata = trajectory[['well']],\n",
    "                        hovertemplate=\"\".join([\"well:%{customdata[0]}<extra></extra>\"])\n",
    "                        )\n",
    "    wells = go.Scatter( x=map_data_middle.X_mean, y=map_data_middle.Y_mean, \n",
    "                        mode='markers',\n",
    "                        # marker=dict(symbol='diamond', color='red', size=7, opacity=0.75, line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                        marker=dict(color=map_data_middle.TVD_SCS, size=map_data_middle.KHtst*mult, colorscale='Viridis_r',  showscale=True,\n",
    "                                    line=dict(color='rgb(47, 57, 61)', width=0.5)),\n",
    "                        customdata = map_data_middle[['well', 'KHtst']],\n",
    "                        hovertemplate=\"\".join([\"well:%{customdata[0]},kh:%{customdata[1]}<extra></extra>\"]))\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(traj)\n",
    "    fig.add_trace(wells)\n",
    "    fig.update_layout(  title_text= ('map of traj and well points'+ ' ' + fmname),\n",
    "                        autosize=True, width=1300, height=900, margin=dict(l=10,r=10,b=10,t=50), showlegend=False)\n",
    "    if print_flag == 'print':\n",
    "        go_offline.plot(fig, filename=path + comment, validate=True, auto_open=False)\n",
    "    else:\n",
    "        pass\n",
    "    return fig.show()\n",
    "display_well_traj(map_trajectory_display, map_data_middle, 'Balakhany VIII', 0.00125, 'plots/', 'Balakhany8_KHtst', 'dont_print')\n",
    "display_well_traj(map_trajectory_display, map_data_middle, 'Balakhany X', 0.003, 'plots/', 'Balakhany10_KHtst', 'dont_print')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Bal VIII / X from FORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_fu_bal8_10():\n",
    "    # KHtst calculation and join to the main dataframe df_bal_net2\n",
    "    df_kh = proph_calculation(df_bal_net2, 'NET_clp2')\n",
    "    df_bal_net2_kh_init = df_bal_net2.set_index(['well','MD']).join(df_kh.drop(['FORMATION_up', 'TST'], axis=1).set_index(['well','MD'])).reset_index()\n",
    "    # Cleaning KHtst dataset according to mentioned well lists\n",
    "    df_bal_net2_kh8 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean8_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany VIII')]\n",
    "    df_bal_net2_kh10 = df_bal_net2_kh_init[(df_bal_net2_kh_init.well.isin(well_clean10_v2)) & (df_bal_net2_kh_init.FORMATION_up == 'Balakhany X')]\n",
    "    df_bal_net2_kh = pd.concat([df_bal_net2_kh8, df_bal_net2_kh10])\n",
    "    # Removing upper part of Bal8/10 and property calc\n",
    "    avg_prop_tst_kh_cut = prep_remove_balfu_calc_prop(df_bal_net2_kh)\n",
    "    # Calculation of well properties and applying blacklist with indecies of outliers\n",
    "    blacklist = [244, 240, 320, 324]\n",
    "    avg_prop_tst_kh_cut0 = prep_dist_calc(avg_prop_tst_kh_cut, 0, blacklist)\n",
    "    avg_prop_tst_kh_cut0 = avg_prop_tst_kh_cut0.convert_dtypes()\n",
    "    well_offsets_df = avg_prop_tst_kh_cut0.copy()\n",
    "    #Saving well propetries dataset to csv\n",
    "    save_tocsv(avg_prop_tst_kh_cut0, 'avg_prop_tst_kh_dist0_cut.csv', 0)\n",
    "    # Displaying features list of dataframe\n",
    "    avg_prop_tst_kh_cut0.columns\n",
    "    return avg_prop_tst_kh_cut0\n",
    "avg_prop_tst_kh_cut0 = removing_fu_bal8_10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prop_tst_kh_cut0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tst_smpl_final(dataset, cut):\n",
    "    def tst_smpl_processing(dataset):\n",
    "        test_tst_smpl = dataset.copy()\n",
    "        test_tst_smpl = test_tst_smpl[~test_tst_smpl.TST_smpl.isna()]\n",
    "\n",
    "        test_tst_smpl_v2 = test_tst_smpl.groupby(['well','FORMATION_up'])[['MD','TST_smpl']].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        test_tst_smpl_v2['TST_smpl_remove'] = 1\n",
    "        test_tst_smpl_v3 = test_tst_smpl.set_index(['well', 'FORMATION_up','MD']).join(\n",
    "                            test_tst_smpl_v2.set_index(['well', 'FORMATION_up','MD']), rsuffix='_r').reset_index()\n",
    "        test_tst_smpl_v3 = test_tst_smpl_v3[test_tst_smpl_v3.TST_smpl_remove != 1]\n",
    "        result = test_tst_smpl_v3.drop(['TST_smpl_r','TST_smpl_remove'], axis=1)\n",
    "        return result\n",
    "    def tst_smpl_cut(dataset_cut, cut):\n",
    "        cc=0\n",
    "        ind_lst = []\n",
    "        well_sel = dataset_cut.reset_index().drop('index',axis=1)\n",
    "        for ind, row in well_sel.iterrows():\n",
    "            cc += row['TST_smpl']\n",
    "            if cc > cut:\n",
    "                ind_lst.append(ind)\n",
    "                cc=0\n",
    "        df_well_fm_lst = []\n",
    "        well_sel['tst_rank'] = 0\n",
    "        well_sel.iloc[:ind_lst[0], well_sel.columns.get_loc('tst_rank')] = 0\n",
    "        df_well_fm_lst.append(well_sel.iloc[:ind_lst[0]])\n",
    "        for i in range(len(ind_lst[:-1])):\n",
    "            well_sel.iloc[ind_lst[i]:ind_lst[i+1], well_sel.columns.get_loc('tst_rank')] = i+1\n",
    "            df_well_fm_lst.append(well_sel.iloc[ind_lst[i]:ind_lst[i+1]])\n",
    "        df_well = pd.concat(df_well_fm_lst)\n",
    "        df_well_v2 = df_well.join(df_well.groupby(['well','tst_rank'])['TST_smpl'].cumsum(), rsuffix='_r')\n",
    "        df_well_v2 = df_well_v2.rename(columns={'TST_smpl_r':'TST_smpl_cumsum'})\n",
    "        result = df_well_v2[[   'well', 'MD', 'FORMATION_up','PHIT','KHtst','X_traj','Y_traj','TVD_SCS',\n",
    "                                'TST_smpl', 'TST_smpl_cumsum', 'tst_rank']]\n",
    "        return result\n",
    "    print('Processing dataset with tst_smpl_processing')\n",
    "    tst_smpl_proc = tst_smpl_processing(dataset)\n",
    "    df_lst = []\n",
    "    for fm in ['Balakhany VIII', 'Balakhany X']:\n",
    "        print('Processing wells with tst_smpl_cut for ' + fm)\n",
    "        for wellname in tqdm(tst_smpl_proc.well.unique()):\n",
    "            try:\n",
    "                well_tst_cut = tst_smpl_proc[(tst_smpl_proc.well==wellname) & (tst_smpl_proc.FORMATION_up == fm)]\n",
    "                res_well_tst_cut = tst_smpl_cut(well_tst_cut, cut)\n",
    "                df_lst.append(res_well_tst_cut)\n",
    "            except:\n",
    "                pass\n",
    "    df_final = pd.concat(df_lst)\n",
    "    df_final['MD'] = df_final['MD'].round(1)\n",
    "    return df_final\n",
    "\n",
    "def tst_resampling_v2(dataset_resample, wellname, fmname, resampled_points):\n",
    "    df1 = dataset_resample[ (dataset_resample.well == wellname) & \n",
    "                            (dataset_resample.FORMATION_up==fmname)].groupby(\n",
    "                            ['well','tst_rank'])['TST_smpl'].count().reset_index()\n",
    "    df1 = df1.rename(columns={'TST_smpl':'tst_smpl_qnt'})\n",
    "    df2 = dataset_resample[ (dataset_resample.well == wellname) & \n",
    "                            (dataset_resample.FORMATION_up==fmname)].set_index(['well','tst_rank']).join(\n",
    "                            df1.set_index(['well','tst_rank'])).reset_index()\n",
    "    df2['window'] = (df2.tst_smpl_qnt/resampled_points).round(0).astype('int')\n",
    "    df_lst = []\n",
    "    for i in df2.tst_rank.unique()[:]:\n",
    "        dataset = df2[df2.tst_rank==i]\n",
    "        window = dataset.window.iloc[0]\n",
    "        md_data = []\n",
    "        for ind in range(0, len(dataset['MD']), window):\n",
    "            lst = dataset['MD'].iloc[ind:ind + window]\n",
    "            central_value = [len(lst) // 2]\n",
    "            md_data.append(lst.iloc[central_value].round(1).values[0])\n",
    "        ph_data = [dataset['PHIT'][ind:ind+window].mean().round(3) for ind in range(0, len(dataset['PHIT']), window)]\n",
    "        ind_data = [ind for ind in range(0, len(dataset['MD']), window)]\n",
    "        tst_rank = [i for ind in range(0, len(dataset['MD']), window)]\n",
    "        df_lst.append(pd.DataFrame(zip( md_data[:resampled_points], \n",
    "                                        tst_rank[:resampled_points], \n",
    "                                        ph_data[:resampled_points]), columns=['MD','tst_rank_aux','PHIT_avg']))\n",
    "    df3 = pd.concat(df_lst)\n",
    "    df3['well'] = wellname\n",
    "    df2['MD'] = df2.MD.round(1)\n",
    "    result_resample = df2.set_index(['well','MD']).join(df3.set_index(['well','MD'])).reset_index()\n",
    "    return result_resample\n",
    "\n",
    "def tst_resampling_v2_per_fm(well_list, data_df, fmname, samples_per_block):\n",
    "    print('Run calculation tst_resampling_v2 for ' + fmname)\n",
    "    well_bal = well_list.well.unique()\n",
    "    df_lst = []\n",
    "    for well in tqdm(well_bal):\n",
    "        df_lst.append(tst_resampling_v2(data_df, well, fmname, samples_per_block))\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "\n",
    "def check_tst_resampling(dataset_resampl, resampled_points, show_print=1):\n",
    "    check_tst_smpl_qty = dataset_resampl.groupby(['well','FORMATION_up','tst_rank'])['PHIT_avg'].count().reset_index()\n",
    "    check_tst_smpl_qty = check_tst_smpl_qty.rename(columns={'PHIT_avg':'PHIT_avg_qty_smpls'})\n",
    "    check_tst_smpl_qty_noneq = check_tst_smpl_qty[check_tst_smpl_qty.PHIT_avg_qty_smpls != resampled_points]\n",
    "    check_tst_smpl_qty_eq = check_tst_smpl_qty[check_tst_smpl_qty.PHIT_avg_qty_smpls == resampled_points]\n",
    "    if show_print == 1:\n",
    "        print('qty_eq = ',len(check_tst_smpl_qty_eq.well.unique()))\n",
    "    else:\n",
    "        pass\n",
    "    qty_noneq = check_tst_smpl_qty_noneq.groupby(['well','FORMATION_up'])['PHIT_avg_qty_smpls'].count().reset_index()\n",
    "    qty_noneq = qty_noneq[qty_noneq.PHIT_avg_qty_smpls > 0]\n",
    "    if show_print == 1:\n",
    "        print('qty_noneq', len(qty_noneq))\n",
    "    else:\n",
    "        pass\n",
    "    qty_noneq = qty_noneq.sort_values(by='PHIT_avg_qty_smpls', ascending=False)\n",
    "    result = {  'qty_noneq': qty_noneq,\n",
    "                'total_ds': check_tst_smpl_qty}\n",
    "    return result\n",
    "\n",
    "def display_tst_resampling(dataset, wellname, fmname):\n",
    "    plt.figure(figsize=(3, 10))\n",
    "    dataset = dataset[(dataset.well == wellname) & (dataset.FORMATION_up==fmname)]\n",
    "    plt.plot(dataset['PHIT'], dataset['MD'], label='PHIT', color='green', linestyle='--', zorder=1)\n",
    "    plt.scatter(dataset['PHIT_avg'], dataset['MD'], label='PHIT_avg', color='red', s=14, zorder=2, alpha=0.75);\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title('MD ' + wellname + ' ' + fmname)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "def tst_resampling_sft(dataset_sft, wellname, fmname):\n",
    "        df_well_resmpl = dataset_sft[(dataset_sft.well == wellname) & (dataset_sft.FORMATION_up == fmname)]\n",
    "        md_data = []\n",
    "        tst_rank_sft = []\n",
    "        for i in df_well_resmpl.tst_rank.unique()[:]:\n",
    "                dataset = df_well_resmpl[df_well_resmpl.tst_rank==i]\n",
    "                central_value = [len(dataset) // 2]\n",
    "                md_data.append(dataset['MD'].iloc[central_value].round(1).values[0])\n",
    "                tst_rank_sft.append(dataset['tst_rank'].iloc[central_value].values[0])\n",
    "        r_avg_s = pd.DataFrame(zip(md_data, tst_rank_sft), columns=['MD','tst_rank_sft'])\n",
    "        df_final_well_final = df_well_resmpl.set_index('MD').join(r_avg_s.set_index('MD')).reset_index()\n",
    "        df_final_well_final['tst_rank_sft'] = df_final_well_final['tst_rank_sft'].fillna(method='ffill')\n",
    "        return df_final_well_final\n",
    "\n",
    "def tst_resampling_sft_per_fm(dataset_sft, fmname):\n",
    "        print('Run shift calculation for ' + fmname)\n",
    "        df_lst_sft = []\n",
    "        for well in tqdm(dataset_sft.well.unique()):\n",
    "                df = tst_resampling_sft(dataset_sft, well, fmname)\n",
    "                df_lst_sft.append(df)\n",
    "        result = pd.concat(df_lst_sft)\n",
    "        return result\n",
    "\n",
    "def display_tst_shifting(dataset, wellname, fmname, rnk):\n",
    "    plt.figure(figsize=(3, 7))\n",
    "    dataset1 = dataset[(dataset.well == wellname) & (dataset.FORMATION_up==fmname) & (dataset.tst_rank==rnk)]\n",
    "    dataset2 = dataset[(dataset.well == wellname) & (dataset.FORMATION_up==fmname) & (dataset.tst_rank==rnk + 1)]\n",
    "    dataset_sft = dataset[(dataset.well == wellname) & (dataset.FORMATION_up==fmname) & (dataset.tst_rank_sft == rnk)]\n",
    "    plt.plot(dataset1['PHIT'], dataset1['MD'], label='PHIT', color='green', linestyle='--', zorder=1)\n",
    "    plt.plot(dataset2['PHIT'], dataset2['MD'], label='PHIT', color='lightgreen', linestyle='--', zorder=1)\n",
    "    plt.plot(dataset_sft['PHIT']+0.05, dataset_sft['MD'], label='PHIT', color='blue', linestyle='--', zorder=1)\n",
    "    plt.scatter(dataset1['PHIT_avg'], dataset1['MD'], label='PHIT_avg', color='darkred', s=18, zorder=2, alpha=0.75)\n",
    "    plt.scatter(dataset2['PHIT_avg'], dataset2['MD'], label='PHIT_avg', color='red', s=18, zorder=2, alpha=0.75)\n",
    "    plt.scatter(dataset1[dataset1['PHIT_avg'].notna()]['PHIT'], \n",
    "                dataset1[dataset1['PHIT_avg'].notna()]['MD'], label='PHIT', color='blue', s=14, zorder=2, alpha=0.5)\n",
    "    plt.scatter(dataset2[dataset2['PHIT_avg'].notna()]['PHIT'], \n",
    "                dataset2[dataset2['PHIT_avg'].notna()]['MD'], label='PHIT', color='blue', s=14, zorder=2, alpha=0.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.title('MD ' + wellname + ' ' + fmname)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    return plt.show()\n",
    "\n",
    "def tst_resampling_numrow(dataset):\n",
    "    df_lst = []\n",
    "    print('Assigning row numbers into each tst_rank for', dataset.FORMATION_up.iloc[0])\n",
    "    desired_list = ['well', 'FORMATION_up', 'TST_smpl_cumsum', 'tst_rank', 'tst_rank_sft', 'PHIT', 'PHIT_avg', 'X_traj', 'Y_traj', 'TVD_SCS']\n",
    "    for wellname in tqdm(dataset.well.unique()):\n",
    "        sample = dataset[(dataset.well == wellname)]\n",
    "        for rankname in sample.tst_rank.unique():  \n",
    "            sample_1 = sample[(sample.tst_rank == rankname) & (sample.PHIT_avg.notna())]\n",
    "            sample_2 = sample_1[desired_list].reset_index()\n",
    "            sample_2['index'] = 1\n",
    "            sample_2['num_row'] = sample_2['index'].cumsum()\n",
    "            sample_3 = sample_2.drop('index', axis=1)\n",
    "            df_lst.append(sample_3)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "\n",
    "def offset_wells_list(dataset, wellname, fmname):\n",
    "    \"\"\"\n",
    "    overview offset wells based on input dataset with well dist\n",
    "    \"\"\"\n",
    "    offset_well_lst = dataset[['well','FORMATION_up','well1', 'well2', 'well3']]\n",
    "    offset_well_lst =offset_well_lst[offset_well_lst.FORMATION_up == fmname]\n",
    "    result = offset_well_lst[offset_well_lst.well==wellname]\n",
    "    return result\n",
    "\n",
    "def target_features_merging(dataset_dist, dataset_data, fmname, offset, dist):\n",
    "    grid_offsets1 = dataset_dist[['well','FORMATION_up', offset]]\n",
    "    grid_offsets1 =grid_offsets1[grid_offsets1.FORMATION_up == fmname]\n",
    "    grid_offsets1 = grid_offsets1[['well', offset]]\n",
    "\n",
    "    grid_offsets1_v2 = grid_offsets1.set_index('well').join(dataset_data.set_index('well')).reset_index()\n",
    "    grid_offsets1_v2 = grid_offsets1_v2.add_suffix('_trgt',axis=1)\n",
    "    offet_trgt = offset + '_trgt'\n",
    "    grid_offsets1_v2 = grid_offsets1_v2.rename(columns = { offet_trgt:'well', 'tst_rank_trgt':'tst_rank', 'num_row_trgt':'num_row'})\n",
    "\n",
    "    df_bal8_sft_num_ftr1 = dataset_data.add_suffix('_ftr', axis=1)\n",
    "    df_bal8_sft_num_ftr1 = df_bal8_sft_num_ftr1.rename(columns = {'well_ftr':'well', 'tst_rank_ftr':'tst_rank', 'num_row_ftr':'num_row'})\n",
    "\n",
    "    grid_offsets1_v3 =  grid_offsets1_v2.set_index(['well','tst_rank','num_row']).join(\n",
    "                        df_bal8_sft_num_ftr1.set_index(['well','tst_rank','num_row'])).reset_index()\n",
    "    grid_offsets1_v3 = grid_offsets1_v3.rename(columns = {'well':'well_ftr', 'tst_rank':'tst_rank_ftr', 'num_row':'num_row_ftr'})\n",
    "    grid_offsets1_v3['offset'] = dist\n",
    "    grid_offsets1_v3_fin = grid_offsets1_v3[[  'well_trgt','FORMATION_up_trgt','TST_smpl_cumsum_trgt', 'PHIT_avg_trgt','tst_rank_sft_trgt','TVD_SCS_trgt', 'X_traj_trgt','Y_traj_trgt', \n",
    "                                                'well_ftr', 'tst_rank_ftr', 'num_row_ftr','TST_smpl_cumsum_ftr', 'PHIT_avg_ftr','tst_rank_sft_ftr',\n",
    "                                                'TVD_SCS_ftr', 'X_traj_ftr','Y_traj_ftr','offset']]\n",
    "    result = grid_offsets1_v3_fin.sort_values(by=['well_trgt','tst_rank_ftr','num_row_ftr'])\n",
    "    return result\n",
    "\n",
    "def check_samples_quantity_per_well_fm(dataset, samples_per_block):\n",
    "    \"\"\"\n",
    "    Checking how many samples per block for well & formation.\n",
    "    Dataset have to contain tst_rank, well and formation names\n",
    "    \"\"\"\n",
    "    groupby_res = dataset.groupby(['well_trgt','FORMATION_up_trgt'])['tst_index_ftr'].count().reset_index()\n",
    "    problems_well_list = groupby_res[groupby_res.tst_index_ftr % samples_per_block != 0]\n",
    "    return problems_well_list\n",
    "\n",
    "def cartesian_to_cylindrical(center, point):\n",
    "    \"\"\"\n",
    "    tut center eto tocko s offseta , a point eto tocka targeta. Mi brali tocku offseta kak nacalo koordinat dla cilindriceskoy sistemi\n",
    "    \"\"\"\n",
    "    x0, y0, z0 = center\n",
    "    x, y, z = point\n",
    "    rho = math.sqrt((x - x0)**2 + (y - y0)**2)\n",
    "    theta = math.atan2(y - y0, x - x0)\n",
    "    z_cylindrical = z - z0\n",
    "    return rho, theta, z_cylindrical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data interpolation, blocking and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_by_depth_fm(dataset_logs, formation_name, step):\n",
    "    def interpolate_by_depth(one_well, formation_name, step):\n",
    "        one_well = one_well.sort_values(by='TST')\n",
    "        well_name = one_well[\"well\"].iloc[0]\n",
    "        data_range = np.floor((one_well[\"TST\"].max() - one_well[\"TST\"].min())/step)\n",
    "        starting_tst = one_well[\"TST\"].iloc[0]\n",
    "        new_TST_values = [starting_tst + i*0.1 for i in range(1,int(data_range))]\n",
    "        interp_X = interp1d(one_well['TST'], one_well['X_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_Y = interp1d(one_well['TST'], one_well['Y_traj'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_PHIT = interp1d(one_well['TST'], one_well['PHIT'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_TVD = interp1d(one_well['TST'], one_well['TVD_SCS'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_NET_clp2 = interp1d(one_well['TST'], one_well['NET_clp2'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_LPERM = interp1d(one_well['TST'], one_well['LPERM'], kind='linear', fill_value=\"extrapolate\")\n",
    "        interp_KHtst = interp1d(one_well['TST'], one_well['KHtst'], kind='linear', fill_value=\"extrapolate\")\n",
    "        # Create a new DataFrame with the interpolated values for new TVD_SCS\n",
    "        new_data = {\n",
    "            'well': [well_name for _ in range(len(new_TST_values))],\n",
    "            'FORMATION_up': [formation_name for _ in range(len(new_TST_values))],\n",
    "            'tst_index': [_ for _ in range(len(new_TST_values))],\n",
    "            'TST': new_TST_values,\n",
    "            'X_traj': interp_X(new_TST_values),\n",
    "            'Y_traj': interp_Y(new_TST_values),\n",
    "            'PHIT': interp_PHIT(new_TST_values),\n",
    "            'TVD_SCS': interp_TVD(new_TST_values),\n",
    "            'NET_clp2': interp_NET_clp2(new_TST_values),\n",
    "            'LPERM': interp_LPERM(new_TST_values),\n",
    "            'KHtst': interp_KHtst(new_TST_values),\n",
    "        }\n",
    "        new_df = pd.DataFrame(new_data)\n",
    "        return new_df\n",
    "    df_lst = []\n",
    "    print(f'Start interpolation of {formation_name}')\n",
    "    for wellnames in tqdm(dataset_logs.well.unique()):\n",
    "        well_sel = dataset_logs[dataset_logs.well == wellnames]\n",
    "        well_interp = interpolate_by_depth(well_sel, formation_name, step)\n",
    "        df_lst.append(well_interp)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "well_bal8 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany VIII')]\n",
    "well_bal10 = df_bal_net2_kh[(df_bal_net2_kh.FORMATION_up == 'Balakhany X')]\n",
    "well_bal8_interp = interpolate_by_depth_fm(well_bal8, 'Balakhany VIII', 0.1)\n",
    "well_bal10_interp = interpolate_by_depth_fm(well_bal10, 'Balakhany X', 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well_bal8_interp_rn = well_bal8_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "well_bal10_interp_rn = well_bal10_interp.rename(columns={'PHIT':'PHIT_orig'})\n",
    "\n",
    "def phit_rolling_averaging(input_dataset, samples_per_window):\n",
    "    df_lst = []\n",
    "    avg_report = []\n",
    "    fmname = input_dataset['FORMATION_up'].iloc[0] \n",
    "    print(f'Start rolling averaging of {fmname}')\n",
    "    for wellname in tqdm(input_dataset.well.unique()):\n",
    "        dataset = input_dataset[input_dataset.well == wellname]\n",
    "        window_size = int(len(dataset) / samples_per_window)\n",
    "        dataset['PHIT'] = dataset['PHIT_orig'].rolling(window=window_size, center=True).mean()\n",
    "        dataset =  dataset.dropna(subset=['PHIT'])\n",
    "        df_lst.append(dataset)\n",
    "        avg_report.append((wellname, len(dataset), window_size, samples_per_window))\n",
    "    result = pd.concat(df_lst)\n",
    "    avg_report_df = pd.DataFrame(avg_report, columns=['well','lenght_ds','window_size','samples_per_window'])\n",
    "    return result, avg_report_df\n",
    "samples_per_window = 100\n",
    "well_bal8_interp_phavg, avg_report_df8 = phit_rolling_averaging(well_bal8_interp_rn, samples_per_window)\n",
    "well_bal10_interp_phavg, avg_report_df10 = phit_rolling_averaging(well_bal10_interp_rn, samples_per_window)\n",
    "well_bal8_interp_phavg['PHIT_clp'] = well_bal8_interp_phavg['PHIT']\n",
    "well_bal10_interp_phavg['PHIT_clp'] = well_bal10_interp_phavg['PHIT']\n",
    "well_bal8_interp_phavg['LPERM_clp'] = well_bal8_interp_phavg['LPERM']\n",
    "well_bal10_interp_phavg['LPERM_clp'] = well_bal10_interp_phavg['LPERM']\n",
    "well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'PHIT_clp'] = 0.12\n",
    "well_bal8_interp_phavg.loc[well_bal8_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1\n",
    "well_bal10_interp_phavg.loc[well_bal10_interp_phavg.NET_clp2 == 0, 'LPERM_clp'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutting_block_lenght(dataset, block_lenght):\n",
    "    df_lst = []\n",
    "    fmname = dataset['FORMATION_up'].iloc[0]\n",
    "    print(f'Start processing of dataset for {fmname} with block lenght {block_lenght}')\n",
    "    for wellname in tqdm(dataset.well.unique()):\n",
    "        data = dataset[dataset.well == wellname]\n",
    "        tst_index_repaired = [i for i in range(0, len(data))]\n",
    "        data['tst_index'] = tst_index_repaired\n",
    "        new_index = [i for i in range(0, len(data), block_lenght)]\n",
    "        data_cut = data[(data.tst_index < new_index[-1])]\n",
    "        df_lst.append(data_cut)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "block_lenght = 100\n",
    "well_bal8_interp_phavg_cut = cutting_block_lenght(well_bal8_interp_phavg, block_lenght)\n",
    "well_bal10_interp_phavg_cut = cutting_block_lenght(well_bal10_interp_phavg, block_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_features_merging_v2(dataset_dist, dataset_data, fmname, offset, dist):\n",
    "    target_offset_well_name = dataset_dist[['well','FORMATION_up', offset]]\n",
    "    target_offset_well_name =target_offset_well_name[target_offset_well_name.FORMATION_up == fmname]\n",
    "    target_offset_well_name = target_offset_well_name[['well', offset]]\n",
    "\n",
    "    well_target = target_offset_well_name.set_index('well').join(dataset_data.set_index('well')).reset_index()\n",
    "    well_target_v2 = well_target.add_suffix('_trgt',axis=1)\n",
    "    offset_well = offset + '_trgt'\n",
    "    well_target_v2 = well_target_v2.rename(columns = { offset_well:'well','FORMATION_up_trgt':'FORMATION_up','tst_index_trgt':'tst_index'})\n",
    "\n",
    "    feature_well = dataset_data.add_suffix('_ftr', axis=1)\n",
    "    feature_well = feature_well.rename(columns = {'well_ftr':'well', 'FORMATION_up_ftr':'FORMATION_up','tst_index_ftr':'tst_index'})\n",
    "\n",
    "    final_df =  well_target_v2.set_index(['well','FORMATION_up','tst_index']).join(feature_well.set_index(['well','FORMATION_up','tst_index'])).reset_index()\n",
    "    # final_df = final_df.rename(columns = {'well':'well_ftr', 'FORMATION_up':'FORMATION_up_ftr', 'tst_index':'tst_index_ftr'})\n",
    "    final_df['offset'] = dist\n",
    "\n",
    "    final_df = final_df[[  'well_trgt','PHIT_trgt', 'PHIT_clp_trgt','TST_trgt', 'TVD_SCS_trgt', 'X_traj_trgt','Y_traj_trgt', \n",
    "                            'well', 'FORMATION_up','tst_index', 'PHIT_ftr', 'PHIT_clp_ftr', 'TST_ftr', 'TVD_SCS_ftr', 'X_traj_ftr','Y_traj_ftr','offset']]\n",
    "    result = final_df.sort_values(by=['well_trgt','well','FORMATION_up','tst_index'])\n",
    "    return result\n",
    "well_interp_bal8_phit_cut_w1 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal8_interp_phavg_cut, 'Balakhany VIII', 'well1', 1)\n",
    "well_interp_bal8_phit_cut_w2 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal8_interp_phavg_cut, 'Balakhany VIII', 'well2', 2)\n",
    "well_interp_bal8_phit_cut_w3 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal8_interp_phavg_cut, 'Balakhany VIII', 'well3', 3)\n",
    "\n",
    "well_interp_bal10_phit_cut_w1 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal10_interp_phavg_cut, 'Balakhany X', 'well1', 1)\n",
    "well_interp_bal10_phit_cut_w2 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal10_interp_phavg_cut, 'Balakhany X', 'well2', 2)\n",
    "well_interp_bal10_phit_cut_w3 = target_features_merging_v2(avg_prop_tst_kh_cut0, well_bal10_interp_phavg_cut, 'Balakhany X', 'well3', 3)\n",
    "\n",
    "final_phit_bal810 = pd.concat([  well_interp_bal8_phit_cut_w1, well_interp_bal8_phit_cut_w2, well_interp_bal8_phit_cut_w3,\n",
    "                                    well_interp_bal10_phit_cut_w1, well_interp_bal10_phit_cut_w2, well_interp_bal10_phit_cut_w3])\n",
    "final_phit_bal810['rho_ftr'] = np.sqrt((final_phit_bal810.X_traj_trgt-final_phit_bal810.X_traj_ftr)**2 + (final_phit_bal810.Y_traj_trgt-final_phit_bal810.Y_traj_ftr)**2)\n",
    "final_phit_bal810['theta_ftr'] = np.arctan2(final_phit_bal810.Y_traj_trgt-final_phit_bal810.Y_traj_ftr, final_phit_bal810.X_traj_trgt-final_phit_bal810.X_traj_ftr)\n",
    "final_phit_bal810['theta_deg_ftr'] = np.degrees(final_phit_bal810['theta_ftr'])\n",
    "final_phit_bal810['z_ftr'] = final_phit_bal810.TVD_SCS_trgt - final_phit_bal810.TVD_SCS_ftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_target_offsets_wells(dataset, block_shift, block_lenght):\n",
    "    df_lst = []\n",
    "    fmname = dataset['FORMATION_up'].iloc[0]\n",
    "    print(f'Calculation augmentation data for {fmname}') \n",
    "    for i in tqdm(dataset.offset.unique()):\n",
    "        well_pair_list = dataset[(dataset.offset == i)][['well_trgt','well']].groupby(\n",
    "                        'well_trgt')['well'].apply(lambda x: x.iloc[0]).reset_index()\n",
    "        for _, row in well_pair_list.iterrows():\n",
    "            well_pair_selected = dataset[(dataset.well_trgt == row['well_trgt']) & (dataset.well == row['well'])]\n",
    "            well_pair_selected['tst_index_sft'] = well_pair_selected['tst_index'] - block_shift\n",
    "            tst_index_sft_positiv = well_pair_selected[well_pair_selected.tst_index_sft >= 0]\n",
    "            new_index = [i for i in range(0, len(tst_index_sft_positiv), block_lenght)]\n",
    "            well_pair_tst_index_sft = well_pair_selected[(well_pair_selected['tst_index_sft'] >= 0) & (well_pair_selected['tst_index_sft'] < new_index[-1])]\n",
    "            df_lst.append(well_pair_tst_index_sft)\n",
    "    result = pd.concat(df_lst)\n",
    "    return result\n",
    "final_phit_bal8 = final_phit_bal810[final_phit_bal810.FORMATION_up == 'Balakhany VIII']\n",
    "final_phit_bal10 = final_phit_bal810[final_phit_bal810.FORMATION_up == 'Balakhany X']\n",
    "block_shift = block_lenght / 2\n",
    "aug_bal8 = augmentation_target_offsets_wells(final_phit_bal8, block_shift, block_lenght)\n",
    "aug_bal10 = augmentation_target_offsets_wells(final_phit_bal10, block_shift, block_lenght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def phit_rollavg_plot(dataset_raw, dataset_rollavg, dataset_interp, welllist, path):\n",
    "#     fig, ax = plt.subplots(1,4, figsize=(8, 10))\n",
    "#     def plot_data(dataset, wellname, column, color, lw, comment, suffix):\n",
    "#         dataset = dataset[(dataset.well == wellname)]\n",
    "#         depth = dataset['TST']\n",
    "#         phit = dataset['PHIT']\n",
    "#         ax[column].plot(phit, depth, label='PHIT' + suffix, color=color, lw=lw, zorder=2, alpha=1)\n",
    "#         ax[column].invert_yaxis()\n",
    "#         ax[column].set_xlim(.3, 0)\n",
    "#         ax[column].set_title(wellname + comment)\n",
    "#         ax[column].legend(fontsize=7)\n",
    "#         ax[column].grid()\n",
    "#     plot_data(dataset_raw,      welllist[3], 0, 'lightgreen', 4, '', '_raw')\n",
    "#     plot_data(dataset_interp,   welllist[3], 0, 'blue', 1, '', '_int')\n",
    "#     plot_data(dataset_rollavg,  welllist[3], 0, 'red', 1, '', '_avg')\n",
    "\n",
    "#     plot_data(dataset_raw,      welllist[2], 1, 'lightgreen', 4, '', '_raw')\n",
    "#     plot_data(dataset_interp,   welllist[2], 1, 'blue', 1, '', '_int')\n",
    "#     plot_data(dataset_rollavg,  welllist[2], 1, 'red', 1, '', '_avg')\n",
    "\n",
    "#     plot_data(dataset_raw,      welllist[1], 2, 'lightgreen', 4, '', '_raw')\n",
    "#     plot_data(dataset_interp,   welllist[1], 2, 'blue', 1, '', '_int')\n",
    "#     plot_data(dataset_rollavg,  welllist[1], 2, 'red', 1, '', '_avg')\n",
    "    \n",
    "#     plot_data(dataset_raw,      welllist[0], 3, 'lightgreen', 4, '', '_raw')\n",
    "#     plot_data(dataset_interp,   welllist[0], 3, 'blue', 1, '', '_int')\n",
    "#     plot_data(dataset_rollavg,  welllist[0], 3, 'red', 1, ' target', '_avg')\n",
    "#     fig.savefig( path + welllist[0] + '.pdf', format='pdf')\n",
    "\n",
    "# dataset_raw = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up == 'Balakhany VIII']\n",
    "# dataset_interp = well_bal8_interp\n",
    "# dataset_rollavg = well_bal8_interp_phavg\n",
    "# offset_wells8 = avg_prop_tst_kh_cut0[avg_prop_tst_kh_cut0.FORMATION_up == 'Balakhany VIII']\n",
    "# path8 = 'C:\\\\jupyter\\\\SPP\\\\test_polygon\\\\bal8\\\\'\n",
    "# for ind, row in offset_wells8.iterrows():\n",
    "#    welllist = [row['well'],row['well1'],row['well2'],row['well3']]\n",
    "#    phit_rollavg_plot(dataset_raw, dataset_rollavg, dataset_interp, welllist, path8)\n",
    "\n",
    "# dataset_raw = df_bal_net2_kh[df_bal_net2_kh.FORMATION_up == 'Balakhany X']\n",
    "# dataset_interp = well_bal10_interp\n",
    "# dataset_rollavg = well_bal10_interp_phavg\n",
    "# offset_wells10 = avg_prop_tst_kh_cut0[avg_prop_tst_kh_cut0.FORMATION_up == 'Balakhany X']\n",
    "# path10 = 'C:\\\\jupyter\\\\SPP\\\\test_polygon\\\\bal10\\\\'\n",
    "# for ind, row in offset_wells10.iterrows():\n",
    "# welllist = [row['well'],row['well1'],row['well2'],row['well3']]\n",
    "# phit_rollavg_plot(dataset_raw, dataset_rollavg, dataset_interp, welllist, path10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_lenght_m = 10\n",
    "# tst_rank_final = tst_smpl_final(df_bal_net2_kh, block_lenght_m)\n",
    "\n",
    "# samples_per_block = 10\n",
    "# tst_rank_bal8 = tst_rank_final[tst_rank_final.FORMATION_up == 'Balakhany VIII']\n",
    "# tst_rank_bal10 = tst_rank_final[tst_rank_final.FORMATION_up == 'Balakhany X']\n",
    "\n",
    "# df_bal8 = tst_resampling_v2_per_fm(tst_rank_bal8, tst_rank_final, 'Balakhany VIII', samples_per_block)\n",
    "# df_bal10 = tst_resampling_v2_per_fm(tst_rank_bal10, tst_rank_final, 'Balakhany X', samples_per_block)\n",
    "\n",
    "# df_bal8_sft = tst_resampling_sft_per_fm(df_bal8, 'Balakhany VIII')\n",
    "# df_bal10_sft = tst_resampling_sft_per_fm(df_bal10, 'Balakhany X')\n",
    "\n",
    "# df_bal8_sft_num = tst_resampling_numrow(df_bal8_sft)\n",
    "# df_bal10_sft_num = tst_resampling_numrow(df_bal10_sft)\n",
    "\n",
    "# grid_offsets8_v3_fin1 = target_features_merging(avg_prop_tst_kh_cut0, df_bal8_sft_num, 'Balakhany VIII', 'well1', 1)\n",
    "# grid_offsets8_v3_fin2 = target_features_merging(avg_prop_tst_kh_cut0, df_bal8_sft_num,'Balakhany VIII', 'well2', 2)\n",
    "# grid_offsets8_v3_fin3 = target_features_merging(avg_prop_tst_kh_cut0, df_bal8_sft_num, 'Balakhany VIII', 'well3', 3)\n",
    "# grid_offsets10_v3_fin1 = target_features_merging(avg_prop_tst_kh_cut0, df_bal10_sft_num, 'Balakhany X', 'well1', 1)\n",
    "# grid_offsets10_v3_fin2 = target_features_merging(avg_prop_tst_kh_cut0, df_bal10_sft_num,'Balakhany X', 'well2', 2)\n",
    "# grid_offsets10_v3_fin3 = target_features_merging(avg_prop_tst_kh_cut0, df_bal10_sft_num, 'Balakhany X', 'well3', 3)\n",
    "# grid_offsets = pd.concat([  grid_offsets8_v3_fin1, grid_offsets8_v3_fin2, grid_offsets8_v3_fin3,\n",
    "#                             grid_offsets10_v3_fin1, grid_offsets10_v3_fin2, grid_offsets10_v3_fin3])\n",
    "# grid_offsets['rho_ftr'] = np.sqrt((grid_offsets.X_traj_trgt-grid_offsets.X_traj_ftr)**2 + (grid_offsets.Y_traj_trgt-grid_offsets.Y_traj_ftr)**2)\n",
    "# grid_offsets['theta_ftr'] = np.arctan2(grid_offsets.Y_traj_trgt-grid_offsets.Y_traj_ftr, grid_offsets.X_traj_trgt-grid_offsets.X_traj_ftr)\n",
    "# grid_offsets['theta_deg_ftr'] = np.degrees(grid_offsets['theta_ftr'])\n",
    "# grid_offsets['z_ftr'] = grid_offsets.TVD_SCS_trgt - grid_offsets.TVD_SCS_ftr\n",
    "# grid_offsets_orig = grid_offsets.copy()\n",
    "# check_samples_quantity_per_well_fm(grid_offsets_orig, samples_per_block)\n",
    "\n",
    "# def looking_for_problems_rank(dataset, wellname, fmname, flag=1):\n",
    "#     \"\"\"\n",
    "#     If falg = 1 function return dataframe with samples per tst_rank_ftr\n",
    "#     If flag = 0 function return number of ranks for removing\n",
    "\n",
    "#     \"\"\"\n",
    "#     count_mode = dataset[       (dataset.well_trgt == wellname) & \n",
    "#                                 (dataset.FORMATION_up_trgt == fmname)]['tst_rank_sft_trgt'].value_counts().mode()\n",
    "#     count_result = dataset[ (dataset.well_trgt == wellname) & \n",
    "#                             (dataset.FORMATION_up_trgt == fmname)][['well_trgt','FORMATION_up_trgt','tst_rank_ftr']].value_counts().reset_index().sort_values(by='tst_rank_ftr')\n",
    "#     rank_to_remove = count_result[count_result['count'] != count_mode[0]]['tst_rank_ftr'].values\n",
    "#     index_remove = dataset[     ((dataset.well_trgt == wellname) & \n",
    "#                                 (dataset.FORMATION_up_trgt == fmname) & \n",
    "#                                 (dataset.tst_rank_ftr.isin(rank_to_remove)))].index\n",
    "#     if len(index_remove) == 0:\n",
    "#         print(f'Check whole welldata {wellname} - {fmname}')\n",
    "#     elif len(index_remove) > 150:\n",
    "#         print(f'Too many rows (> 100) for removing {wellname} - {fmname} - {len(index_remove)}')\n",
    "#     else:\n",
    "#         pass\n",
    "#     if flag == 1:\n",
    "#         result = count_result\n",
    "#     if flag == 0:\n",
    "#         result = rank_to_remove\n",
    "#     return result\n",
    "# # lst_rank_1 = looking_for_problems_rank(grid_offsets, 'CHIRAG6', 'Balakhany X', 0)\n",
    "# # lst_rank_2 = looking_for_problems_rank(grid_offsets, 'D01', 'Balakhany VIII', 0)\n",
    "# # lst_rank_3 = looking_for_problems_rank(grid_offsets, 'D01', 'Balakhany X', 0)\n",
    "# # # lst_rank_4 = looking_for_problems_rank(grid_offsets, 'E01Z', 'Balakhany VIII', 0)\n",
    "# # lst_rank_5 = looking_for_problems_rank(grid_offsets, 'E01Z', 'Balakhany X', 0)\n",
    "# # # lst_rank_6 = looking_for_problems_rank(grid_offsets, 'E05Z', 'Balakhany X', 0)\n",
    "# # # lst_rank_7 = looking_for_problems_rank(grid_offsets, 'GCA6', 'Balakhany X', 0)\n",
    "# lst_rank_1 = looking_for_problems_rank(grid_offsets, 'E05Z', 'Balakhany VIII', 0)\n",
    "# lst_rank_2 = looking_for_problems_rank(grid_offsets, 'E05Z', 'Balakhany X', 0)\n",
    "\n",
    "# grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'E05Z') & \n",
    "#                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany VIII'))]\n",
    "# grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'E05Z') & \n",
    "#                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X') & \n",
    "#                                         (grid_offsets_orig.tst_rank_ftr.isin(lst_rank_2)))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'CHIRAG6') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X') & \n",
    "# #                                         (grid_offsets_orig.tst_rank_ftr.isin(lst_rank_1)))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'D01') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany VIII') &\n",
    "# #                                         (grid_offsets_orig.tst_rank_ftr.isin(lst_rank_2)))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'D01') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X') &\n",
    "# #                                         (grid_offsets_orig.tst_rank_ftr.isin(lst_rank_3)))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'E01Z') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany VIII'))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'E01Z') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X') &\n",
    "# #                                         (grid_offsets_orig.tst_rank_ftr.isin(lst_rank_5)))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'E05Z') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X'))]\n",
    "# # grid_offsets_orig = grid_offsets_orig[  ~((grid_offsets_orig.well_trgt == 'GCA6') & \n",
    "# #                                         (grid_offsets_orig.FORMATION_up_trgt == 'Balakhany X'))]\n",
    "# check_samples_quantity_per_well_fm(grid_offsets_orig, samples_per_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_features_target_martix(dataset_input, wellname, fmname, var):\n",
    "#     try:\n",
    "#         fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(5, 3))\n",
    "#         dataset = dataset_input[(dataset_input.well_trgt == wellname) & (dataset_input.FORMATION_up == fmname)]\n",
    "#         phit_ftr = var + '_ftr'\n",
    "#         phit_trgt = var + '_trgt'\n",
    "        \n",
    "#         ftr_well1 = dataset[(dataset.offset == 1)]['well'].iloc[0]\n",
    "#         ftr_well2 = dataset[(dataset.offset == 2)]['well'].iloc[0]\n",
    "#         ftr_well3 = dataset[(dataset.offset == 3)]['well'].iloc[0]\n",
    "        \n",
    "#         feature_x1 = dataset[(dataset.offset == 1)][phit_ftr]\n",
    "#         ind1 = [i for i in range(len(feature_x1))]\n",
    "#         feature_x2 = dataset[(dataset.offset == 2)][phit_ftr]\n",
    "#         ind2 = [i for i in range(len(feature_x2))]\n",
    "#         feature_x3 = dataset[(dataset.offset == 3)][phit_ftr]\n",
    "#         ind3 = [i for i in range(len(feature_x3))]\n",
    "\n",
    "#         target_y = dataset[(dataset.offset == 1)][phit_trgt]\n",
    "#         target_ind = dataset[(dataset.offset == 1)][phit_trgt]\n",
    "#         ind = [i for i in range(len(target_ind))]\n",
    "\n",
    "#         ax1.plot(target_y, ind, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "#         ax2.plot(feature_x1, ind1, label='PHIT ftr3', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "#         ax3.plot(feature_x2, ind2, label='PHIT ftr2', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "#         ax4.plot(feature_x3, ind3, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "\n",
    "#         ax1.invert_yaxis(), ax2.invert_yaxis(), ax3.invert_yaxis(), ax4.invert_yaxis()\n",
    "#         ax1.set_xlim(0.1, 0.3), ax2.set_xlim(0.1, 0.3), ax3.set_xlim(0.1, 0.3), ax4.set_xlim(0.1, 0.3)       \n",
    "#         ax1.set_title(wellname, fontsize=8),  ax2.set_title(ftr_well1, fontsize=8), ax3.set_title(ftr_well2, fontsize=8), ax4.set_title(ftr_well3, fontsize=8)\n",
    "#         ax1.grid(), ax2.grid(), ax3.grid(), ax4.grid()\n",
    "#         ax1.tick_params(axis='both', labelsize=8), ax2.tick_params(axis='both', labelsize=8), ax3.tick_params(axis='both', labelsize=8), ax4.tick_params(axis='both', labelsize=8)\n",
    "#         fig.tight_layout()\n",
    "#         fig.show()\n",
    "#     except Exception as e:\n",
    "#         print(f'Its look like the desired formation is absent, error is \"{e}\"')\n",
    "# # display_features_target_martix(final_phit_bal8, 'A01W','Balakhany VIII', 'PHIT_clp')\n",
    "        \n",
    "# for well_matrix in final_phit_bal8.well_trgt.unique():\n",
    "#     display_features_target_martix(final_phit_bal8, well_matrix,'Balakhany VIII', 'PHIT_clp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_features_target(dataset_input, wellname, fmname, var):\n",
    "    try:\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(6, 7))\n",
    "        dataset = dataset_input[(dataset_input.well_trgt == wellname) & (dataset_input.FORMATION_up == fmname)]\n",
    "        phit_ftr = var + '_ftr'\n",
    "        phit_trgt = var + '_trgt'\n",
    "        ftr_well1 = dataset[(dataset.offset == 1)]['well'].iloc[0]\n",
    "        ftr_well2 = dataset[(dataset.offset == 2)]['well'].iloc[0]\n",
    "        ftr_well3 = dataset[(dataset.offset == 3)]['well'].iloc[0]\n",
    "        rho_well1 = dataset[(dataset.offset == 1)]['rho_ftr'].mean()\n",
    "        rho_well2 = dataset[(dataset.offset == 2)]['rho_ftr'].mean()\n",
    "        rho_well3 = dataset[(dataset.offset == 3)]['rho_ftr'].mean()\n",
    "\n",
    "        feature_x1 = dataset[(dataset.offset == 1)][phit_ftr]\n",
    "        median1 = feature_x1.median()\n",
    "        feature_x1 = dataset[(dataset.offset == 1)][phit_ftr].fillna(0)\n",
    "        feature_x2 = dataset[(dataset.offset == 2)][phit_ftr]\n",
    "        median2 = feature_x2.median()\n",
    "        feature_x2 = dataset[(dataset.offset == 2)][phit_ftr].fillna(0)\n",
    "        feature_x3 = dataset[(dataset.offset == 3)][phit_ftr]\n",
    "        median3 = feature_x3.median()\n",
    "        feature_x3 = dataset[(dataset.offset == 3)][phit_ftr].fillna(0)\n",
    "\n",
    "        target_y = dataset[(dataset.offset == 1)][phit_trgt]\n",
    "        target_ind = dataset[(dataset.offset == 1)][phit_trgt]\n",
    "        ind = [i for i in range(len(target_ind))]\n",
    "        cutoff = [0.13 for i in range(len(target_ind))]\n",
    "\n",
    "        ax1.plot(feature_x3, ind, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        ax1.plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        ax2.plot(feature_x2, ind, label='PHIT ftr2', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        ax2.plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        ax3.plot(feature_x1, ind, label='PHIT ftr3', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        ax3.plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        ax4.plot(target_y, ind, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "        ax4.plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "\n",
    "        ax1.invert_yaxis(), ax2.invert_yaxis(), ax3.invert_yaxis(), ax4.invert_yaxis()\n",
    "        ax1.set_xlim(.3, 0), ax2.set_xlim(.3, 0), ax3.set_xlim(.3, 0), ax4.set_xlim(.3, 0)\n",
    "        ax1.set_title('F_3 ' + str(int(rho_well3)) + ' ' + ftr_well3, fontsize=10)\n",
    "        ax2.set_title('F_2 ' + str(int(rho_well2)) + ' ' + ftr_well2, fontsize=10)\n",
    "        ax3.set_title('F_1 ' + str(int(rho_well1)) + ' ' + ftr_well1, fontsize=10)\n",
    "        ax4.set_title('Target ' + wellname, fontsize=10)\n",
    "        fig.suptitle('Target  ' + wellname + ' ' + fmname)\n",
    "        ax1.grid(), ax2.grid(), ax3.grid(), ax4.grid()\n",
    "        ax1.legend(fontsize=8), ax2.legend(fontsize=8), ax3.legend(fontsize=8), ax4.legend(fontsize=8)\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f'Its look like the desired formation is absent, error is \"{e}\"')\n",
    "def display_frts_target_polars(dataset, wellname, fmname):\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(6, 7))\n",
    "    ftr_well1 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['well_ftr'].iloc[0]\n",
    "    # ftr_well2 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 2)]['well_ftr'].iloc[0]\n",
    "    # ftr_well3 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 3)]['well_ftr'].iloc[0]\n",
    "\n",
    "    feature_x1 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['rho_ftr']\n",
    "    median1 = feature_x1.median()\n",
    "    feature_x1 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['rho_ftr'].fillna(median1)\n",
    "    feature_x2 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['theta_deg_ftr']\n",
    "    median2 = feature_x2.median()\n",
    "    feature_x2 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['theta_deg_ftr'].fillna(median2)\n",
    "    feature_x3 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['z_ftr']\n",
    "    median3 = feature_x3.median()\n",
    "    feature_x3 = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['z_ftr'].fillna(median3)\n",
    "\n",
    "    target_y = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['PHIT_avg_trgt']\n",
    "    target_ind = dataset[(dataset.well_trgt == wellname) & (dataset.offset == 1)]['PHIT_avg_trgt']\n",
    "    ind = [i for i in range(len(target_ind))]\n",
    "\n",
    "    ax1.scatter(feature_x1, ind, label='rho_ftr', color='brown', s=14, zorder=2, alpha=0.5)\n",
    "    ax2.scatter(feature_x2, ind, label='theta_ftr', color='purple', s=14, zorder=2, alpha=0.5)\n",
    "    ax3.scatter(feature_x3, ind, label='z_ftr3', color='orange', s=14, zorder=2, alpha=0.5)\n",
    "    ax4.scatter(target_y, ind, label='PHIT trgt', color='red', s=14, zorder=2, alpha=0.5)\n",
    "\n",
    "    ax1.invert_yaxis(), ax2.invert_yaxis(), ax3.invert_yaxis(), ax4.invert_yaxis()\n",
    "    # ax1.set_xlim(0, 0.3), ax2.set_xlim(0, 0.3), ax3.set_xlim(0, 0.3), \n",
    "    ax4.set_xlim(0, 0.3)\n",
    "    ax1.set_title('rho_ftr ' + ftr_well1, fontsize=10)\n",
    "    ax2.set_title('theta_ftr ' + ftr_well1, fontsize=10)\n",
    "    ax3.set_title('z_ftr ' + ftr_well1, fontsize=10)\n",
    "    ax4.set_title('Target ' + wellname, fontsize=10)\n",
    "    fig.suptitle('Target  ' + wellname + ' ' + fmname)\n",
    "    ax1.grid(), ax2.grid(), ax3.grid(), ax4.grid()\n",
    "    ax1.legend(fontsize=8), ax2.legend(fontsize=8), ax3.legend(fontsize=8), ax4.legend(fontsize=8)\n",
    "    fig.show();\n",
    "display_features_target(final_phit_bal8, 'B16Y','Balakhany VIII', 'PHIT_clp')\n",
    "# display_frts_target_polars(grid_offsets, wellname,'Balakhany X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D AENC run 3f1t+aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "final_phit_bal810_run = final_phit_bal810.copy()\n",
    "final_phit_bal810_run = final_phit_bal810_run[['well_trgt', 'PHIT_trgt', 'PHIT_clp_trgt', 'TST_trgt','TVD_SCS_trgt', \n",
    "                                               'well', 'FORMATION_up', 'tst_index', 'PHIT_ftr', 'PHIT_clp_ftr', 'TST_ftr', 'TVD_SCS_ftr', 'offset']]\n",
    "final_phit_bal810_run = final_phit_bal810_run.sort_values(by=['well_trgt','FORMATION_up','tst_index'])\n",
    "aug_bal8 = aug_bal8.sort_values(by=['well_trgt','FORMATION_up','tst_index_sft'])\n",
    "final_phit_bal8_run = final_phit_bal810_run[(final_phit_bal810_run.FORMATION_up == 'Balakhany VIII')]\n",
    "\n",
    "def train_test_wells_selection(dataset, test_well_qty, rerun):\n",
    "    if rerun == 0:\n",
    "        print('Well list is not randomized')\n",
    "    if rerun == 1:\n",
    "        well_list = list(dataset.well.unique())\n",
    "        test_well_list = []\n",
    "        for _ in range(test_well_qty):\n",
    "            well = well_list[np.random.randint(1, len(well_list))]\n",
    "            test_well_list.append(well)\n",
    "        train_well_list = [x for x in well_list if x not in test_well_list]\n",
    "    return train_well_list, test_well_list\n",
    "train_well_list, test_well_list = train_test_wells_selection(final_phit_bal8_run, 3, 1)\n",
    "print(test_well_list)\n",
    "\n",
    "def train_wells_preparation(dataset_main, dataset_aug, train_well_list, block_size, var):\n",
    "    train_dataset = dataset_main[dataset_main.well_trgt.isin(train_well_list)]\n",
    "    aug_dataset = dataset_aug[dataset_aug.well_trgt.isin(train_well_list)]\n",
    "    var_ftr = var + '_ftr'\n",
    "    var_trgt = var + '_trgt'\n",
    "    train_dataset['PHIT_ftr%'] = train_dataset[var_ftr]*100\n",
    "    train_dataset['PHIT_trgt%'] = train_dataset[var_trgt]*100\n",
    "    aug_dataset['PHIT_ftr%'] = aug_dataset[var_ftr]*100\n",
    "    aug_dataset['PHIT_trgt%'] = aug_dataset[var_trgt]*100\n",
    "\n",
    "    df_x1_ph = train_dataset[(train_dataset.offset==1)]['PHIT_ftr%'].fillna(0).values\n",
    "    aug_x1 = aug_dataset[(aug_dataset.offset==1)]['PHIT_ftr%'].fillna(0).values\n",
    "\n",
    "    df_x2_ph = train_dataset[(train_dataset.offset==2)]['PHIT_ftr%'].fillna(0).values\n",
    "    aug_x2 = aug_dataset[(aug_dataset.offset==2)]['PHIT_ftr%'].fillna(0).values\n",
    "\n",
    "    df_x3_ph = train_dataset[(train_dataset.offset==3)]['PHIT_ftr%'].fillna(0).values\n",
    "    aug_x3 = aug_dataset[(aug_dataset.offset==3)]['PHIT_ftr%'].fillna(0).values\n",
    "\n",
    "    df_ph = np.stack((df_x1_ph, df_x2_ph, df_x3_ph), axis=1)\n",
    "    aug = np.stack((aug_x1, aug_x2, aug_x3), axis=1)\n",
    "    df_ph_final = np.concatenate((df_ph, aug))\n",
    "    # df_ph_norm = scaler.fit_transform(df_ph_final)\n",
    "    X_train = df_ph_final.reshape(-1, block_size, 1, 3)\n",
    "    \n",
    "    y_well = train_dataset[     (train_dataset.offset==1)]['well_trgt'].reset_index().drop('index', axis=1).values\n",
    "    y_fm = train_dataset[       (train_dataset.offset==1)]['FORMATION_up'].reset_index().drop('index', axis=1).values\n",
    "    y_data = train_dataset[     (train_dataset.offset==1)]['PHIT_trgt%'].reset_index().drop('index', axis=1).values\n",
    "    y_well_aug = aug_dataset[     (aug_dataset.offset==1)]['well_trgt'].reset_index().drop('index', axis=1).values\n",
    "    y_fm_aug = aug_dataset[       (aug_dataset.offset==1)]['FORMATION_up'].reset_index().drop('index', axis=1).values\n",
    "    y_data_aug = aug_dataset[     (aug_dataset.offset==1)]['PHIT_trgt%'].reset_index().drop('index', axis=1).values    \n",
    "\n",
    "    # y_data = scaler.fit_transform(y_data)\n",
    "    # y_data_aug = scaler.fit_transform(y_data_aug)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_final_aug = np.stack((y_well_aug, y_fm_aug, y_data_aug), axis=1)\n",
    "    y_final_v2 = np.concatenate((y_final, y_final_aug))\n",
    "    y_reshape = y_final_v2.reshape(-1, block_size, 1, 3)\n",
    "    y_train = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    return X_train, y_train\n",
    "X_train, y_train = train_wells_preparation(final_phit_bal8_run, aug_bal8, train_well_list, block_lenght,'PHIT')\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "def test_wells_preparation(dataset_main, test_well_list, block_size, var):\n",
    "    test_dataset = dataset_main[dataset_main.well_trgt.isin(test_well_list)]\n",
    "    var_ftr = var + '_ftr'\n",
    "    var_trgt = var + '_trgt'\n",
    "    test_dataset['PHIT_ftr%'] = test_dataset[var_ftr]*100\n",
    "    test_dataset['PHIT_trgt%'] = test_dataset[var_trgt]*100\n",
    "\n",
    "    df_x1_ph = test_dataset[(test_dataset.offset==1)]['PHIT_ftr%'].fillna(0).values\n",
    "    df_x2_ph = test_dataset[(test_dataset.offset==2)]['PHIT_ftr%'].fillna(0).values\n",
    "    df_x3_ph = test_dataset[(test_dataset.offset==3)]['PHIT_ftr%'].fillna(0).values\n",
    "\n",
    "    df_ph = np.stack((df_x1_ph, df_x2_ph, df_x3_ph), axis=1)\n",
    "    # df_ph_norm = scaler.fit_transform(df_ph)\n",
    "    X_test = df_ph.reshape(-1, block_size, 1, 3)\n",
    "\n",
    "    y_well = test_dataset[     (test_dataset.offset==1)]['well_trgt'].reset_index().drop('index', axis=1).values\n",
    "    y_fm = test_dataset[       (test_dataset.offset==1)]['FORMATION_up'].reset_index().drop('index', axis=1).values\n",
    "    y_data = test_dataset[     (test_dataset.offset==1)]['PHIT_trgt%'].reset_index().drop('index', axis=1).values                  \n",
    "    # y_data = scaler.fit_transform(y_data)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_reshape = y_final.reshape(-1, block_size, 1, 3)\n",
    "    y_test = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    return X_test, y_test, y_final\n",
    "X_test, y_test, y_test_fullds = test_wells_preparation(final_phit_bal8_run, test_well_list, block_lenght,'PHIT')\n",
    "print(X_test.shape, y_test.shape, y_test_fullds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Running RandomForestRegressor as the baseline for CNN\n",
    "# X_rfr = X_train.reshape(-1, 3)\n",
    "# y_rfr = y_train.numpy().reshape(-1, 1)\n",
    "# X_train_rfr, X_test_rfr, y_train_rfr, y_test_rfr = train_test_split(X_rfr, y_rfr, test_size=0.3, random_state=42, shuffle=False)\n",
    "# rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_regressor.fit(X_train_rfr, y_train_rfr)\n",
    "# predictions = rf_regressor.predict(X_test_rfr)\n",
    "# mae = mean_absolute_error(y_test_rfr, predictions)\n",
    "# print(\"mean_absolute_error RandomForestRegressor:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN models cementery\n",
    "# model_cnn1 = Sequential([\n",
    "#     Conv1D(128, 3, activation='relu',  data_format=\"channels_last\", padding='same', input_shape =(10,1,3)),\n",
    "#     MaxPooling1D(2, padding='same'),\n",
    "#     Dropout(0.25),\n",
    "#     Conv1D(64, 3, activation='relu', padding='same'),\n",
    "#     MaxPooling1D(2, padding='same'),\n",
    "#     Dropout(0.25),\n",
    "#     Conv1D(32, 3, activation='relu', padding='same'),\n",
    "#     Flatten(),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(10)  \n",
    "# ])\n",
    "# model_cnn1 = Adam(learning_rate=0.001)\n",
    "\n",
    "# model_cnn = Sequential([\n",
    "#     Conv2D(128, (5,1), activation='relu',  data_format=\"channels_last\", padding='valid', input_shape =(block_size,1,3)),\n",
    "#     MaxPooling2D((3,1), padding='valid'),\n",
    "#     Dropout(0.125),\n",
    "#     Conv2D(64, (3,1), activation='relu', padding='valid',),\n",
    "#     MaxPooling2D((2,1), padding='valid'),\n",
    "#     Dropout(0.25),\n",
    "#     Conv2D(32, (2,1), activation='relu', padding='valid',),\n",
    "#     MaxPooling2D((3,1), padding='valid'),\n",
    "#     Flatten(),\n",
    "#     Dense(256, activation='relu'),\n",
    "#     Dense(block_size)  \n",
    "# ])\n",
    "# opt_cnn = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aenc = Sequential([\n",
    "    Conv2D(filters=256, kernel_size=(3,3), strides=1, activation='relu', padding='same', \n",
    "           kernel_initializer = RandomUniform(-1,1), input_shape =(block_lenght, 1, 3)),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=32, kernel_size=(1,1), strides=1, activation='relu', padding='same'),\n",
    "    Conv2DTranspose(filters=256, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=1, kernel_size=(3,3), strides=1, activation=\"relu\", padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(block_lenght)\n",
    "    ])\n",
    "opt_aenc = Adam(learning_rate=0.001)\n",
    "# opt_aenc = tfa.optimizers.Lookahead(opt_aenc)\n",
    "model_aenc.summary()\n",
    "\n",
    "def run_model(X_train, y_train, X_test, y_test, model, optmzr, lss, eph, btch_sz, erl_stp, vrbs):\n",
    "    model.compile(optimizer=optmzr, loss=lss)\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    if erl_stp == 1:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs, validation_data=(X_test, y_test),callbacks=[early_stop])\n",
    "    if erl_stp == 0:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs, validation_data=(X_test, y_test))  \n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    return y_pred\n",
    "y_pred = run_model(X_train, y_train, X_test, y_test, model_aenc, opt_aenc, 'mse', 200, 50, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_data_proc(X_test, y_test_init, y_test,  y_pred):\n",
    "    X_test1_data = np.array(X_test[:,:,:,0]).flatten()\n",
    "    X_test2_data = np.array(X_test[:,:,:,1]).flatten()\n",
    "    X_test3_data = np.array(X_test[:,:,:,2]).flatten()\n",
    "    y_test_data = np.array(y_test[:,:,0]).flatten()\n",
    "    y_test_wellname = y_test_init[:,0].flatten()\n",
    "    y_test_formation = y_test_init[:,1].flatten()\n",
    "    y_pred_data = y_pred.flatten()\n",
    "    y_test_full = np.stack((y_test_wellname, y_test_formation, y_test_data, y_pred_data, \n",
    "                            X_test1_data, X_test2_data, X_test3_data), axis=1)\n",
    "    result = pd.DataFrame(y_test_full, columns=['well_trgt','FORMATION_up_trgt','PHIT_avg_trgt%', 'PHIT_avg_trgt%_pred', \n",
    "                                                        'PHIT_avg_ftr%_1','PHIT_avg_ftr%_2','PHIT_avg_ftr%_3'])\n",
    "    mae = mean_absolute_error(result['PHIT_avg_trgt%'], result['PHIT_avg_trgt%_pred'])\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    return result\n",
    "def model_data_display(dataset, dataset_offsets, fmname):\n",
    "    well_test = dataset.well_trgt.unique()\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1, 12, figsize=(20, 8))\n",
    "        def track_display(dataset, well_test, well_num, track_num):\n",
    "            feature_x1 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_1']\n",
    "            feature_x2 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_2']\n",
    "            feature_x3 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname) ]['PHIT_avg_ftr%_3']\n",
    "            target_y3 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "            prediction_y3 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%_pred']\n",
    "            target_ind3 = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "            ind3 = [i for i in range(len(target_ind3))]\n",
    "            cutoff3 = [13 for i in range(len(target_ind3))]\n",
    "            ax[track_num].plot(feature_x3, ind3, label='PHIT ftr3', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num].plot(cutoff3, ind3, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num].set_title('well3 ' + offset_wells_list(dataset_offsets, well_test[0],'Balakhany VIII')['well3'].iloc[0], fontsize=9)\n",
    "            ax[track_num].set_yticks(range(min(ind3), max(ind3), 50))\n",
    "            ax[track_num].grid(axis='y')\n",
    "\n",
    "            ax[track_num+1].plot(feature_x2, ind3, label='PHIT ftr2', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+1].plot(cutoff3, ind3, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num+1].set_title('well2 ' + offset_wells_list(dataset_offsets, well_test[0],'Balakhany VIII')['well2'].iloc[0], fontsize=9)\n",
    "            ax[track_num+1].set_yticks(range(min(ind3), max(ind3), 50))\n",
    "            ax[track_num+1].grid(axis='y')\n",
    "\n",
    "            ax[track_num+2].plot(feature_x1, ind3, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+2].plot(cutoff3, ind3, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num+2].set_title('well1 ' + offset_wells_list(dataset_offsets, well_test[0],'Balakhany VIII')['well1'].iloc[0], fontsize=9)\n",
    "            ax[track_num+2].set_yticks(range(min(ind3), max(ind3), 50))\n",
    "            ax[track_num+2].grid(axis='y')\n",
    "\n",
    "            ax[track_num+3].plot(target_y3, ind3, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+3].plot(prediction_y3, ind3, label='PHIT pred', color='blue', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+3].plot(cutoff3, ind3, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num+3].set_yticks(range(min(ind3), max(ind3), 50))\n",
    "            ax[track_num+3].grid(axis='y')\n",
    "\n",
    "            ax[track_num].invert_yaxis(), ax[track_num+1].invert_yaxis(), ax[track_num+2].invert_yaxis(), ax[track_num+3].invert_yaxis()\n",
    "            ax[track_num].set_xlim(30, 0), ax[track_num+1].set_xlim(30, 0), ax[track_num+2].set_xlim(30, 0), ax[track_num+3].set_xlim(30, 0)\n",
    "            ax[track_num+3].set_title('Target ' + well_test[0], fontsize=11)\n",
    "        track_display(dataset, well_test, 0, 0)\n",
    "        track_display(dataset, well_test, 1, 4)\n",
    "        track_display(dataset, well_test, 2, 8)\n",
    "        fig.tight_layout()\n",
    "    except Exception as e:\n",
    "        print(f'Probably 1 or several wells are absent due to the error \"{e}\"')\n",
    "    return fig.show()\n",
    "\n",
    "model_full_df = model_data_proc(X_test, y_test_fullds, y_test, y_pred)\n",
    "model_data_display(model_full_df, avg_prop_tst_kh_cut0, 'Balakhany VIII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D AENC run 1t1t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_phit_bal810_run = final_phit_bal810.copy()\n",
    "final_phit_bal810_run = final_phit_bal810_run[['well_trgt', 'PHIT_trgt', 'TST_trgt','TVD_SCS_trgt', \n",
    "                                               'well', 'FORMATION_up', 'tst_index', 'PHIT_ftr', 'TST_ftr', 'TVD_SCS_ftr', 'offset']]\n",
    "final_phit_bal810_run = final_phit_bal810_run.sort_values(by=['well_trgt','FORMATION_up','tst_index'])\n",
    "aug_bal8 = aug_bal8.sort_values(by=['well_trgt','FORMATION_up','tst_index_sft'])\n",
    "final_phit_bal8_run = final_phit_bal810_run[(final_phit_bal810_run.FORMATION_up == 'Balakhany VIII')]\n",
    "\n",
    "def self_train_test_wells_selection(dataset, test_well_qty, rerun):\n",
    "    if rerun == 0:\n",
    "        print('Well list is not randomized')\n",
    "    if rerun == 1:\n",
    "        well_list = list(dataset.well.unique())\n",
    "        test_well_list = []\n",
    "        for _ in range(test_well_qty):\n",
    "            well = well_list[np.random.randint(1, len(well_list))]\n",
    "            test_well_list.append(well)\n",
    "        train_well_list = [x for x in well_list if x not in test_well_list]\n",
    "    return train_well_list, test_well_list\n",
    "train_well_list_slf, test_well_list_slf = self_train_test_wells_selection(final_phit_bal8_run, 3, 1)\n",
    "print(test_well_list_slf)\n",
    "\n",
    "def self_train_wells_preparation(dataset_main, train_well_list, block_size):\n",
    "    train_dataset = dataset_main[dataset_main.well_trgt.isin(train_well_list)]\n",
    "    train_dataset['PHIT_trgt%'] = train_dataset['PHIT_trgt']*100\n",
    "\n",
    "    phit_feature =train_dataset[(train_dataset.offset==1)]['PHIT_trgt%'].fillna(0).values\n",
    "    phit_feature_norm = scaler.fit_transform(phit_feature.reshape(-1, 1))\n",
    "    X_train = phit_feature_norm.reshape(-1, block_size, 1)\n",
    "    \n",
    "    y_well = train_dataset[     (train_dataset.offset==1)]['well_trgt'].values.reshape(-1, 1)\n",
    "    y_fm = train_dataset[       (train_dataset.offset==1)]['FORMATION_up'].values.reshape(-1, 1)\n",
    "    y_data = train_dataset[     (train_dataset.offset==1)]['PHIT_trgt%'].values.reshape(-1, 1)\n",
    "\n",
    "    y_data = scaler.fit_transform(y_data)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_reshape = y_final.reshape(-1, block_size, 1, 3)\n",
    "    y_train = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    return X_train, y_train, y_final\n",
    "X_train_slf, y_train_slf, y_final_slf = self_train_wells_preparation(final_phit_bal8_run, train_well_list_slf, block_lenght)\n",
    "print(X_train_slf.shape, y_train_slf.shape)\n",
    "\n",
    "def self_test_wells_preparation(dataset_main, test_well_list, block_size):\n",
    "    test_dataset = dataset_main[dataset_main.well_trgt.isin(test_well_list)]\n",
    "    test_dataset['PHIT_trgt%'] = test_dataset['PHIT_trgt']*100\n",
    "\n",
    "    phit_feature = test_dataset[(test_dataset.offset==1)]['PHIT_trgt%'].fillna(0).values\n",
    "    phit_feature_norm = scaler.fit_transform(phit_feature.reshape(-1, 1))\n",
    "    X_test = phit_feature_norm.reshape(-1, block_size, 1, 1)\n",
    "\n",
    "    y_well = test_dataset[     (test_dataset.offset==1)]['well_trgt'].reset_index().drop('index', axis=1).values\n",
    "    y_fm = test_dataset[       (test_dataset.offset==1)]['FORMATION_up'].reset_index().drop('index', axis=1).values\n",
    "    y_data = test_dataset[     (test_dataset.offset==1)]['PHIT_trgt%'].reset_index().drop('index', axis=1).values   \n",
    "\n",
    "    y_data = scaler.fit_transform(y_data)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_reshape = y_final.reshape(-1, block_size, 1, 3)\n",
    "    y_test = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    return X_test, y_test, y_final\n",
    "X_test_slf, y_test_slf, y_test_fullds_slf= self_test_wells_preparation(final_phit_bal8_run, test_well_list_slf, block_lenght)\n",
    "print(X_test_slf.shape, y_test_slf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aenc = Sequential([\n",
    "    Conv2D(filters=256, kernel_size=(3,3), strides=1, activation='relu', padding='same', \n",
    "           kernel_initializer = RandomUniform(-1,1), input_shape =(block_lenght, 1, 1)),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=32, kernel_size=(1,1), strides=1, activation='relu', padding='same'),\n",
    "    Conv2DTranspose(filters=256, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=1, kernel_size=(3,3), strides=1, activation=\"relu\", padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(block_lenght)\n",
    "    ])\n",
    "opt_aenc = Adam(learning_rate=0.001)\n",
    "model_aenc.summary()\n",
    "\n",
    "def run_model_self(X_train, y_train, model, optmzr, lss, eph, btch_sz, erl_stp, vrbs):\n",
    "    model.compile(optimizer=optmzr, loss=lss)\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    if erl_stp == 1:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs, callbacks=[early_stop])\n",
    "    if erl_stp == 0:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs)  \n",
    "    y_pred = model.predict(X_test_slf)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    # plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    return y_pred\n",
    "y_pred_slf = run_model_self(X_train_slf, y_train_slf, model_aenc, opt_aenc, 'mse', 100, 25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Archiv of old code\n",
    "        # feature_x4 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_1']\n",
    "        # feature_x5 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_2']\n",
    "        # feature_x6 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname) ]['PHIT_avg_ftr%_3']\n",
    "        # target_y2 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "        # prediction_y2 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%_pred']\n",
    "        # target_ind2 = dataset[(dataset.well_trgt == well_test[1]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "        # ind2 = [i for i in range(len(target_ind2))]\n",
    "        # cutoff2 = [0.4 for i in range(len(target_ind2))]\n",
    "        # ax[4].plot(feature_x6, ind2, label='PHIT ftr3', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[4].plot(cutoff2, ind2, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "        # ax[4].set_title('well3 ' + offset_wells_list(dataset_offsets, well_test[1],'Balakhany VIII')['well3'].iloc[0], fontsize=9)\n",
    "        # ax[4].set_yticks(range(min(ind2), max(ind2)+1, 50))\n",
    "        # ax[4].grid(axis='y')\n",
    "\n",
    "        # ax[5].plot(feature_x5, ind2, label='PHIT ftr2', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[5].plot(cutoff2, ind2, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "        # ax[5].set_title('well2 ' + offset_wells_list(dataset_offsets, well_test[1],'Balakhany VIII')['well2'].iloc[0], fontsize=9)\n",
    "        # ax[5].set_yticks(range(min(ind2), max(ind2)+1, 50))\n",
    "        # ax[5].grid(axis='y')\n",
    "\n",
    "        # ax[6].plot(feature_x4, ind2, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[6].plot(cutoff2, ind2, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "        # ax[6].set_title('well1 ' + offset_wells_list(dataset_offsets, well_test[1],'Balakhany VIII')['well1'].iloc[0], fontsize=9)\n",
    "        # ax[6].set_yticks(range(min(ind2), max(ind2)+1, 50))\n",
    "        # ax[6].grid(axis='y')\n",
    "\n",
    "        # ax[7].plot(target_y2, ind2, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[7].plot(prediction_y2, ind2, label='PHIT pred', color='blue', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[7].plot(cutoff2, ind2, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "        # ax[7].set_yticks(range(min(ind2), max(ind2)+1, 50))\n",
    "        # ax[7].grid(axis='y')\n",
    "\n",
    "        # ax[4].invert_yaxis(), ax[5].invert_yaxis(), ax[6].invert_yaxis(), ax[7].invert_yaxis()\n",
    "        # ax[4].set_xlim(1, 0), ax[5].set_xlim(1, 0), ax[6].set_xlim(1, 0), ax[7].set_xlim(1, 0)\n",
    "        # ax[7].set_title('Target ' + well_test[1], fontsize=11)\n",
    "\n",
    "        # feature_x7 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_1']\n",
    "        # feature_x8 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_ftr%_2']\n",
    "        # feature_x9 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname) ]['PHIT_avg_ftr%_3']\n",
    "        # target_y1 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "        # prediction_y1 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%_pred']\n",
    "        # target_ind1 = dataset[(dataset.well_trgt == well_test[2]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_avg_trgt%']\n",
    "        # ind1 = [i for i in range(len(target_ind1))]\n",
    "        # cutoff1 = [0.4 for i in range(len(target_ind1))]\n",
    "        # ax[8].plot(feature_x7, ind1, label='PHIT ftr3', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[8].plot(cutoff1, ind1, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        # ax[8].set_title('well3 ' + offset_wells_list(dataset_offsets, well_test[2],'Balakhany VIII')['well3'].iloc[0], fontsize=9)\n",
    "        # ax[8].set_yticks(range(min(ind1), max(ind1)+1, 50))\n",
    "        # ax[8].grid(axis='y')\n",
    "\n",
    "        # ax[9].plot(feature_x8, ind1, label='PHIT ftr2', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[9].plot(cutoff1, ind1, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        # ax[9].set_title('well2 ' + offset_wells_list(dataset_offsets, well_test[2],'Balakhany VIII')['well2'].iloc[0], fontsize=9)\n",
    "        # ax[9].set_yticks(range(min(ind1), max(ind1)+1, 50))\n",
    "        # ax[9].grid(axis='y')\n",
    "\n",
    "        # ax[10].plot(feature_x9, ind1, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[10].plot(cutoff1, ind1, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        # ax[10].set_title('well1 ' + offset_wells_list(dataset_offsets, well_test[2],'Balakhany VIII')['well1'].iloc[0], fontsize=9)\n",
    "        # ax[10].set_yticks(range(min(ind1), max(ind1)+1, 50))\n",
    "        # ax[10].grid(axis='y')\n",
    "\n",
    "        # ax[11].plot(target_y1, ind1, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[11].plot(prediction_y1, ind1, label='PHIT pred', color='blue', lw=2, zorder=2, alpha=0.5)\n",
    "        # ax[11].plot(cutoff1, ind1, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.66)\n",
    "        # ax[11].set_yticks(range(min(ind1), max(ind1)+1, 50))\n",
    "        # ax[11].grid(axis='y')\n",
    "\n",
    "        # ax[8].invert_yaxis(), ax[9].invert_yaxis(), ax[10].invert_yaxis(), ax[11].invert_yaxis()\n",
    "        # ax[8].set_xlim(1, 0), ax[9].set_xlim(1, 0), ax[10].set_xlim(1, 0), ax[11].set_xlim(1, 0)\n",
    "        # ax[11].set_title('Target ' + well_test[2], fontsize=11);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_data_proc(X_test, y_test_init, y_test,  y_pred):\n",
    "    X_test_data = np.array(X_test[:,:,:,0]).flatten()\n",
    "    y_test_data = np.array(y_test[:,:,0]).flatten()\n",
    "    y_test_wellname = y_test_init[:,0].flatten()\n",
    "    y_test_formation = y_test_init[:,1].flatten()\n",
    "    y_pred_data = y_pred.flatten()\n",
    "    y_test_full = np.stack((y_test_wellname, y_test_formation, y_test_data, y_pred_data, X_test_data), axis=1)\n",
    "    result = pd.DataFrame(y_test_full, columns=['well_trgt','FORMATION_up_trgt','PHIT_trgt%', 'PHIT_trgt%_pred', 'xtest_PHIT_trgt%'])\n",
    "    mae = mean_absolute_error(result['xtest_PHIT_trgt%'], result['PHIT_trgt%_pred'])\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    return result\n",
    "def self_data_display(dataset, fmname):\n",
    "    well_test = dataset.well_trgt.unique()\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(7, 7))\n",
    "        def track_display(dataset, fmname, well_num):\n",
    "            target_y = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['xtest_PHIT_trgt%']\n",
    "            prediction_y = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_trgt%_pred']\n",
    "            target_ind = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['xtest_PHIT_trgt%']\n",
    "            ind = [i for i in range(len(target_ind))]\n",
    "            ax[well_num].plot(target_y, ind, label='PHIT trgt', color='red', lw=4, zorder=2, alpha=0.5)\n",
    "            ax[well_num].plot(prediction_y, ind, label='PHIT pred', color='blue', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[well_num].set_yticks(range(min(ind), max(ind), 50))\n",
    "            ax[well_num].grid(axis='y')\n",
    "            ax[well_num].invert_yaxis()\n",
    "            ax[well_num].set_xlim(1, 0)\n",
    "            ax[well_num].set_title('Target ' + well_test[well_num], fontsize=11)\n",
    "            ax[well_num].tick_params(axis='both', labelsize=8)\n",
    "        track_display(dataset, fmname, 0)\n",
    "        track_display(dataset, fmname, 1)\n",
    "        track_display(dataset, fmname, 2)\n",
    "        plt.tight_layout()\n",
    "    except Exception as e:\n",
    "        print(f'Probably 1 or several wells are absent due to the error \"{e}\"')\n",
    "    return fig.show()\n",
    "\n",
    "self_full_df = self_data_proc(X_test_slf, y_test_fullds_slf, y_test_slf, y_pred_slf)\n",
    "self_data_display(self_full_df, 'Balakhany VIII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D AENC 1f1t+collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_phit_bal810_run = final_phit_bal810.copy()\n",
    "final_phit_bal810_run = final_phit_bal810_run[['well_trgt', 'PHIT_trgt', 'PHIT_clp_trgt', 'TST_trgt','TVD_SCS_trgt', \n",
    "                                               'well', 'FORMATION_up', 'tst_index', 'PHIT_ftr', 'PHIT_clp_ftr', 'TST_ftr', 'TVD_SCS_ftr', 'offset']]\n",
    "final_phit_bal810_run = final_phit_bal810_run.sort_values(by=['well_trgt','FORMATION_up','tst_index'])\n",
    "aug_bal8 = aug_bal8.sort_values(by=['well_trgt','FORMATION_up','tst_index_sft'])\n",
    "final_phit_bal8_run = final_phit_bal810_run[(final_phit_bal810_run.FORMATION_up == 'Balakhany VIII')]\n",
    "\n",
    "def seq_train_test_wells_selection(dataset, test_well_qty, rerun):\n",
    "    if rerun == 0:\n",
    "        print('Well list is not randomized')\n",
    "    if rerun == 1:\n",
    "        well_list = list(dataset.well.unique())\n",
    "        test_well_list = []\n",
    "        for _ in range(test_well_qty):\n",
    "            well = well_list[np.random.randint(1, len(well_list))]\n",
    "            test_well_list.append(well)\n",
    "        train_well_list = [x for x in well_list if x not in test_well_list]\n",
    "    return train_well_list, test_well_list\n",
    "train_well_list, test_well_list = seq_train_test_wells_selection(final_phit_bal8_run, 3, 1)\n",
    "print('wells for test:', test_well_list)\n",
    "\n",
    "def seq_train_wells_preparation(dataset_main, dataset_aug, train_well_list, block_size, var):\n",
    "    train_dataset = dataset_main[dataset_main.well_trgt.isin(train_well_list)]\n",
    "    aug_dataset = dataset_aug[dataset_aug.well_trgt.isin(train_well_list)]\n",
    "    var_ftr = var + '_ftr'\n",
    "    var_trgt = var + '_trgt'\n",
    "    train_dataset['PHIT_ftr%'] = train_dataset[var_ftr]*100\n",
    "    train_dataset['PHIT_trgt%'] = train_dataset[var_trgt]*100\n",
    "    aug_dataset['PHIT_ftr%'] = aug_dataset[var_ftr]*100\n",
    "    aug_dataset['PHIT_trgt%'] = aug_dataset[var_trgt]*100\n",
    "\n",
    "    def feature_aug_stacking(train_dataset, aug_dataset, offset_num):\n",
    "        feature = train_dataset[(train_dataset.offset==offset_num)]['PHIT_ftr%'].fillna(0).values\n",
    "        aug = aug_dataset[(aug_dataset.offset==offset_num)]['PHIT_ftr%'].fillna(0).values\n",
    "        concat_data = np.concatenate((feature, aug))\n",
    "        # concat_data_norm = scaler.fit_transform(concat_data.reshape(-1,1))\n",
    "        # result = concat_data_norm.reshape(-1, block_size, 1)  \n",
    "        result = concat_data.reshape(-1, block_size, 1) \n",
    "        return result\n",
    "    X_train_1 = feature_aug_stacking(train_dataset, aug_dataset, 1)\n",
    "    X_train_2 = feature_aug_stacking(train_dataset, aug_dataset, 2)\n",
    "    X_train_3 = feature_aug_stacking(train_dataset, aug_dataset, 3)\n",
    "   \n",
    "    y_well = train_dataset[     (train_dataset.offset==1)]['well_trgt'].values.reshape(-1, 1)\n",
    "    y_fm = train_dataset[       (train_dataset.offset==1)]['FORMATION_up'].values.reshape(-1, 1)\n",
    "    y_data = train_dataset[     (train_dataset.offset==1)]['PHIT_trgt%'].values.reshape(-1, 1)\n",
    "    y_well_aug = aug_dataset[     (aug_dataset.offset==1)]['well_trgt'].values.reshape(-1, 1)\n",
    "    y_fm_aug = aug_dataset[       (aug_dataset.offset==1)]['FORMATION_up'].values.reshape(-1, 1)\n",
    "    y_data_aug = aug_dataset[     (aug_dataset.offset==1)]['PHIT_trgt%'].values.reshape(-1, 1)\n",
    "\n",
    "    # y_data = scaler.fit_transform(y_data)\n",
    "    # y_data_aug = scaler.fit_transform(y_data_aug)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_final_aug = np.stack((y_well_aug, y_fm_aug, y_data_aug), axis=1)\n",
    "    y_final_v2 = np.concatenate((y_final, y_final_aug))\n",
    "    y_reshape = y_final_v2.reshape(-1, block_size, 1, 3)\n",
    "    y_train = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "    return X_train_1, X_train_2, X_train_3, y_train\n",
    "X_train_1, X_train_2, X_train_3, y_train_seq = seq_train_wells_preparation(final_phit_bal8_run, aug_bal8, train_well_list, block_lenght, 'PHIT_clp')\n",
    "print('X_train / y_train datasets: ',X_train_1.shape, y_train_seq.shape)\n",
    "\n",
    "def seq_test_wells_preparation(dataset_main, test_well_list, block_size, var):\n",
    "    test_dataset = dataset_main[dataset_main.well_trgt.isin(test_well_list)]\n",
    "    var_ftr = var + '_ftr'\n",
    "    var_trgt = var + '_trgt'\n",
    "    test_dataset['PHIT_ftr%'] = test_dataset[var_ftr]*100\n",
    "    test_dataset['PHIT_trgt%'] = test_dataset[var_trgt]*100\n",
    "\n",
    "    def feature_offset(test_dataset, offset_num):\n",
    "        phit_feature = test_dataset[(test_dataset.offset==offset_num)]['PHIT_ftr%'].fillna(0).values\n",
    "        # phit_feature_norm = scaler.fit_transform(phit_feature.reshape(-1, 1))\n",
    "        # X_test = phit_feature_norm.reshape(-1, block_size, 1)\n",
    "        X_test = phit_feature.reshape(-1, block_size, 1)\n",
    "        return X_test\n",
    "    X_test_1 = feature_offset(test_dataset, 1)\n",
    "    X_test_2 = feature_offset(test_dataset, 2)\n",
    "    X_test_3 = feature_offset(test_dataset, 3)\n",
    "\n",
    "    y_well = test_dataset[     (test_dataset.offset==1)]['well_trgt'].values.reshape(-1, 1)\n",
    "    y_fm = test_dataset[       (test_dataset.offset==1)]['FORMATION_up'].values.reshape(-1, 1)\n",
    "    y_data = test_dataset[     (test_dataset.offset==1)]['PHIT_trgt%'].values.reshape(-1, 1)\n",
    "\n",
    "    # y_data = scaler.fit_transform(y_data)\n",
    "    y_final = np.stack((y_well, y_fm, y_data), axis=1)\n",
    "    y_reshape = y_final.reshape(-1, block_size, 1, 3)\n",
    "    y_test = y_reshape[:,:,:,2].reshape(-1, block_size, 1)\n",
    "    y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "    return X_test_1, X_test_2, X_test_3, y_test, y_final, y_reshape\n",
    "X_test_1, X_test_2, X_test_3, y_test_seq, y_test_fullds_seq, y_reshape = seq_test_wells_preparation(final_phit_bal8_run, test_well_list, block_lenght, 'PHIT_clp')\n",
    "print('X_test / y_test / y_final datasets: ', X_test_1.shape, y_test_seq.shape, y_test_fullds_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aenc_seq = Sequential([\n",
    "    Conv2D(filters=256, kernel_size=(3,3), strides=1, activation='relu', padding='same', \n",
    "           kernel_initializer = RandomUniform(-1,1), input_shape =(block_lenght, 1, 1)),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=32, kernel_size=(1,1), strides=1, activation='relu', padding='same'),\n",
    "    Conv2DTranspose(filters=256, kernel_size=(3,3), strides=2, activation = 'relu', padding='same'),\n",
    "    Conv2DTranspose(filters=1, kernel_size=(3,3), strides=1, activation=\"relu\", padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(block_lenght)\n",
    "    ])\n",
    "opt_aenc_seq = Adam(learning_rate=0.001)\n",
    "model_aenc_seq.summary()\n",
    "\n",
    "def run_model_seq(X_train, y_train, X_test, y_test, model, optmzr, lss, eph, btch_sz, erl_stp, vrbs):\n",
    "    model.compile(optimizer=optmzr, loss=lss)\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    if erl_stp == 1:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs, validation_data=(X_test, y_test),callbacks=[early_stop])\n",
    "    if erl_stp == 0:\n",
    "        history = model.fit(X_train, y_train, epochs=eph, batch_size=btch_sz, verbose=vrbs, validation_data=(X_test, y_test))  \n",
    "    y_pred = model.predict(X_test)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    return y_pred\n",
    "y_pred_seq = run_model_seq(X_train_1, y_train_seq, X_test_1, y_test_seq, model_aenc_seq, opt_aenc_seq, 'mse', 200, 50, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2662,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_3offset_wells('B32Z', 'Balakhany X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_data_proc(X_test, y_test_init, y_test,  y_pred):\n",
    "    X_test_data = np.array(X_test[:,:,0]).flatten()\n",
    "    y_test_data = np.array(y_test[:,:,0]).flatten()\n",
    "    y_test_wellname = y_test_init[:,0].flatten()\n",
    "    y_test_formation = y_test_init[:,1].flatten()\n",
    "    y_pred_data = y_pred.flatten()\n",
    "    y_test_full = np.stack((y_test_wellname, y_test_formation, y_test_data, y_pred_data, X_test_data), axis=1)\n",
    "    result = pd.DataFrame(y_test_full, columns=['well_trgt','FORMATION_up_trgt','PHIT_trgt%', 'PHIT_trgt%_pred', 'xtest_PHIT_ftr%'])\n",
    "    mae = mean_absolute_error(result['xtest_PHIT_ftr%'], result['PHIT_trgt%_pred'])\n",
    "    print(\"Mean Absolute Error (MAE):\", mae)\n",
    "    return result\n",
    "def seq_data_display(dataset, dataset_offsets, fmname):\n",
    "    well_test = dataset.well_trgt.unique()\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1, 6, figsize=(12, 10))\n",
    "        def track_display(dataset, well_test, well_num, track_num):\n",
    "            feature = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['xtest_PHIT_ftr%']\n",
    "            target = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_trgt%']\n",
    "            prediction = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_trgt%_pred']\n",
    "            target_ind = dataset[(dataset.well_trgt == well_test[well_num]) & (dataset.FORMATION_up_trgt == fmname)]['PHIT_trgt%_pred']\n",
    "            ind = [i for i in range(len(target_ind))]\n",
    "            cutoff = [13 for i in range(len(target_ind))]\n",
    "            ax[track_num].plot(feature, ind, label='PHIT ftr1', color='green', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num].plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num].set_title('well1 ' + offset_wells_list(dataset_offsets, well_test[well_num],'Balakhany VIII')['well1'].iloc[0], fontsize=9)\n",
    "            ax[track_num].set_yticks(range(min(ind), max(ind), 50))\n",
    "            ax[track_num].invert_yaxis()\n",
    "            ax[track_num].set_xlim(30, 0)\n",
    "            ax[track_num].grid(axis='y')\n",
    "            ax[track_num+1].plot(target, ind, label='PHIT trgt', color='red', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+1].plot(prediction, ind, label='PHIT pred', color='blue', lw=2, zorder=2, alpha=0.5)\n",
    "            ax[track_num+1].plot(cutoff, ind, lw=2, color='black', linestyle = '--', label='phit_cutoff', alpha=0.33)\n",
    "            ax[track_num+1].set_yticks(range(min(ind), max(ind), 50))\n",
    "            ax[track_num+1].invert_yaxis()\n",
    "            ax[track_num+1].set_xlim(30, 0)\n",
    "            ax[track_num+1].grid(axis='y')\n",
    "            ax[track_num+1].legend(fontsize=7)\n",
    "            ax[track_num+1].set_title('Target ' + well_test[well_num], fontsize=11)\n",
    "        track_display(dataset, well_test, 0, 0)\n",
    "        track_display(dataset, well_test, 1, 2)\n",
    "        track_display(dataset, well_test, 2, 4)\n",
    "    except Exception as e:\n",
    "        print(f'Probably 1 or several wells are absent due to the error \"{e}\"')\n",
    "    return fig.show()\n",
    "\n",
    "seq_full_df = seq_data_proc(X_test_1, y_test_fullds_seq, y_test_seq, y_pred_seq)\n",
    "seq_data_display(seq_full_df, avg_prop_tst_kh_cut0, 'Balakhany VIII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary jobs - data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_plots_phit_vsh_matrix(dataset, platform, variable, flag, max_var, comment):\n",
    "    \"\"\"\n",
    "    flag = 'phit' or 'perm'\n",
    "    \"\"\"\n",
    "    rows = 4\n",
    "    columns = 9\n",
    "    wells_letter = [wellname for wellname in dataset.well.unique() if wellname.startswith(platform)]\n",
    "    fig, ax = plt.subplots(rows,columns, figsize=(16,rows*3))\n",
    "    counter = 0\n",
    "    y_real_list = []\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                data = dataset[dataset.well==wells_letter[counter]]\n",
    "                y_real_list.append(len(data))\n",
    "                counter +=1\n",
    "    max_ind = max(y_real_list)\n",
    "    counter = 0\n",
    "    for j in range(0, rows):\n",
    "        for i in range(0, columns):\n",
    "            if counter < len(wells_letter):\n",
    "                well_data = dataset[dataset.well==wells_letter[counter]]\n",
    "                ind = well_data[variable]\n",
    "                y_real = [k for k in range(len(ind))]\n",
    "                y_desired = [k for k in range(max_ind)]\n",
    "                y_diff = len(y_desired) - len(y_real)\n",
    "                values_to_add = [0.12 for k in range(y_diff)]\n",
    "                x = well_data[variable]\n",
    "                x_gr = well_data['VSH_GRcube']\n",
    "                x_new = pd.concat([x, pd.Series(values_to_add)])\n",
    "                x_gr_new = pd.concat([x_gr, pd.Series(values_to_add)])          \n",
    "                if flag == 'phit':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='green', lw=1.5, alpha=1, zorder=1)\n",
    "                    ax[j,i].set_xlim(0.1, 0.35)\n",
    "                    # twin = ax[j,i].twiny()\n",
    "                    # twin.plot(x_gr_new, y_desired, color='green', lw=2, alpha=0.5, zorder=0)\n",
    "                    # twin.set_xlim(0, 1)\n",
    "                if flag == 'perm':\n",
    "                    ax[j,i].plot(x_new, y_desired, color='purple', lw=2, alpha=0.75)\n",
    "                    ax[j,i].set_xscale('log')\n",
    "                    ax[j,i].set_xlim(0.1, max_var)\n",
    "                ax[j,i].set_title(wells_letter[counter] + comment)\n",
    "                ax[j,i].invert_yaxis()\n",
    "                ax[j,i].grid()\n",
    "                counter +=1\n",
    "\n",
    "    return plt.tight_layout()\n",
    "# for letter in ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J']:\n",
    "for letter in ['A','B']:\n",
    "    well_plots_phit_vsh_matrix(well_bal8_interp_phavg, letter, 'PHIT_clp', 'phit', 0.35, ' bal8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PHIT to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a01w = well_bal8_interp_phavg_cut[well_bal8_interp_phavg_cut.well == 'A01W'][['TST','PHIT']]\n",
    "a01w['PHIT_int'] = a01w['PHIT'].round(3)\n",
    "a01w_pvt = a01w.pivot(index='TST', columns='PHIT_int', values='PHIT')\n",
    "a01w_pvt = a01w_pvt.apply(lambda row: row.fillna(row.median()), axis=1)\n",
    "a01w_pvt2 = a01w.pivot(index='TST', columns='PHIT_int', values='PHIT')\n",
    "fig, ax = plt.subplots(1,2,figsize=(4,10))\n",
    "# ax[0].imshow(a01w_pvt)\n",
    "ax[0].imshow(a01w_pvt2.fillna(0), alpha=0.5)\n",
    "ax[0].set_title('A01W');\n",
    "ax[1].plot(a01w.PHIT, a01w.index, color='red', lw=2)\n",
    "ax[1].invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GR-correlation track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df = df_dist_kh_bal_fin.copy()\n",
    "distance_df = distance_df[['well', 'FORMATION_up', 'dist1', 'well1']]\n",
    "distance_df8 = distance_df[distance_df.FORMATION_up == 'Balakhany VIII']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df = df_bal_net2_kh.copy()\n",
    "distance_df = distance_df[['well', 'FORMATION_up', 'X_mean', 'Y_mean','TVD_SCS']]\n",
    "distance_df8 = distance_df[distance_df.FORMATION_up=='Balakhany VIII']\n",
    "distance_df8_gb = distance_df8.groupby('well').apply(lambda x: x.iloc[0]).drop('well', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pd.DataFrame(euclidean_distances(distance_df8_gb[['X_mean', 'Y_mean', 'TVD_SCS']]), columns=list(distance_df8_gb.well))\n",
    "dist = dist.join(distance_df8_gb['well'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "well = 'A05'\n",
    "lst = []\n",
    "def searching_wells(dist, well):\n",
    "    for i in range(len(dist['well'])):\n",
    "        if well == dist['well'].iloc[i]:\n",
    "            row_data = dist[dist.well==well].drop('well', axis=1).T.sort_values(by=i).reset_index()\n",
    "    return row_data.iloc[1]['index'], row_data.iloc[2]['index'], row_data.iloc[3]['index'], row_data.iloc[4]['index'], row_data.iloc[5]['index'], row_data.iloc[6]['index']\n",
    "test = searching_wells(dist, well)\n",
    "lst.append(test[0])\n",
    "well = lst[0]\n",
    "test2 = searching_wells(dist, lst[0])\n",
    "lst.append(test2[1])\n",
    "test3 = searching_wells(dist, lst[-1])\n",
    "lst.append(test3[0])\n",
    "test4 = searching_wells(dist, lst[-1])\n",
    "lst.append(test4[0])\n",
    "test5 = searching_wells(dist, lst[-1])\n",
    "lst.append(test5[0])\n",
    "test6 = searching_wells(dist, lst[-1])\n",
    "lst.append(test6[3])\n",
    "test7 = searching_wells(dist, lst[-1])\n",
    "lst.append(test7[5])\n",
    "test8 = searching_wells(dist, lst[-1])\n",
    "lst.append(test8[0])\n",
    "test9 = searching_wells(dist, lst[-1])\n",
    "lst.append(test9[5])\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_list = ['A05Z', 'A01W', 'A01X', 'A01Y', 'A11Z', 'A03Z', 'A12U', 'A10', 'A07Z']\n",
    "gr_correls = df_bal_net2_kh[(df_bal_net2_kh.well.isin(wells_list)) & (df_bal_net2_kh.FORMATION_up == 'Balakhany VIII')]\n",
    "gr_correls = gr_correls[['well', 'FORMATION_up', 'GR_N']]\n",
    "# well_bal8_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 1\n",
    "def well_gaussian_filter(dataset, wellname, fmname, percentage):  \n",
    "    data = dataset[dataset.well == wellname]['GR_N']\n",
    "    coeff = percentage/100\n",
    "    sigma = int(round(len(data)*coeff, 0))\n",
    "    data = data.reset_index().drop('index', axis=1)\n",
    "    smoothed_data = gaussian_filter(data, sigma=sigma)\n",
    "    data['GN_N_gaus'] = smoothed_data\n",
    "    data['well'] = wellname\n",
    "    data['FORMATION_up'] = fmname\n",
    "    return data\n",
    "df_lst = []\n",
    "for wellname in wells_list:\n",
    "    smooth_data = well_gaussian_filter(gr_correls, wellname, 'Balakhany VIII', percentage)\n",
    "    df_lst.append(smooth_data)\n",
    "gr_correls_smooth = pd.concat(df_lst)\n",
    "\n",
    "counter = 0\n",
    "variable = 'GR_N'\n",
    "dataset = gr_correls_smooth\n",
    "variable_smooth = 'GN_N_gaus'\n",
    "comment = '_bal8'\n",
    "fig, ax = plt.subplots(1, 9, figsize=(14,4))\n",
    "for i in range(0, 9):\n",
    "    if counter < len(wells_list):\n",
    "        well_data = gr_correls_smooth[gr_correls_smooth.well==wells_list[counter]]\n",
    "        ind = well_data[variable]\n",
    "        y_real = [k for k in range(len(ind))]\n",
    "        x = well_data[variable]\n",
    "        x_smooth = well_data[variable_smooth]\n",
    "        ax[i].plot(x, y_real, color='green', lw=2, alpha=1, zorder=1)\n",
    "        ax[i].plot(x_smooth, y_real, color='red', lw=1.5, alpha=1, zorder=2)\n",
    "        ax[i].set_xlim(0, 150)\n",
    "        ax[i].set_title(wells_list[counter] + comment)\n",
    "        ax[i].invert_yaxis()\n",
    "        counter +=1\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "G.add_nodes_from(['J28', 'J29', 'J30', 'A01W'])\n",
    "G.add_edges_from([('A01W', 'J28', {'weight': 7114}),\n",
    "                  ('A01W', 'J29', {'weight': 6714}),\n",
    "                  ('A01W', 'J30', {'weight': 4150})])\n",
    "\n",
    "# Draw the graph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, font_weight='bold', node_size=700, node_color='skyblue', font_color='black')\n",
    "labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Find the shortest route through all nodes\n",
    "shortest_route = None\n",
    "min_distance = float('inf')\n",
    "\n",
    "for route in permutations(G.nodes):\n",
    "    distance = sum(G[route[i]][route[i + 1]]['weight'] for i in range(len(route) - 1))\n",
    "    distance += G[route[-1]][route[0]]['weight']  # Add the edge back to the starting node\n",
    "\n",
    "    if distance < min_distance:\n",
    "        min_distance = distance\n",
    "        shortest_route = route\n",
    "\n",
    "print(f\"Shortest Route: {shortest_route}\")\n",
    "print(f\"Minimum Distance: {min_distance}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
