{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.float_format = lambda x: '%.5f' % x\n",
    "pd.options.display.max_columns = 15\n",
    "pd.options.display.max_rows = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\general_logs\\logs8_ntd_v5.csv')\n",
    "df10 = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\general_logs\\logs10_ntd_v4.csv')\n",
    "df8_ntd = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\layers\\ntd_top_phi_bot8_bp_v4.csv').drop(columns=['Unnamed: 0'])\n",
    "df10_ntd = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\layers\\ntd_top_phi_bot10_bp_v4.csv').drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "test = df8[df8.well.isin(['C01','C01AY'])][['well','formation_up','xmean','ymean']].drop_duplicates()\n",
    "display(test, test['xmean'].iloc[1])\n",
    "def calculate_distance(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "calculate_distance(test['xmean'].iloc[0], test['ymean'].iloc[0], test['xmean'].iloc[1], test['ymean'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.columns, df10.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8_ntd.columns, df10_ntd.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P50 calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cdf(df, wellname):\n",
    "    data = df[df.well == wellname].copy()\n",
    "    mu = data['htst'].mean()\n",
    "    sigma = data['htst'].std()\n",
    "\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    cdf = norm.cdf(x, mu, sigma)\n",
    "    value_p50 = norm.ppf(0.5, mu, sigma)\n",
    "    plt.plot(x, cdf)\n",
    "    plt.xlabel('htst')\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title(f'CDF of htst for well {wellname}')\n",
    "    print(wellname, mu, sigma, value_p50);\n",
    "\n",
    "def ecdf(df, wellname, p):\n",
    "    data = df[(df.well == wellname) & (df.htst > 0.5)]['htst']\n",
    "    # Sort the data\n",
    "    x = np.sort(data)\n",
    "    x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "    # Calculate ECDF: y-value for each data point\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    # Plot ECDF\n",
    "    vnorm_p = np.percentile(x_norm, p*100)\n",
    "    value_p = np.percentile(x, p*100)\n",
    "    plt.plot(x_norm, y, marker='.', linestyle='none', label=wellname)\n",
    "    plt.scatter(vnorm_p, p, s=50, alpha=0.5, ec='black')\n",
    "    plt.xlabel('htst norm')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Cumulative Probability')\n",
    "    plt.title(f'ECDF of htst for well')\n",
    "    plt.margins(0.02)  # Keeps data off plot edges\n",
    "    print(wellname, 'value p50:',value_p)\n",
    "    return x, y\n",
    "\n",
    "b39 = ecdf(df8_ntd, 'B39', 0.5)\n",
    "d37 = ecdf(df8_ntd, 'D37', 0.5)\n",
    "b01y = ecdf(df8_ntd, 'C16', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quntile_calc(df, cutoff, quant):\n",
    "    df_lst = []\n",
    "    for wellname in df.well.unique():\n",
    "        data = df[(df.well == wellname) & (df.htst > cutoff)]\n",
    "        p50_1m = data['htst'].quantile(quant)\n",
    "        df_res = pd.DataFrame({'well': wellname, f'htst_p{quant*100:.0f}_{cutoff}m': p50_1m}, index=[0])\n",
    "        df_lst.append(df_res)\n",
    "    df_res = pd.concat(df_lst).reset_index(drop=True)\n",
    "    return df_res\n",
    "htst_p50 = quntile_calc(df8_ntd, 1, 0.5)\n",
    "# df8_ntd_p50 = df8_ntd.set_index('well').join(htst_p50.set_index('well'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agg fuctions bal8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8_field = df8[['well','formation_up', 'field']].drop_duplicates()\n",
    "df8_field.replace({'field': {'2_CENTRAL AZERI': 'CENTRAL AZERI', '3_EAST AZERI': 'EAST AZERI', '1_WEST AZERI': 'WEST AZERI'}}, inplace=True)\n",
    "df10_field = df10[['well','formation_up', 'field']].drop_duplicates()\n",
    "df_field = pd.concat([df8_field, df10_field]).drop_duplicates().reset_index(drop=True)\n",
    "df_field.loc[df_field.formation_up == 'Balakhany VIII', 'formation'] = 'bal8'\n",
    "df_field.loc[df_field.formation_up == 'Balakhany X', 'formation'] = 'bal10'\n",
    "df_field = df_field.drop(columns=['formation_up'])\n",
    "df_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fluid_assign(df, fm):\n",
    "    df_fluid = df.groupby('well')['fluid_code'].apply(lambda x: x.mode()).reset_index().drop(columns='level_1')\n",
    "    df_fluid['fluid'] = 'un'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 1, 'fluid'] = 'gas'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 2, 'fluid'] = 'oil'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 3, 'fluid'] = 'water'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 4, 'fluid'] = 'tr_gas'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 5, 'fluid'] = 'res_gas'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 6, 'fluid'] = 'spt_oil'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 7, 'fluid'] = 'lcg'\n",
    "    df_fluid.loc[df_fluid.fluid_code == 8, 'fluid'] = 'lco'\n",
    "    df_fluid['formation'] = fm\n",
    "    return df_fluid\n",
    "\n",
    "df8_fluid = fluid_assign(df8, 'bal8').drop(columns='fluid_code')\n",
    "df10_fluid = fluid_assign(df10, 'bal10').drop(columns='fluid_code')\n",
    "df_fluid = pd.concat([df8_fluid, df10_fluid]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_func_ntd(df, fm):\n",
    "    def gmean_func(x):\n",
    "        return np.exp(np.mean(np.log(x)))\n",
    "\n",
    "    def p25_1m(x):\n",
    "        x = x[x>1]\n",
    "        return np.percentile(x, 25)\n",
    "    def p50_1m(x):\n",
    "        x = x[x>1]\n",
    "        return np.percentile(x, 50)\n",
    "    def p75_1m(x):\n",
    "        x = x[x>1]\n",
    "        return np.percentile(x, 75)\n",
    "    def p25(x):\n",
    "        return np.percentile(x, 25)\n",
    "    def p50(x):\n",
    "        return np.percentile(x, 50)\n",
    "    def p75(x):\n",
    "        return np.percentile(x, 75)\n",
    "\n",
    "    aggregation_functions = {\n",
    "        'phit_avg': ['mean', 'median', 'sum'],  \n",
    "        'vsh_avg': ['mean', 'median', 'sum'],\n",
    "        'perm_avg': ['sum', gmean_func, p25, p50, p75],\n",
    "        'htst': ['sum', 'count', p50, p25_1m, p50_1m, p75_1m],\n",
    "        'khtst':['sum']}\n",
    "    # Group by 'well' and aggregate according to the defined functions\n",
    "    aggregated_df = df.groupby('well').agg(aggregation_functions).reset_index()\n",
    "    aggregated_df.columns = ['_'.join(col).strip() for col in aggregated_df.columns.values]\n",
    "    aggregated_df = aggregated_df.rename(columns={'well_': 'well'})\n",
    "    aggregated_df['formation'] = fm\n",
    "    return aggregated_df\n",
    "\n",
    "df8_ntd_agg = agg_func_ntd(df8_ntd, 'bal8')\n",
    "df10_ntd_agg = agg_func_ntd(df10_ntd, 'bal10')\n",
    "df_ntd_agg = pd.concat([df8_ntd_agg, df10_ntd_agg]).reset_index(drop=True)\n",
    "df_ntd_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_ntd_agg[['phit_avg_mean', 'vsh_avg_mean', 'perm_avg_p50', 'htst_sum', 'htst_count', 'khtst_sum', 'htst_p50_1m', 'formation']], hue='formation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_run(df, eps_run, samples_run):\n",
    "    # Selecting numerical columns (excluding 'formation' since it's categorical)\n",
    "    features = df\n",
    "\n",
    "    # Standardizing the features (important for distance-based algorithms like DBSCAN)\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Instantiate DBSCAN\n",
    "    dbscan = DBSCAN(eps = eps_run, min_samples = samples_run)  # Adjust eps and min_samples based on your dataset\n",
    "\n",
    "    # Fit DBSCAN to the scaled features\n",
    "    dbscan.fit(features_scaled)\n",
    "\n",
    "    # Extract labels (-1 indicates outliers/noise)\n",
    "    labels = dbscan.labels_\n",
    "\n",
    "    # Add cluster labels to the original DataFrame\n",
    "    features['cluster'] = labels\n",
    "\n",
    "    # # Identifying outliers\n",
    "    # outliers = features[features['cluster'] == -1]\n",
    "    # print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "    # # You can also explore the number of clusters found\n",
    "    # n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    # print(\"Number of clusters:\", n_clusters)\n",
    "    sns.pairplot(features, hue='cluster', palette='viridis');\n",
    "    return features\n",
    "\n",
    "dbscan_res = dbscan_run(df_ntd_agg[['phit_avg_mean', 'htst_p50_1m']], 0.7, 3)\n",
    "df_ntd_agg_v2 = df_ntd_agg.join(dbscan_res['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v2 = df_ntd_agg_v2[df_ntd_agg_v2.cluster != -1]\n",
    "sns.pairplot(df_ntd_agg_v2[['phit_avg_mean', 'vsh_avg_mean', 'perm_avg_gmean_func', \n",
    "                            'htst_sum', 'htst_count', 'khtst_sum', 'htst_p50_1m', \n",
    "                            'formation']], hue='formation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log8_ntd = pd.read_csv(r'C:\\jupyter\\SPP\\inputoutput\\general_logs\\logs8_ntd_v5.csv')\n",
    "# log8_ntd[['well','khtst', 'formation_up']].groupby('well').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df_ntd_agg_v2[['phit_avg_mean', 'vsh_avg_mean', 'perm_avg_p50',\n",
    "                            'khtst_sum',\n",
    "                            'htst_sum', 'htst_count', 'htst_p50',\n",
    "                            'htst_p25_1m', 'htst_p50_1m', 'htst_p75_1m',\n",
    "                            'formation']], hue='formation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v3 = (df_ntd_agg_v2.set_index(['well','formation']).join(df_fluid.set_index(['well','formation']))).join(df_field.set_index(['well','formation'])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v3.field.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ntd_agg_v3.to_csv(r'C:\\jupyter\\SPP\\inputoutput\\layers\\ntd_agg_v3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
